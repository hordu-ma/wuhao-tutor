#!/usr/bin/env python3
"""
æ•°æ®åº“å¤‡ä»½å’Œæ¢å¤è„šæœ¬
æ”¯æŒPostgreSQLæ•°æ®åº“çš„è‡ªåŠ¨å¤‡ä»½ã€æ¢å¤å’Œç®¡ç†
"""

import asyncio
import sys
import os
import subprocess
import json
from typing import Dict, List, Optional, Any
from pathlib import Path
from datetime import datetime, timezone
import logging
import shutil
import gzip

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.core.config import get_settings

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class DatabaseBackupManager:
    """æ•°æ®åº“å¤‡ä»½ç®¡ç†å™¨"""

    def __init__(self, environment: str = "production"):
        """
        åˆå§‹åŒ–å¤‡ä»½ç®¡ç†å™¨

        Args:
            environment: ç¯å¢ƒåç§°
        """
        os.environ["ENVIRONMENT"] = environment
        self.settings = get_settings()
        self.environment = environment
        self.backup_dir = project_root / "backups"
        self.backup_dir.mkdir(exist_ok=True)

        logger.info(f"åˆå§‹åŒ–å¤‡ä»½ç®¡ç†å™¨ - ç¯å¢ƒ: {environment}")
        logger.info(f"å¤‡ä»½ç›®å½•: {self.backup_dir}")

    def _get_pg_dump_command(self) -> List[str]:
        """è·å–pg_dumpå‘½ä»¤å‚æ•°"""
        return [
            "pg_dump",
            "-h", self.settings.POSTGRES_SERVER,
            "-p", str(self.settings.POSTGRES_PORT),
            "-U", self.settings.POSTGRES_USER,
            "-d", self.settings.POSTGRES_DB,
            "--no-password",
            "--verbose",
            "--clean",
            "--if-exists",
            "--create"
        ]

    def _get_pg_restore_command(self) -> List[str]:
        """è·å–pg_restoreåŸºç¡€å‘½ä»¤å‚æ•°"""
        return [
            "psql",
            "-h", self.settings.POSTGRES_SERVER,
            "-p", str(self.settings.POSTGRES_PORT),
            "-U", self.settings.POSTGRES_USER,
            "--no-password",
            "-v", "ON_ERROR_STOP=1"
        ]

    def _set_pgpassword(self) -> Dict[str, str]:
        """è®¾ç½®PostgreSQLå¯†ç ç¯å¢ƒå˜é‡"""
        env = os.environ.copy()
        if self.settings.POSTGRES_PASSWORD:
            env["PGPASSWORD"] = self.settings.POSTGRES_PASSWORD
        return env

    def create_backup(self, backup_name: Optional[str] = None, compress: bool = True) -> tuple[bool, str]:
        """
        åˆ›å»ºæ•°æ®åº“å¤‡ä»½

        Args:
            backup_name: å¤‡ä»½åç§°ï¼Œå¦‚æœä¸ºNoneåˆ™è‡ªåŠ¨ç”Ÿæˆ
            compress: æ˜¯å¦å‹ç¼©å¤‡ä»½æ–‡ä»¶

        Returns:
            (success, backup_file_path)
        """
        try:
            # ç”Ÿæˆå¤‡ä»½æ–‡ä»¶å
            if not backup_name:
                timestamp = datetime.now(timezone.utc).strftime("%Y%m%d_%H%M%S")
                backup_name = f"{self.settings.POSTGRES_DB}_{self.environment}_{timestamp}"

            backup_file = self.backup_dir / f"{backup_name}.sql"

            logger.info(f"ğŸ”„ å¼€å§‹åˆ›å»ºæ•°æ®åº“å¤‡ä»½: {backup_name}")
            logger.info(f"   ç›®æ ‡æ–‡ä»¶: {backup_file}")

            # æ‰§è¡Œpg_dump
            cmd = self._get_pg_dump_command()
            env = self._set_pgpassword()

            with open(backup_file, 'w', encoding='utf-8') as f:
                result = subprocess.run(
                    cmd,
                    stdout=f,
                    stderr=subprocess.PIPE,
                    env=env,
                    text=True,
                    timeout=3600  # 1å°æ—¶è¶…æ—¶
                )

            if result.returncode != 0:
                error_msg = result.stderr or "æœªçŸ¥é”™è¯¯"
                logger.error(f"âŒ æ•°æ®åº“å¤‡ä»½å¤±è´¥: {error_msg}")
                if backup_file.exists():
                    backup_file.unlink()
                return False, ""

            # å‹ç¼©å¤‡ä»½æ–‡ä»¶
            final_backup_file = str(backup_file)
            if compress:
                compressed_file = backup_file.with_suffix('.sql.gz')
                with open(backup_file, 'rb') as f_in:
                    with gzip.open(compressed_file, 'wb') as f_out:
                        shutil.copyfileobj(f_in, f_out)

                backup_file.unlink()  # åˆ é™¤æœªå‹ç¼©æ–‡ä»¶
                final_backup_file = str(compressed_file)

            # è·å–æ–‡ä»¶å¤§å°
            file_size = Path(final_backup_file).stat().st_size
            size_mb = file_size / (1024 * 1024)

            logger.info(f"âœ… æ•°æ®åº“å¤‡ä»½æˆåŠŸ!")
            logger.info(f"   å¤‡ä»½æ–‡ä»¶: {final_backup_file}")
            logger.info(f"   æ–‡ä»¶å¤§å°: {size_mb:.2f} MB")

            # åˆ›å»ºå¤‡ä»½å…ƒæ•°æ®
            self._save_backup_metadata(backup_name, final_backup_file, file_size, compress)

            return True, final_backup_file

        except subprocess.TimeoutExpired:
            logger.error("âŒ æ•°æ®åº“å¤‡ä»½è¶…æ—¶")
            return False, ""
        except Exception as e:
            logger.error(f"âŒ åˆ›å»ºå¤‡ä»½å¤±è´¥: {e}")
            return False, ""

    def restore_backup(self, backup_file: str, target_db: Optional[str] = None) -> bool:
        """
        ä»å¤‡ä»½æ–‡ä»¶æ¢å¤æ•°æ®åº“

        Args:
            backup_file: å¤‡ä»½æ–‡ä»¶è·¯å¾„
            target_db: ç›®æ ‡æ•°æ®åº“åï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é…ç½®ä¸­çš„æ•°æ®åº“

        Returns:
            æ¢å¤æ˜¯å¦æˆåŠŸ
        """
        # åˆå§‹åŒ–ä¸´æ—¶æ–‡ä»¶å˜é‡
        temp_file = None

        try:
            backup_path = Path(backup_file)
            if not backup_path.exists():
                logger.error(f"âŒ å¤‡ä»½æ–‡ä»¶ä¸å­˜åœ¨: {backup_file}")
                return False

            logger.info(f"ğŸ”„ å¼€å§‹æ¢å¤æ•°æ®åº“å¤‡ä»½: {backup_path.name}")
            logger.warning("âš ï¸  æ­¤æ“ä½œå°†å®Œå…¨æ›¿æ¢ç°æœ‰æ•°æ®åº“å†…å®¹!")

            # å¦‚æœæ˜¯å‹ç¼©æ–‡ä»¶ï¼Œå…ˆè§£å‹
            restore_file = backup_path

            if backup_path.suffix == '.gz':
                temp_file = backup_path.with_suffix('')
                logger.info("ğŸ”„ è§£å‹å¤‡ä»½æ–‡ä»¶...")
                with gzip.open(backup_path, 'rb') as f_in:
                    with open(temp_file, 'wb') as f_out:
                        shutil.copyfileobj(f_in, f_out)
                restore_file = temp_file

            # æ‰§è¡Œæ¢å¤
            cmd = self._get_pg_restore_command()
            if target_db:
                cmd.extend(["-d", target_db])
            else:
                cmd.extend(["-d", "postgres"])  # è¿æ¥åˆ°é»˜è®¤æ•°æ®åº“æ‰§è¡ŒCREATE DATABASE

            env = self._set_pgpassword()

            with open(restore_file, 'r', encoding='utf-8') as f:
                result = subprocess.run(
                    cmd,
                    stdin=f,
                    stderr=subprocess.PIPE,
                    env=env,
                    text=True,
                    timeout=3600
                )

            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            if temp_file and temp_file.exists():
                temp_file.unlink()

            if result.returncode != 0:
                error_msg = result.stderr or "æœªçŸ¥é”™è¯¯"
                logger.error(f"âŒ æ•°æ®åº“æ¢å¤å¤±è´¥: {error_msg}")
                return False

            logger.info("âœ… æ•°æ®åº“æ¢å¤æˆåŠŸ!")
            return True

        except subprocess.TimeoutExpired:
            logger.error("âŒ æ•°æ®åº“æ¢å¤è¶…æ—¶")
            return False
        except Exception as e:
            logger.error(f"âŒ æ¢å¤å¤‡ä»½å¤±è´¥: {e}")
            return False
        finally:
            # ç¡®ä¿æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            if temp_file and temp_file.exists():
                temp_file.unlink()

    def list_backups(self) -> List[Dict[str, Any]]:
        """åˆ—å‡ºæ‰€æœ‰å¤‡ä»½æ–‡ä»¶"""
        backups = []

        # æ‰«æå¤‡ä»½æ–‡ä»¶
        for file_path in self.backup_dir.glob("*.sql*"):
            if file_path.is_file():
                stat = file_path.stat()
                backup_info = {
                    "name": file_path.stem.replace('.sql', ''),
                    "file_path": str(file_path),
                    "file_name": file_path.name,
                    "size": stat.st_size,
                    "size_mb": round(stat.st_size / (1024 * 1024), 2),
                    "created_at": datetime.fromtimestamp(stat.st_mtime, tz=timezone.utc),
                    "compressed": file_path.suffix == '.gz'
                }

                # å°è¯•åŠ è½½å…ƒæ•°æ®
                metadata = self._load_backup_metadata(backup_info["name"])
                if metadata:
                    backup_info.update(metadata)

                backups.append(backup_info)

        # æŒ‰åˆ›å»ºæ—¶é—´æ’åº
        backups.sort(key=lambda x: x["created_at"], reverse=True)
        return backups

    def delete_backup(self, backup_name: str) -> bool:
        """
        åˆ é™¤æŒ‡å®šå¤‡ä»½

        Args:
            backup_name: å¤‡ä»½åç§°

        Returns:
            åˆ é™¤æ˜¯å¦æˆåŠŸ
        """
        try:
            # æŸ¥æ‰¾å¤‡ä»½æ–‡ä»¶
            sql_file = self.backup_dir / f"{backup_name}.sql"
            gz_file = self.backup_dir / f"{backup_name}.sql.gz"

            deleted = False
            if sql_file.exists():
                sql_file.unlink()
                deleted = True
                logger.info(f"âœ… å·²åˆ é™¤å¤‡ä»½æ–‡ä»¶: {sql_file.name}")

            if gz_file.exists():
                gz_file.unlink()
                deleted = True
                logger.info(f"âœ… å·²åˆ é™¤å¤‡ä»½æ–‡ä»¶: {gz_file.name}")

            # åˆ é™¤å…ƒæ•°æ®
            metadata_file = self.backup_dir / f"{backup_name}.metadata.json"
            if metadata_file.exists():
                metadata_file.unlink()

            if not deleted:
                logger.error(f"âŒ å¤‡ä»½æ–‡ä»¶ä¸å­˜åœ¨: {backup_name}")
                return False

            return True

        except Exception as e:
            logger.error(f"âŒ åˆ é™¤å¤‡ä»½å¤±è´¥: {e}")
            return False

    def cleanup_old_backups(self, keep_count: int = 10, keep_days: int = 30) -> int:
        """
        æ¸…ç†æ—§çš„å¤‡ä»½æ–‡ä»¶

        Args:
            keep_count: ä¿ç•™çš„å¤‡ä»½æ•°é‡
            keep_days: ä¿ç•™çš„å¤©æ•°

        Returns:
            åˆ é™¤çš„å¤‡ä»½æ•°é‡
        """
        try:
            backups = self.list_backups()
            deleted_count = 0

            cutoff_date = datetime.now(timezone.utc).timestamp() - (keep_days * 24 * 3600)

            # ä¿ç•™æœ€æ–°çš„ keep_count ä¸ªå¤‡ä»½ï¼Œåˆ é™¤è¶…è¿‡ keep_days å¤©çš„å¤‡ä»½
            for i, backup in enumerate(backups):
                should_delete = False

                if i >= keep_count:
                    should_delete = True
                    reason = f"è¶…è¿‡ä¿ç•™æ•°é‡é™åˆ¶ ({keep_count})"
                elif backup["created_at"].timestamp() < cutoff_date:
                    should_delete = True
                    reason = f"è¶…è¿‡ä¿ç•™å¤©æ•°é™åˆ¶ ({keep_days}å¤©)"

                if should_delete:
                    reason = "è¿‡æœŸ" if should_delete else ""
                    if self.delete_backup(backup["name"]):
                        deleted_count += 1
                        logger.info(f"ğŸ—‘ï¸  å·²åˆ é™¤æ—§å¤‡ä»½: {backup['name']} ({reason})")

            logger.info(f"âœ… å¤‡ä»½æ¸…ç†å®Œæˆï¼Œåˆ é™¤äº† {deleted_count} ä¸ªæ—§å¤‡ä»½")
            return deleted_count

        except Exception as e:
            logger.error(f"âŒ æ¸…ç†å¤‡ä»½å¤±è´¥: {e}")
            return 0

    def _save_backup_metadata(self, backup_name: str, file_path: str, file_size: int, compressed: bool) -> None:
        """ä¿å­˜å¤‡ä»½å…ƒæ•°æ®"""
        try:
            metadata = {
                "backup_name": backup_name,
                "file_path": file_path,
                "file_size": file_size,
                "compressed": compressed,
                "environment": self.environment,
                "database_name": self.settings.POSTGRES_DB,
                "database_host": self.settings.POSTGRES_SERVER,
                "created_at": datetime.now(timezone.utc).isoformat(),
                "created_by": "db_backup.py",
                "version": "1.0"
            }

            metadata_file = self.backup_dir / f"{backup_name}.metadata.json"
            with open(metadata_file, 'w', encoding='utf-8') as f:
                json.dump(metadata, f, indent=2, ensure_ascii=False)

        except Exception as e:
            logger.warning(f"âš ï¸  ä¿å­˜å¤‡ä»½å…ƒæ•°æ®å¤±è´¥: {e}")

    def _load_backup_metadata(self, backup_name: str) -> Optional[Dict[str, Any]]:
        """åŠ è½½å¤‡ä»½å…ƒæ•°æ®"""
        try:
            metadata_file = self.backup_dir / f"{backup_name}.metadata.json"
            if metadata_file.exists():
                with open(metadata_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
        except Exception:
            pass
        return None

    def verify_backup(self, backup_file: str) -> bool:
        """
        éªŒè¯å¤‡ä»½æ–‡ä»¶å®Œæ•´æ€§

        Args:
            backup_file: å¤‡ä»½æ–‡ä»¶è·¯å¾„

        Returns:
            éªŒè¯æ˜¯å¦æˆåŠŸ
        """
        try:
            backup_path = Path(backup_file)
            if not backup_path.exists():
                logger.error(f"âŒ å¤‡ä»½æ–‡ä»¶ä¸å­˜åœ¨: {backup_file}")
                return False

            logger.info(f"ğŸ” éªŒè¯å¤‡ä»½æ–‡ä»¶: {backup_path.name}")

            # æ£€æŸ¥æ–‡ä»¶å¤§å°
            file_size = backup_path.stat().st_size
            if file_size == 0:
                logger.error("âŒ å¤‡ä»½æ–‡ä»¶ä¸ºç©º")
                return False

            # å¦‚æœæ˜¯å‹ç¼©æ–‡ä»¶ï¼Œå°è¯•è§£å‹éªŒè¯
            if backup_path.suffix == '.gz':
                try:
                    with gzip.open(backup_path, 'rt', encoding='utf-8') as f:
                        # è¯»å–æ–‡ä»¶å¤´éƒ¨éªŒè¯
                        header = f.read(1024)
                        if not header.strip():
                            logger.error("âŒ å‹ç¼©å¤‡ä»½æ–‡ä»¶å†…å®¹ä¸ºç©º")
                            return False

                        # æ£€æŸ¥æ˜¯å¦åŒ…å«SQL DDLè¯­å¥
                        if 'CREATE' not in header.upper() and 'INSERT' not in header.upper():
                            logger.warning("âš ï¸  å¤‡ä»½æ–‡ä»¶å¯èƒ½ä¸åŒ…å«æœ‰æ•ˆçš„SQLè¯­å¥")

                except gzip.BadGzipFile:
                    logger.error("âŒ å¤‡ä»½æ–‡ä»¶å‹ç¼©æ ¼å¼æŸå")
                    return False
            else:
                # æ™®é€šSQLæ–‡ä»¶éªŒè¯
                try:
                    with open(backup_path, 'r', encoding='utf-8') as f:
                        header = f.read(1024)
                        if not header.strip():
                            logger.error("âŒ å¤‡ä»½æ–‡ä»¶å†…å®¹ä¸ºç©º")
                            return False
                except UnicodeDecodeError:
                    logger.error("âŒ å¤‡ä»½æ–‡ä»¶ç¼–ç æ ¼å¼é”™è¯¯")
                    return False

            logger.info(f"âœ… å¤‡ä»½æ–‡ä»¶éªŒè¯é€šè¿‡")
            logger.info(f"   æ–‡ä»¶å¤§å°: {file_size / (1024 * 1024):.2f} MB")
            return True

        except Exception as e:
            logger.error(f"âŒ éªŒè¯å¤‡ä»½æ–‡ä»¶å¤±è´¥: {e}")
            return False


class DatabaseBackupCLI:
    """æ•°æ®åº“å¤‡ä»½å‘½ä»¤è¡Œæ¥å£"""

    def __init__(self):
        self.manager = None

    def cmd_create(self, env: str, name: Optional[str], compress: bool) -> None:
        """åˆ›å»ºå¤‡ä»½"""
        self.manager = DatabaseBackupManager(env)
        success, backup_file = self.manager.create_backup(name, compress)
        if success:
            print(f"âœ… å¤‡ä»½åˆ›å»ºæˆåŠŸ: {backup_file}")
        else:
            print("âŒ å¤‡ä»½åˆ›å»ºå¤±è´¥")
            sys.exit(1)

    def cmd_restore(self, env: str, backup_file: str, target_db: Optional[str]) -> None:
        """æ¢å¤å¤‡ä»½"""
        self.manager = DatabaseBackupManager(env)

        # ç¡®è®¤æ¢å¤æ“ä½œ
        print(f"âš ï¸  å³å°†æ¢å¤æ•°æ®åº“å¤‡ä»½: {backup_file}")
        print(f"   ç›®æ ‡ç¯å¢ƒ: {env}")
        if target_db:
            print(f"   ç›®æ ‡æ•°æ®åº“: {target_db}")
        print("   æ­¤æ“ä½œå°†å®Œå…¨æ›¿æ¢ç°æœ‰æ•°æ®åº“å†…å®¹!")

        confirm = input("ç¡®è®¤ç»§ç»­å—? (yes/N): ").strip().lower()
        if confirm != 'yes':
            print("âŒ æ“ä½œå·²å–æ¶ˆ")
            return

        success = self.manager.restore_backup(backup_file, target_db)
        if success:
            print("âœ… æ•°æ®åº“æ¢å¤æˆåŠŸ")
        else:
            print("âŒ æ•°æ®åº“æ¢å¤å¤±è´¥")
            sys.exit(1)

    def cmd_list(self, env: str) -> None:
        """åˆ—å‡ºæ‰€æœ‰å¤‡ä»½"""
        self.manager = DatabaseBackupManager(env)
        backups = self.manager.list_backups()

        if not backups:
            print("ğŸ“‹ æš‚æ— å¤‡ä»½æ–‡ä»¶")
            return

        print(f"ğŸ“‹ å¤‡ä»½åˆ—è¡¨ (ç¯å¢ƒ: {env}):")
        print(f"{'åç§°':<30} {'å¤§å°':<10} {'å‹ç¼©':<6} {'åˆ›å»ºæ—¶é—´':<20}")
        print("-" * 75)

        for backup in backups:
            compressed = "æ˜¯" if backup["compressed"] else "å¦"
            created_at = backup["created_at"].strftime("%Y-%m-%d %H:%M:%S")
            print(f"{backup['name']:<30} {backup['size_mb']:>7.2f}MB {compressed:<6} {created_at}")

    def cmd_delete(self, env: str, backup_name: str) -> None:
        """åˆ é™¤å¤‡ä»½"""
        self.manager = DatabaseBackupManager(env)

        confirm = input(f"ç¡®è®¤åˆ é™¤å¤‡ä»½ '{backup_name}'? (yes/N): ").strip().lower()
        if confirm != 'yes':
            print("âŒ æ“ä½œå·²å–æ¶ˆ")
            return

        success = self.manager.delete_backup(backup_name)
        if success:
            print(f"âœ… å¤‡ä»½å·²åˆ é™¤: {backup_name}")
        else:
            print("âŒ åˆ é™¤å¤‡ä»½å¤±è´¥")
            sys.exit(1)

    def cmd_cleanup(self, env: str, keep_count: int, keep_days: int) -> None:
        """æ¸…ç†æ—§å¤‡ä»½"""
        self.manager = DatabaseBackupManager(env)
        deleted_count = self.manager.cleanup_old_backups(keep_count, keep_days)
        print(f"âœ… æ¸…ç†å®Œæˆï¼Œåˆ é™¤äº† {deleted_count} ä¸ªæ—§å¤‡ä»½")

    def cmd_verify(self, env: str, backup_file: str) -> None:
        """éªŒè¯å¤‡ä»½æ–‡ä»¶"""
        self.manager = DatabaseBackupManager(env)
        success = self.manager.verify_backup(backup_file)
        if success:
            print("âœ… å¤‡ä»½æ–‡ä»¶éªŒè¯é€šè¿‡")
        else:
            print("âŒ å¤‡ä»½æ–‡ä»¶éªŒè¯å¤±è´¥")
            sys.exit(1)


def main():
    """ä¸»å‡½æ•°"""
    import argparse

    parser = argparse.ArgumentParser(description="æ•°æ®åº“å¤‡ä»½ç®¡ç†å·¥å…·")

    subparsers = parser.add_subparsers(dest='command', help='å¯ç”¨å‘½ä»¤')

    # createå‘½ä»¤
    create_parser = subparsers.add_parser('create', help='åˆ›å»ºæ•°æ®åº“å¤‡ä»½')
    create_parser.add_argument(
        "--env",
        choices=["development", "testing", "production"],
        default="production",
        help="ç¯å¢ƒåç§° (é»˜è®¤: production)"
    )
    create_parser.add_argument('--name', help='å¤‡ä»½åç§°')
    create_parser.add_argument('--no-compress', action='store_true', help='ä¸å‹ç¼©å¤‡ä»½æ–‡ä»¶')

    # restoreå‘½ä»¤
    restore_parser = subparsers.add_parser('restore', help='æ¢å¤æ•°æ®åº“å¤‡ä»½')
    restore_parser.add_argument(
        "--env",
        choices=["development", "testing", "production"],
        default="production",
        help="ç¯å¢ƒåç§° (é»˜è®¤: production)"
    )
    restore_parser.add_argument('backup_file', help='å¤‡ä»½æ–‡ä»¶è·¯å¾„')
    restore_parser.add_argument('--target-db', help='ç›®æ ‡æ•°æ®åº“å')

    # listå‘½ä»¤
    list_parser = subparsers.add_parser('list', help='åˆ—å‡ºæ‰€æœ‰å¤‡ä»½')
    list_parser.add_argument(
        "--env",
        choices=["development", "testing", "production"],
        default="production",
        help="ç¯å¢ƒåç§° (é»˜è®¤: production)"
    )

    # deleteå‘½ä»¤
    delete_parser = subparsers.add_parser('delete', help='åˆ é™¤å¤‡ä»½')
    delete_parser.add_argument(
        "--env",
        choices=["development", "testing", "production"],
        default="production",
        help="ç¯å¢ƒåç§° (é»˜è®¤: production)"
    )
    delete_parser.add_argument('backup_name', help='å¤‡ä»½åç§°')

    # cleanupå‘½ä»¤
    cleanup_parser = subparsers.add_parser('cleanup', help='æ¸…ç†æ—§å¤‡ä»½')
    cleanup_parser.add_argument(
        "--env",
        choices=["development", "testing", "production"],
        default="production",
        help="ç¯å¢ƒåç§° (é»˜è®¤: production)"
    )
    cleanup_parser.add_argument('--keep-count', type=int, default=10, help='ä¿ç•™å¤‡ä»½æ•°é‡')
    cleanup_parser.add_argument('--keep-days', type=int, default=30, help='ä¿ç•™å¤©æ•°')

    # verifyå‘½ä»¤
    verify_parser = subparsers.add_parser('verify', help='éªŒè¯å¤‡ä»½æ–‡ä»¶')
    verify_parser.add_argument(
        "--env",
        choices=["development", "testing", "production"],
        default="production",
        help="ç¯å¢ƒåç§° (é»˜è®¤: production)"
    )
    verify_parser.add_argument('backup_file', help='å¤‡ä»½æ–‡ä»¶è·¯å¾„')

    args = parser.parse_args()

    if not args.command:
        parser.print_help()
        return

    cli = DatabaseBackupCLI()

    if args.command == 'create':
        cli.cmd_create(args.env, args.name, not args.no_compress)
    elif args.command == 'restore':
        cli.cmd_restore(args.env, args.backup_file, args.target_db)
    elif args.command == 'list':
        cli.cmd_list(args.env)
    elif args.command == 'delete':
        cli.cmd_delete(args.env, args.backup_name)
    elif args.command == 'cleanup':
        cli.cmd_cleanup(args.env, args.keep_count, args.keep_days)
    elif args.command == 'verify':
        cli.cmd_verify(args.env, args.backup_file)


if __name__ == "__main__":
    main()
