#!/usr/bin/env python3
"""
å†å²é”™é¢˜çŸ¥è¯†ç‚¹å…³è”æ‰¹é‡è¡¥å…¨è„šæœ¬

åŠŸèƒ½:
1. æ‰«ææ‰€æœ‰ç¼ºå°‘çŸ¥è¯†ç‚¹å…³è”çš„å†å²é”™é¢˜
2. ä» knowledge_points JSON å­—æ®µæå–çŸ¥è¯†ç‚¹
3. æ‰¹é‡åˆ›å»º KnowledgeMastery å’Œ MistakeKnowledgePoint å…³è”
4. ä¸ºæ‰€æœ‰ç”¨æˆ·é‡æ–°ç”ŸæˆçŸ¥è¯†å›¾è°±å¿«ç…§

ä½¿ç”¨æ–¹æ³•:
    python scripts/backfill_knowledge_associations.py [--dry-run] [--batch-size=100]

ä½œè€…: AI Agent
åˆ›å»ºæ—¶é—´: 2025-11-15
ç‰ˆæœ¬: v1.0
"""

import asyncio
import sys
from datetime import datetime
from pathlib import Path
from typing import Dict, List
from uuid import UUID

from sqlalchemy import select, text
from sqlalchemy.ext.asyncio import AsyncSession

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

# noqa: E402 - æ¨¡å—å¯¼å…¥å¿…é¡»åœ¨ sys.path ä¿®æ”¹ä¹‹å
from src.core.database import AsyncSessionLocal  # noqa: E402
from src.core.logging import configure_logging, get_logger  # noqa: E402
from src.models.study import KnowledgeMastery, MistakeRecord  # noqa: E402
from src.services.knowledge_graph_service import KnowledgeGraphService  # noqa: E402

# é…ç½®æ—¥å¿—
configure_logging()
logger = get_logger(__name__)


class KnowledgeAssociationBackfiller:
    """çŸ¥è¯†ç‚¹å…³è”æ‰¹é‡è¡¥å…¨å™¨"""

    def __init__(self, db: AsyncSession, dry_run: bool = False, batch_size: int = 100):
        self.db = db
        self.dry_run = dry_run
        self.batch_size = batch_size
        self.stats = {
            "total_mistakes": 0,
            "mistakes_without_assoc": 0,
            "mistakes_processed": 0,
            "associations_created": 0,
            "snapshots_generated": 0,
            "errors": 0,
            "skipped": 0,
        }

    async def run(self) -> Dict:
        """æ‰§è¡Œæ‰¹é‡è¡¥å…¨"""
        logger.info("=" * 80)
        logger.info("ğŸš€ å¼€å§‹å†å²é”™é¢˜çŸ¥è¯†ç‚¹å…³è”æ‰¹é‡è¡¥å…¨")
        logger.info(f"â° æ‰§è¡Œæ—¶é—´: {datetime.now().isoformat()}")
        logger.info(f"ğŸ“Š æ¨¡å¼: {'DRY-RUN (ä»…æ£€æŸ¥)' if self.dry_run else 'æ­£å¼æ‰§è¡Œ'}")
        logger.info(f"ğŸ“¦ æ‰¹æ¬¡å¤§å°: {self.batch_size}")
        logger.info("=" * 80)

        try:
            # Step 1: æ‰«æç¼ºå¤±å…³è”çš„é”™é¢˜
            mistakes_without_assoc = await self._find_mistakes_without_associations()
            self.stats["mistakes_without_assoc"] = len(mistakes_without_assoc)

            if not mistakes_without_assoc:
                logger.info("âœ… æ‰€æœ‰å†å²é”™é¢˜éƒ½å·²æœ‰çŸ¥è¯†ç‚¹å…³è”,æ— éœ€è¡¥å…¨")
                return self.stats

            logger.info(
                f"\nğŸ“‹ å‘ç° {len(mistakes_without_assoc)} ä¸ªé”™é¢˜ç¼ºå°‘çŸ¥è¯†ç‚¹å…³è”"
            )

            if self.dry_run:
                logger.info("\nğŸ” DRY-RUN æ¨¡å¼,ä»…æ˜¾ç¤ºå¾…å¤„ç†æ•°æ®:")
                await self._preview_mistakes(mistakes_without_assoc[:10])
                logger.info(
                    f"\nğŸ’¡ å®é™…æ‰§è¡Œæ—¶å°†å¤„ç† {len(mistakes_without_assoc)} ä¸ªé”™é¢˜"
                )
                return self.stats

            # Step 2: æ‰¹é‡è¡¥å…¨å…³è”
            await self._backfill_associations(mistakes_without_assoc)

            # Step 3: é‡æ–°ç”Ÿæˆæ‰€æœ‰å¿«ç…§
            await self._regenerate_all_snapshots()

            logger.info("\n" + "=" * 80)
            logger.info("âœ… æ‰¹é‡è¡¥å…¨ä»»åŠ¡å®Œæˆ")
            self._print_stats()
            logger.info("=" * 80)

            return self.stats

        except Exception as e:
            logger.error(f"âŒ æ‰¹é‡è¡¥å…¨ä»»åŠ¡å¤±è´¥: {e}", exc_info=True)
            self.stats["errors"] += 1
            raise

    async def _find_mistakes_without_associations(self) -> List[MistakeRecord]:
        """æŸ¥æ‰¾ç¼ºå°‘çŸ¥è¯†ç‚¹å…³è”çš„é”™é¢˜"""
        logger.info("\nğŸ” Step 1: æ‰«æç¼ºå°‘çŸ¥è¯†ç‚¹å…³è”çš„é”™é¢˜...")

        # æŸ¥è¯¢æ‰€æœ‰é”™é¢˜æ€»æ•°
        result = await self.db.execute(text("SELECT COUNT(*) FROM mistake_records"))
        self.stats["total_mistakes"] = int(result.scalar() or 0)

        # æŸ¥è¯¢ç¼ºå¤±å…³è”çš„é”™é¢˜ (æœ‰ knowledge_points ä½†æ— å…³è”è®°å½•)
        stmt = text("""
            SELECT m.*
            FROM mistake_records m
            LEFT JOIN mistake_knowledge_points mkp ON m.id = mkp.mistake_id
            WHERE mkp.id IS NULL
              AND m.knowledge_points IS NOT NULL
              AND m.knowledge_points != '[]'
              AND m.knowledge_points != 'null'
            ORDER BY m.created_at DESC
        """)

        result = await self.db.execute(stmt)
        rows = result.fetchall()

        # è½¬æ¢ä¸º MistakeRecord å¯¹è±¡
        mistakes = []
        for row in rows:
            mistake = await self.db.get(MistakeRecord, row.id)
            if mistake:
                mistakes.append(mistake)

        total = self.stats['total_mistakes']
        coverage_pct = (len(mistakes) / total * 100) if total > 0 else 0
        logger.info(
            f"   æ€»é”™é¢˜æ•°: {total}, "
            f"ç¼ºå°‘å…³è”: {len(mistakes)} ({coverage_pct:.1f}%)"
        )

        return mistakes

    async def _preview_mistakes(self, mistakes: List[MistakeRecord]) -> None:
        """é¢„è§ˆå¾…å¤„ç†é”™é¢˜ (DRY-RUN æ¨¡å¼)"""
        logger.info(f"\nğŸ“ å‰ {len(mistakes)} ä¸ªå¾…å¤„ç†é”™é¢˜:")

        for i, mistake in enumerate(mistakes, 1):
            kp_list = mistake.knowledge_points or []
            # ç¡®ä¿ kp_list æ˜¯åˆ—è¡¨ç±»å‹
            if not isinstance(kp_list, list):
                kp_list = []
            
            logger.info(
                f"   {i}. ID={str(mistake.id)[:8]}... | "
                f"Subject={str(mistake.subject)} | "
                f"KPs={len(kp_list)} | "
                f"Created={mistake.created_at.strftime('%Y-%m-%d')}"
            )
            if kp_list and len(kp_list) > 0:
                logger.info(f"      çŸ¥è¯†ç‚¹: {', '.join(str(kp) for kp in kp_list[:3])}")

    async def _backfill_associations(self, mistakes: List[MistakeRecord]) -> None:
        """æ‰¹é‡è¡¥å…¨çŸ¥è¯†ç‚¹å…³è”"""
        logger.info(f"\nğŸ”— Step 2: æ‰¹é‡è¡¥å…¨çŸ¥è¯†ç‚¹å…³è” (æ‰¹æ¬¡å¤§å°: {self.batch_size})...")

        kg_service = KnowledgeGraphService(self.db)

        # åˆ†æ‰¹å¤„ç†
        total = len(mistakes)
        for i in range(0, total, self.batch_size):
            batch = mistakes[i : i + self.batch_size]
            batch_num = i // self.batch_size + 1
            total_batches = (total + self.batch_size - 1) // self.batch_size

            logger.info(
                f"\nğŸ“¦ å¤„ç†æ‰¹æ¬¡ {batch_num}/{total_batches} "
                f"({len(batch)} ä¸ªé”™é¢˜)..."
            )

            for j, mistake in enumerate(batch, 1):
                try:
                    # è·³è¿‡æ²¡æœ‰çŸ¥è¯†ç‚¹çš„é”™é¢˜
                    kp_list = mistake.knowledge_points or []
                    # ç¡®ä¿ kp_list æ˜¯åˆ—è¡¨ç±»å‹
                    if not isinstance(kp_list, list):
                        kp_list = []
                    
                    if not kp_list or len(kp_list) == 0:
                        logger.debug(
                            f"   [{j}/{len(batch)}] è·³è¿‡ {mistake.id[:8]}... (æ— çŸ¥è¯†ç‚¹)"
                        )
                        self.stats["skipped"] += 1
                        continue

                    # æ„é€  ai_feedback
                    ai_feedback = {
                        "knowledge_points": [
                            {"name": kp, "relevance": 0.8} for kp in kp_list
                        ]
                    }

                    # åˆ›å»ºå…³è”
                    ocr_text_value = str(mistake.ocr_text) if hasattr(mistake.ocr_text, '__str__') else None
                    associations = await kg_service.analyze_and_associate_knowledge_points(
                        mistake_id=UUID(str(mistake.id)),
                        user_id=UUID(str(mistake.user_id)),
                        subject=str(mistake.subject),
                        ocr_text=ocr_text_value,
                        ai_feedback=ai_feedback,
                    )

                    if associations:
                        self.stats["mistakes_processed"] += 1
                        self.stats["associations_created"] += len(associations)
                        logger.info(
                            f"   âœ… [{j}/{len(batch)}] {mistake.id[:8]}... | "
                            f"åˆ›å»º {len(associations)} ä¸ªå…³è” | "
                            f"Subject={mistake.subject}"
                        )
                    else:
                        logger.warning(
                            f"   âš ï¸  [{j}/{len(batch)}] {mistake.id[:8]}... | "
                            f"æœªåˆ›å»ºå…³è”"
                        )
                        self.stats["skipped"] += 1

                except Exception as e:
                    logger.error(
                        f"   âŒ [{j}/{len(batch)}] {mistake.id[:8]}... | "
                        f"å¤„ç†å¤±è´¥: {e}"
                    )
                    self.stats["errors"] += 1

            # æ¯æ‰¹æäº¤ä¸€æ¬¡
            try:
                await self.db.commit()
                logger.info(
                    f"   ğŸ’¾ æ‰¹æ¬¡ {batch_num} æäº¤æˆåŠŸ "
                    f"(å·²å¤„ç†: {self.stats['mistakes_processed']}/{total})"
                )
            except Exception as e:
                logger.error(f"   âŒ æ‰¹æ¬¡ {batch_num} æäº¤å¤±è´¥: {e}")
                await self.db.rollback()
                self.stats["errors"] += 1

        logger.info(
            f"\nâœ… å…³è”è¡¥å…¨å®Œæˆ: "
            f"æˆåŠŸ {self.stats['mistakes_processed']}/{total}, "
            f"è·³è¿‡ {self.stats['skipped']}, "
            f"å¤±è´¥ {self.stats['errors']}"
        )

    async def _regenerate_all_snapshots(self) -> None:
        """ä¸ºæ‰€æœ‰ç”¨æˆ·é‡æ–°ç”ŸæˆçŸ¥è¯†å›¾è°±å¿«ç…§"""
        logger.info("\nğŸ“¸ Step 3: é‡æ–°ç”Ÿæˆæ‰€æœ‰ç”¨æˆ·çŸ¥è¯†å›¾è°±å¿«ç…§...")

        # æŸ¥è¯¢æ‰€æœ‰ç”¨æˆ·-å­¦ç§‘ç»„åˆ
        stmt = select(KnowledgeMastery.user_id, KnowledgeMastery.subject).distinct()
        result = await self.db.execute(stmt)
        combinations = result.all()

        if not combinations:
            logger.warning("   âš ï¸  æœªå‘ç°çŸ¥è¯†æŒæ¡åº¦è®°å½•,è·³è¿‡å¿«ç…§ç”Ÿæˆ")
            return

        logger.info(f"   å‘ç° {len(combinations)} ä¸ªç”¨æˆ·-å­¦ç§‘ç»„åˆ")

        kg_service = KnowledgeGraphService(self.db)
        success_count = 0

        for i, (user_id, subject) in enumerate(combinations, 1):
            try:
                await kg_service.create_knowledge_graph_snapshot(
                    user_id=UUID(str(user_id)),
                    subject=subject,
                    period_type="backfill",
                    auto_commit=False,
                )
                success_count += 1
                logger.info(
                    f"   âœ… [{i}/{len(combinations)}] "
                    f"User={str(user_id)[:8]}... | Subject={subject}"
                )
            except Exception as e:
                logger.error(
                    f"   âŒ [{i}/{len(combinations)}] "
                    f"User={str(user_id)[:8]}... | Subject={subject} | "
                    f"Error={e}"
                )
                self.stats["errors"] += 1

        # ç»Ÿä¸€æäº¤
        try:
            await self.db.commit()
            self.stats["snapshots_generated"] = success_count
            logger.info(f"\n   ğŸ’¾ å¿«ç…§æ‰¹é‡æäº¤æˆåŠŸ: {success_count}/{len(combinations)}")
        except Exception as e:
            logger.error(f"   âŒ å¿«ç…§æ‰¹é‡æäº¤å¤±è´¥: {e}")
            await self.db.rollback()

    def _print_stats(self) -> None:
        """æ‰“å°ç»Ÿè®¡ä¿¡æ¯"""
        logger.info("\nğŸ“Š æ‰§è¡Œç»Ÿè®¡:")
        logger.info(f"   æ€»é”™é¢˜æ•°: {self.stats['total_mistakes']}")
        logger.info(f"   ç¼ºå°‘å…³è”: {self.stats['mistakes_without_assoc']}")
        logger.info(f"   æˆåŠŸå¤„ç†: {self.stats['mistakes_processed']}")
        logger.info(f"   åˆ›å»ºå…³è”: {self.stats['associations_created']}")
        logger.info(f"   ç”Ÿæˆå¿«ç…§: {self.stats['snapshots_generated']}")
        logger.info(f"   è·³è¿‡: {self.stats['skipped']}")
        logger.info(f"   é”™è¯¯: {self.stats['errors']}")

        if self.stats["mistakes_processed"] > 0:
            coverage = (
                self.stats["mistakes_processed"]
                / self.stats["mistakes_without_assoc"]
                * 100
            )
            logger.info(f"   è¡¥å…¨è¦†ç›–ç‡: {coverage:.1f}%")


async def main(dry_run: bool = False, batch_size: int = 100):
    """ä¸»å‡½æ•°"""
    async with AsyncSessionLocal() as db:
        backfiller = KnowledgeAssociationBackfiller(db, dry_run, batch_size)
        stats = await backfiller.run()

        # è¿”å›é€€å‡ºç 
        if stats["errors"] > 0:
            sys.exit(1)
        else:
            sys.exit(0)


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="å†å²é”™é¢˜çŸ¥è¯†ç‚¹å…³è”æ‰¹é‡è¡¥å…¨")
    parser.add_argument(
        "--dry-run", action="store_true", help="DRY-RUN æ¨¡å¼,ä»…æ£€æŸ¥ä¸æ‰§è¡Œ"
    )
    parser.add_argument(
        "--batch-size", type=int, default=100, help="æ‰¹æ¬¡å¤§å° (é»˜è®¤: 100)"
    )

    args = parser.parse_args()

    asyncio.run(main(dry_run=args.dry_run, batch_size=args.batch_size))
