"""
å­¦ä¹ é—®ç­”æœåŠ¡
åŸºäºç™¾ç‚¼AIçš„æ™ºèƒ½å­¦ä¹ åŠ©æ‰‹æœåŠ¡
"""

import asyncio
import json
import logging
import time
from datetime import datetime, timedelta
from typing import Any, Dict, List, Optional, Tuple
from uuid import UUID

from sqlalchemy import and_, desc, func, join, select
from sqlalchemy.exc import SQLAlchemyError
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy.orm import selectinload

from src.core.config import get_settings
from src.core.exceptions import (
    BailianServiceError,
    NotFoundError,
    ServiceError,
    ValidationError,
)
from src.models.homework import HomeworkSubmission
from src.models.learning import (
    Answer,
    ChatSession,
    LearningAnalytics,
    Question,
    SessionStatus,
)
from src.models.user import User
from src.repositories.base_repository import BaseRepository
from src.schemas.learning import (
    AnswerResponse,
    AskQuestionRequest,
    AskQuestionResponse,
    CreateSessionRequest,
    FeedbackRequest,
    HomeworkCorrectionResult,
    LearningAnalyticsResponse,
    QuestionCorrectionItem,
    QuestionHistoryQuery,
    QuestionResponse,
    SessionListQuery,
    SessionResponse,
)
from src.services.bailian_service import (
    AIContext,
    ChatMessage,
    MessageRole,
    get_bailian_service,
)
from src.services.formula_service import FormulaService
from src.utils.cache import cache_result
from src.utils.type_converters import (
    extract_orm_bool,
    extract_orm_int,
    extract_orm_str,
    extract_orm_uuid_str,
    safe_str,
)

logger = logging.getLogger("learning_service")
settings = get_settings()

# ========== ä½œä¸šæ‰¹æ”¹ Prompt å¸¸é‡ ==========

HOMEWORK_CORRECTION_PROMPT = """
ä½ æ˜¯ä¸€ä¸ªèµ„æ·±çš„æ•™è‚²å·¥ä½œè€…å’Œå­¦ç§‘ä¸“å®¶ï¼Œæ“…é•¿æ‰¹æ”¹å­¦ç”Ÿä½œä¸šã€‚

ç°åœ¨è¯·æ‰¹æ”¹å­¦ç”Ÿæäº¤çš„ä½œä¸šã€‚è¯·æŒ‰ç…§ä»¥ä¸‹è¦æ±‚è¿›è¡Œæ‰¹æ”¹ï¼š

1. **é€é¢˜åˆ†æ**ï¼šå¯¹æ¯ä¸€é“é¢˜ç›®è¿›è¡Œä»”ç»†åˆ†æ
2. **å‡†ç¡®åˆ¤æ–­**ï¼šåˆ¤æ–­å­¦ç”Ÿç­”æ¡ˆæ˜¯å¦æ­£ç¡®ã€æ˜¯å¦é—æ¼
3. **é”™è¯¯åˆ†ç±»**ï¼šå¯¹é”™è¯¯ç­”æ¡ˆè¿›è¡Œåˆ†ç±»ï¼ˆå¦‚è®¡ç®—é”™è¯¯ã€æ¦‚å¿µé”™è¯¯ã€ç†è§£é”™è¯¯ã€å•ä½é”™è¯¯ç­‰ï¼‰
4. **çŸ¥è¯†ç‚¹æå–**ï¼šæå–æ¯é¢˜æ¶‰åŠçš„æ ¸å¿ƒçŸ¥è¯†ç‚¹
5. **è¯¦ç»†è§£æ**ï¼šç»™å‡ºæ¸…æ™°çš„è§£é¢˜æ€è·¯å’Œæ­£ç¡®ç­”æ¡ˆ

è¯·è¿”å›ä¸¥æ ¼çš„ JSON æ ¼å¼çš„ç»“æœï¼Œæ ¼å¼å¦‚ä¸‹ï¼ˆå¿…é¡»æ˜¯æœ‰æ•ˆçš„ JSONï¼‰ï¼š

{{
  "corrections": [
    {{
      "question_number": <é¢˜å·>,
      "question_type": "<é¢˜ç›®ç±»å‹ï¼Œå¦‚é€‰æ‹©é¢˜ã€å¡«ç©ºé¢˜ã€è§£ç­”é¢˜ç­‰>",
      "is_unanswered": <æ˜¯å¦æœªä½œç­”ï¼Œtrue/false>,
      "student_answer": "<å­¦ç”Ÿç­”æ¡ˆï¼Œå¦‚æœæœªä½œç­”åˆ™ä¸ºnull>",
      "correct_answer": "<æ­£ç¡®ç­”æ¡ˆ>",
      "error_type": "<é”™è¯¯ç±»å‹ï¼Œå¦‚è®¡ç®—é”™è¯¯ã€æ¦‚å¿µé”™è¯¯ç­‰ï¼Œå¦‚æœæ­£ç¡®åˆ™ä¸ºnull>",
      "explanation": "<æ‰¹æ”¹è¯´æ˜å’Œè§£æè¿‡ç¨‹>",
      "knowledge_points": ["<çŸ¥è¯†ç‚¹1>", "<çŸ¥è¯†ç‚¹2>"],
      "score": <è¯¥é¢˜å¾—åˆ†ç™¾åˆ†æ¯”ï¼Œ0-100>
    }}
  ],
  "summary": "<ä½œä¸šæ€»ä½“è¯„è¯­ï¼ŒåŒ…æ‹¬å­¦ç”Ÿçš„ä¼˜ç‚¹å’Œéœ€è¦æ”¹è¿›çš„åœ°æ–¹>",
  "overall_score": <æ•´ä»½ä½œä¸šå¾—åˆ†ç™¾åˆ†æ¯”ï¼Œ0-100>,
  "total_questions": <é¢˜ç›®æ€»æ•°>,
  "unanswered_count": <æœªä½œç­”é¢˜æ•°>,
  "error_count": <å‡ºé”™é¢˜æ•°>
}}

æ³¨æ„ï¼š
- å¿…é¡»è¿”å›æœ‰æ•ˆçš„ JSON æ ¼å¼
- é¢˜å·åº”ä» 1 å¼€å§‹
- å¯¹äºæ­£ç¡®ç­”æ¡ˆï¼Œerror_type åº”ä¸º null
- å¯¹äºæœªä½œç­”çš„é¢˜ç›®ï¼Œis_unanswered åº”ä¸º trueï¼Œstudent_answer ä¸º null
- score åº”è¯¥åæ˜ è¯¥é¢˜çš„æ­£ç¡®ç¨‹åº¦ï¼ˆ0 è¡¨ç¤ºå®Œå…¨é”™è¯¯æˆ–æœªä½œç­”ï¼Œ100 è¡¨ç¤ºå®Œå…¨æ­£ç¡®ï¼‰
- çŸ¥è¯†ç‚¹åº”è¯¥å…·ä½“æ˜ç¡®ï¼Œæœ€å¤š 3 ä¸ª
- å­¦ç§‘ï¼š{subject}
"""


class LearningService:
    """å­¦ä¹ é—®ç­”æœåŠ¡"""

    def __init__(self, db: AsyncSession):
        self.db = db
        self.bailian_service = get_bailian_service()
        self.formula_service = FormulaService()  # åˆå§‹åŒ–å…¬å¼æœåŠ¡

        # åˆå§‹åŒ–ä»“å‚¨
        self.session_repo = BaseRepository(ChatSession, db)
        self.question_repo = BaseRepository(Question, db)
        self.answer_repo = BaseRepository(Answer, db)
        self.analytics_repo = BaseRepository(LearningAnalytics, db)

    # ========== é—®ç­”æ ¸å¿ƒåŠŸèƒ½ ==========

    async def ask_question(
        self, user_id: str, request: AskQuestionRequest
    ) -> AskQuestionResponse:
        """
        æé—®åŠŸèƒ½

        Args:
            user_id: ç”¨æˆ·ID
            request: æé—®è¯·æ±‚

        Returns:
            AskQuestionResponse: é—®ç­”å“åº”
        """
        start_time = time.time()

        try:
            # 1. è·å–æˆ–åˆ›å»ºä¼šè¯
            session = await self._get_or_create_session(user_id, request)
            session_id_str = extract_orm_uuid_str(session, "id")  # ğŸ”§ ç«‹å³æå–ID

            # 2. ä¿å­˜é—®é¢˜
            question = await self._save_question(user_id, session_id_str, request)
            question_id_str = extract_orm_uuid_str(question, "id")  # ğŸ”§ ç«‹å³æå–ID

            # 3. æ„å»ºAIä¸Šä¸‹æ–‡
            ai_context = await self._build_ai_context(
                user_id, session, request.use_context
            )

            # 4. æ„å»ºå¯¹è¯æ¶ˆæ¯
            messages = await self._build_conversation_messages(
                session_id_str,
                request,
                ai_context,
                request.include_history,
                request.max_history,
            )

            # 5. è°ƒç”¨AIç”Ÿæˆç­”æ¡ˆ
            # Convert ChatMessage objects to dict format if needed
            message_dicts = []
            for msg in messages:
                if hasattr(msg, "role") and hasattr(msg, "content"):
                    msg_dict: Dict[str, Any] = {
                        "role": msg.role.value,
                        "content": msg.content,
                    }
                    # å¦‚æœæœ‰å›¾ç‰‡URLsï¼Œæ·»åŠ åˆ°å­—å…¸ä¸­
                    if hasattr(msg, "image_urls") and msg.image_urls:
                        msg_dict["image_urls"] = msg.image_urls
                        logger.info(
                            f"ğŸ–¼ï¸ æ¶ˆæ¯åŒ…å«å›¾ç‰‡: role={msg.role.value}, "
                            f"image_count={len(msg.image_urls)}, "
                            f"urls={msg.image_urls}"
                        )
                    message_dicts.append(msg_dict)
                else:
                    message_dicts.append(msg)

            # ğŸ” æœ€ç»ˆè°ƒè¯•ï¼šæ‰“å°å®Œæ•´çš„message_dicts
            messages_summary = [
                {
                    "role": m.get("role"),
                    "has_images": bool(m.get("image_urls")),
                    "image_count": len(m.get("image_urls", [])),
                }
                for m in message_dicts
            ]
            logger.info(
                f"ğŸ“¤ å‡†å¤‡è°ƒç”¨AI: message_count={len(message_dicts)}, "
                f"messages_with_images={sum(1 for m in messages_summary if m['has_images'])}, "
                f"total_images={sum(m['image_count'] for m in messages_summary)}"
            )
            logger.info(
                f"ğŸ“‹ æ¶ˆæ¯è¯¦æƒ…: {json.dumps(messages_summary, ensure_ascii=False)}"
            )

            ai_response = await self.bailian_service.chat_completion(
                messages=message_dicts,
                context=ai_context,
                max_tokens=settings.AI_MAX_TOKENS,
                temperature=settings.AI_TEMPERATURE,
                top_p=settings.AI_TOP_P,
            )

            if not ai_response.success:
                raise BailianServiceError(f"AIè°ƒç”¨å¤±è´¥: {ai_response.error_message}")

            # 6. ä¿å­˜ç­”æ¡ˆ
            answer = await self._save_answer(question_id_str, ai_response)
            answer_id_str = extract_orm_uuid_str(answer, "id")  # ğŸ”§ ç«‹å³æå–ID

            # 7. æ›´æ–°ä¼šè¯ç»Ÿè®¡
            await self._update_session_stats(session_id_str, ai_response.tokens_used)

            # 8. æ›´æ–°ç”¨æˆ·å­¦ä¹ åˆ†æ
            await self._update_learning_analytics(user_id, question, answer)

            # ğŸ¯ 9. ä½œä¸šæ‰¹æ”¹ä¸“ç”¨é€»è¾‘ï¼ˆæ–°å¢ï¼‰
            correction_result = None
            mistakes_created_count = 0
            try:
                # 9.1 æ£€æµ‹æ˜¯å¦ä¸ºä½œä¸šæ‰¹æ”¹åœºæ™¯
                is_correction_scenario = self._is_homework_correction_scenario(
                    request.question_type.value if request.question_type else None,
                    extract_orm_str(question, "content") or "",
                    request.image_urls,
                )

                # ğŸ” [è°ƒè¯•] è¯¦ç»†æ—¥å¿—
                logger.info(
                    f"ğŸ” æ‰¹æ”¹åœºæ™¯æ£€æµ‹: is_correction={is_correction_scenario}, "
                    f"question_type={request.question_type}, "
                    f"content='{extract_orm_str(question, 'content') or ''}', "
                    f"has_images={bool(request.image_urls)}, "
                    f"image_count={len(request.image_urls or [])}"
                )

                if is_correction_scenario:
                    logger.info("ğŸ“ æ£€æµ‹åˆ°ä½œä¸šæ‰¹æ”¹åœºæ™¯ï¼Œå¯åŠ¨ä¸“ç”¨é€»è¾‘")

                    # 9.2 è°ƒç”¨AIè¿›è¡Œæ‰¹æ”¹
                    subject = extract_orm_str(request, "subject") or "math"
                    user_hint = extract_orm_str(question, "content")

                    correction_result = await self._call_ai_for_homework_correction(
                        image_urls=request.image_urls or [],
                        subject=subject,
                        user_hint=user_hint,
                    )

                    # 9.3 å¦‚æœæ‰¹æ”¹æˆåŠŸï¼Œé€é¢˜åˆ›å»ºé”™é¢˜
                    if correction_result:
                        (
                            mistakes_created_count,
                            mistake_list,
                        ) = await self._create_mistakes_from_correction(
                            user_id=user_id,
                            correction_result=correction_result,
                            subject=subject,
                            image_urls=request.image_urls or [],
                        )
                        logger.info(
                            f"âœ… ä½œä¸šæ‰¹æ”¹å®Œæˆ: åˆ›å»º {mistakes_created_count} ä¸ªé”™é¢˜"
                        )
            except Exception as correction_err:
                logger.warning(f"ä½œä¸šæ‰¹æ”¹å¤±è´¥ï¼Œä½†ä¸å½±å“é—®ç­”: {str(correction_err)}")

            # ğŸ¯ 10. æ™ºèƒ½é”™é¢˜è‡ªåŠ¨åˆ›å»ºï¼ˆç®€åŒ–è§„åˆ™ç‰ˆï¼‰
            # å¦‚æœä¸æ˜¯æ‰¹æ”¹åœºæ™¯ï¼Œä½¿ç”¨åŸæœ‰é€»è¾‘
            mistake_created = False
            mistake_info = None
            if not correction_result:  # åªåœ¨éæ‰¹æ”¹åœºæ™¯æ‰§è¡Œ
                try:
                    mistake_result = await self._auto_create_mistake_if_needed(
                        user_id, question, answer, request
                    )
                    if mistake_result:
                        mistake_created = True
                        mistake_info = mistake_result
                        logger.info(
                            f"âœ… é”™é¢˜è‡ªåŠ¨åˆ›å»ºæˆåŠŸ: user_id={user_id}, "
                            f"mistake_id={mistake_info.get('id')}, "
                            f"category={mistake_info.get('category')}"
                        )
                except Exception as mistake_err:
                    logger.warning(f"é”™é¢˜åˆ›å»ºå¤±è´¥ï¼Œä½†ä¸å½±å“é—®ç­”: {str(mistake_err)}")

            # 11. æ„å»ºå“åº”
            processing_time = int((time.time() - start_time) * 1000)

            # ğŸ”§ é‡æ–°æŸ¥è¯¢å¯¹è±¡ä»¥ç¡®ä¿æ‰€æœ‰å±æ€§å·²åŠ è½½ï¼ˆé¿å… MissingGreenlet é”™è¯¯ï¼‰
            # ä½¿ç”¨ä¹‹å‰æå–çš„IDæ¥é¿å…è§¦å‘æ‡’åŠ è½½
            question_stmt = select(Question).where(Question.id == question_id_str)
            question_result = await self.db.execute(question_stmt)
            question = question_result.scalar_one()

            answer_stmt = select(Answer).where(Answer.id == answer_id_str)
            answer_result = await self.db.execute(answer_stmt)
            answer = answer_result.scalar_one()

            session_stmt = select(ChatSession).where(ChatSession.id == session_id_str)
            session_result = await self.db.execute(session_stmt)
            session = session_result.scalar_one()

            return AskQuestionResponse(
                question=QuestionResponse.model_validate(question),
                answer=AnswerResponse.model_validate(answer),
                session=SessionResponse.model_validate(session),
                processing_time=processing_time,
                tokens_used=ai_response.tokens_used,
                mistake_created=mistake_created,  # ğŸ¯ ç®€åŒ–è§„åˆ™åˆ›å»º
                mistake_info=mistake_info,  # ğŸ¯ ç®€åŒ–è§„åˆ™ä¿¡æ¯
                correction_result=correction_result,  # ğŸ¯ æ‰¹æ”¹ç»“æœ
                mistakes_created=mistakes_created_count,  # ğŸ¯ æ‰¹æ”¹åˆ›å»ºçš„é”™é¢˜æ•°
            )

        except Exception as e:
            logger.error(
                f"æé—®å¤„ç†å¤±è´¥: {str(e)}", extra={"user_id": user_id}, exc_info=True
            )

            # æ›´æ–°é—®é¢˜çŠ¶æ€ä¸ºå¤±è´¥
            try:
                # å®‰å…¨åœ°è·å–questionå˜é‡
                question_var = locals().get("question")
                if question_var is not None:
                    await self.question_repo.update(
                        extract_orm_uuid_str(question_var, "id"),
                        {"is_processed": False},
                    )
            except SQLAlchemyError as db_error:
                logger.warning(f"æ¸…ç†é—®é¢˜çŠ¶æ€å¤±è´¥: {db_error}")
                pass  # Ignore update errors during exception handling

            raise ServiceError(f"æé—®å¤„ç†å¤±è´¥: {str(e)}") from e

    async def _stream_with_keepalive(self, source_stream, keepalive_interval: int = 5):
        """
        ä¸ºæµæ·»åŠ  keepalive å¿ƒè·³ï¼Œé˜²æ­¢é•¿æ—¶é—´æ— æ¶ˆæ¯å¯¼è‡´å‰ç«¯è¶…æ—¶

        Args:
            source_stream: åŸå§‹æµç”Ÿæˆå™¨
            keepalive_interval: å¿ƒè·³é—´éš”ï¼ˆç§’ï¼‰

        Yields:
            ä»æºæµäº§ç”Ÿçš„æ•°æ®æˆ–å¿ƒè·³ä¿¡å·
        """
        import asyncio

        last_yield_time = asyncio.get_event_loop().time()

        try:
            async for chunk in source_stream:
                # å‘é€çœŸå®æ•°æ®å—
                yield chunk
                last_yield_time = asyncio.get_event_loop().time()
        except asyncio.TimeoutError:
            # å¦‚æœæºæµè¶…æ—¶ï¼Œå‘é€å¿ƒè·³è®©å‰ç«¯çŸ¥é“æˆ‘ä»¬è¿˜åœ¨å¤„ç†
            while True:
                current_time = asyncio.get_event_loop().time()
                time_since_last_yield = current_time - last_yield_time

                if time_since_last_yield > keepalive_interval:
                    # å‘é€å¿ƒè·³ä¿¡å·ï¼Œä¿æŒè¿æ¥æ´»è·ƒ
                    yield {
                        "type": "keepalive",
                        "content": "",
                        "full_content": "",
                    }
                    logger.debug(
                        f"ğŸ“¡ å‘é€ keepalive å¿ƒè·³ ({int(time_since_last_yield)}s)"
                    )
                    last_yield_time = current_time

                await asyncio.sleep(1)

    async def ask_question_stream(self, user_id: str, request: AskQuestionRequest):
        """
        æµå¼æé—®åŠŸèƒ½

        Args:
            user_id: ç”¨æˆ·ID
            request: æé—®è¯·æ±‚

        Yields:
            dict: SSE æ ¼å¼çš„å¢é‡å“åº”
                - type: "content" | "done" | "error"
                - content: å¢é‡æ–‡æœ¬å†…å®¹
                - full_content: ç´¯ç§¯çš„å®Œæ•´å†…å®¹
                - finish_reason: å®ŒæˆåŸå›  (null | "stop")
                - question_id: é—®é¢˜ID (ä»…åœ¨ type="done" æ—¶)
                - answer_id: ç­”æ¡ˆID (ä»…åœ¨ type="done" æ—¶)
                - usage: tokenä½¿ç”¨æƒ…å†µ (ä»…åœ¨ type="done" æ—¶)
        """
        question = None
        session = None
        full_answer_content = ""

        try:
            # 1. è·å–æˆ–åˆ›å»ºä¼šè¯
            session = await self._get_or_create_session(user_id, request)
            session_id = extract_orm_uuid_str(session, "id")

            # 2. ä¿å­˜é—®é¢˜ï¼ˆçŠ¶æ€ä¸ºæœªå¤„ç†ï¼‰
            question = await self._save_question(user_id, session_id, request)
            question_id = extract_orm_uuid_str(question, "id")

            # 3. æ„å»ºAIä¸Šä¸‹æ–‡
            ai_context = await self._build_ai_context(
                user_id, session, request.use_context
            )

            # 4. æ„å»ºå¯¹è¯æ¶ˆæ¯
            messages = await self._build_conversation_messages(
                session_id,
                request,
                ai_context,
                request.include_history,
                request.max_history,
            )

            # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼
            message_dicts = []
            for msg in messages:
                if hasattr(msg, "role") and hasattr(msg, "content"):
                    msg_dict: Dict[str, Any] = {
                        "role": msg.role.value,
                        "content": msg.content,
                    }
                    if hasattr(msg, "image_urls") and msg.image_urls:
                        msg_dict["image_urls"] = msg.image_urls
                    message_dicts.append(msg_dict)
                else:
                    message_dicts.append(msg)

            # 4. æµå¼è°ƒç”¨AIï¼ˆæ”¯æŒå›¾ç‰‡å’Œæ–‡æœ¬ï¼‰
            logger.info(
                f"å¼€å§‹æµå¼è°ƒç”¨ - æ¶ˆæ¯æ•°: {len(message_dicts)}, "
                f"å½“å‰è¯·æ±‚å›¾ç‰‡: {len(request.image_urls or [])}"
            )

            async for chunk in self.bailian_service.chat_completion_stream(
                messages=message_dicts,
                context=ai_context,
                max_tokens=settings.AI_MAX_TOKENS,
                temperature=settings.AI_TEMPERATURE,
                top_p=settings.AI_TOP_P,
            ):
                # ğŸ”§ é˜²å¾¡æ€§æ£€æŸ¥ï¼šç¡®ä¿ chunk ä¸ä¸º None
                if chunk is None:
                    logger.warning("æ”¶åˆ° None chunkï¼Œè·³è¿‡å¤„ç†")
                    continue

                # ğŸ“ è°ƒè¯•ï¼šæ‰“å°æ¯ä¸ª chunk çš„ä¿¡æ¯ï¼ˆä½¿ç”¨ debug çº§åˆ«ï¼Œå‡å°‘æ—¥å¿— I/Oï¼‰
                logger.debug(
                    f"ğŸ“¦ æ”¶åˆ° chunk: content_len={len(chunk.get('content', ''))}, finish_reason={chunk.get('finish_reason')}"
                )

                # ç´¯ç§¯å®Œæ•´å†…å®¹
                if chunk.get("content"):
                    full_answer_content = chunk.get("full_content", "")

                # å‘é€å¢é‡å†…å®¹
                yield {
                    "type": "content",
                    "content": chunk.get("content", ""),
                    "full_content": full_answer_content,
                    "finish_reason": chunk.get("finish_reason"),
                }

                # æµå¼å®Œæˆåä¿å­˜æ•°æ®
                if chunk.get("finish_reason") == "stop":
                    logger.info("âœ… æµå¼ç”Ÿæˆå®Œæˆï¼Œå¼€å§‹åå¤„ç†")

                    # ğŸ”§ 5.5 ç«‹å³å‘é€"å†…å®¹æ¥æ”¶å®Œæˆ"ä¿¡å·ï¼Œä¸é˜»å¡å‰ç«¯
                    yield {
                        "type": "content_finished",
                        "full_content": full_answer_content,
                        "finish_reason": "stop",
                    }
                    logger.info("ğŸ“¤ å·²å‘é€ content_finished ä¿¡å·ç»™å‰ç«¯")

                    # 6. ä¿å­˜ç­”æ¡ˆï¼ˆæœ€å°å¿…è¦æ“ä½œï¼Œå¿«é€Ÿå®Œæˆï¼‰
                    try:
                        answer_data = {
                            "question_id": question_id,
                            "content": full_answer_content,
                            "tokens_used": chunk.get("usage", {}).get(
                                "total_tokens", 0
                            ),
                            "model_name": chunk.get("model", "qwen-turbo"),
                        }
                        answer = await self.answer_repo.create(answer_data)
                        answer_id = extract_orm_uuid_str(answer, "id")

                        # 7. æ›´æ–°é—®é¢˜çŠ¶æ€
                        await self.question_repo.update(
                            question_id,
                            {"is_processed": True},
                        )

                        # 8. æ›´æ–°ä¼šè¯ç»Ÿè®¡
                        tokens_used = chunk.get("usage", {}).get("total_tokens", 0)
                        await self._update_session_stats(session_id, tokens_used)

                        logger.info(f"âœ… æ ¸å¿ƒæ•°æ®ä¿å­˜å®Œæˆ: answer_id={answer_id}")

                        # ğŸ”§ æäº¤äº‹åŠ¡ï¼Œç¡®ä¿æ ¸å¿ƒæ•°æ®ç«‹å³æŒä¹…åŒ–ï¼ˆä¿®å¤1ï¼‰
                        await self.db.commit()
                        logger.info("ğŸ’¾ æ ¸å¿ƒæ•°æ®äº‹åŠ¡å·²æäº¤")
                    except Exception as save_err:
                        logger.error(f"ä¿å­˜ç­”æ¡ˆå¤±è´¥: {save_err}", exc_info=True)
                        await self.db.rollback()
                        yield {
                            "type": "error",
                            "message": f"ä¿å­˜ç­”æ¡ˆå¤±è´¥: {str(save_err)}",
                        }
                        return

                    # 9. ç«‹å³å‘é€ done äº‹ä»¶ï¼ˆå…³é”®ä¿®å¤ï¼ï¼‰
                    done_event = {
                        "type": "done",
                        "question_id": question_id,
                        "answer_id": answer_id,
                        "session_id": session_id,
                        "usage": chunk.get("usage", {}),
                        "full_content": full_answer_content,
                    }
                    yield done_event
                    logger.info("ğŸ“¤ å·²å‘é€ done äº‹ä»¶ï¼Œå‰ç«¯æµå¼å“åº”å®Œæˆ")

                    # ğŸ”§ 10. åå°å¼‚æ­¥å¤„ç†é•¿æ—¶é—´æ“ä½œï¼ˆä¸é˜»å¡å‰ç«¯ï¼‰
                    asyncio.create_task(
                        self._background_post_processing(
                            user_id=user_id,
                            question_id=question_id,
                            answer_id=answer_id,
                            full_answer_content=full_answer_content,
                            request=request,
                            question=question,
                            chunk=chunk,
                        )
                    )
                    logger.info("ğŸ”„ åå°å¤„ç†ä»»åŠ¡å·²åˆ›å»º")

        except BailianServiceError as e:
            logger.error(f"AIæœåŠ¡è°ƒç”¨å¤±è´¥: {e}")
            yield {"type": "error", "message": f"AIæœåŠ¡æš‚æ—¶ä¸å¯ç”¨: {str(e)}"}
        except Exception as e:
            logger.error(f"æµå¼æé—®å¤„ç†å¤±è´¥: {e}", exc_info=True)

            # æ›´æ–°é—®é¢˜çŠ¶æ€ä¸ºå¤±è´¥
            if question:
                try:
                    await self.question_repo.update(
                        extract_orm_uuid_str(question, "id"),
                        {"is_processed": False},
                    )
                except SQLAlchemyError as db_error:
                    logger.warning(f"æ¸…ç†é—®é¢˜çŠ¶æ€å¤±è´¥: {db_error}")
                    pass

            yield {"type": "error", "message": f"æé—®å¤„ç†å¤±è´¥: {str(e)}"}

    async def _background_post_processing(
        self,
        user_id: str,
        question_id: str,
        answer_id: str,
        full_answer_content: str,
        request: AskQuestionRequest,
        question: Question,
        chunk: Dict[str, Any],
    ) -> None:
        """
        åå°å¤„ç†é•¿æ—¶é—´æ“ä½œï¼ˆä¸é˜»å¡ WebSocket å‰ç«¯å“åº”ï¼‰

        åŒ…æ‹¬ï¼š
        - å…¬å¼å¢å¼º
        - å­¦ä¹ åˆ†ææ›´æ–°
        - ä½œä¸šæ‰¹æ”¹å’Œé”™é¢˜åˆ›å»º
        """
        try:
            logger.info(
                f"ğŸ”„ [åå°] å¼€å§‹åå¤„ç†: user_id={user_id}, answer_id={answer_id}"
            )

            # 1. å…¬å¼å¢å¼ºï¼ˆå¯é€‰ï¼‰
            enhanced_content = full_answer_content
            try:
                processed_content = (
                    await self.formula_service.process_text_with_formulas(
                        full_answer_content
                    )
                )
                if processed_content and processed_content != full_answer_content:
                    enhanced_content = processed_content
                    logger.info(
                        f"âœ… [åå°] å…¬å¼å¢å¼ºæˆåŠŸï¼Œå†…å®¹é•¿åº¦: {len(enhanced_content)}"
                    )
                    # æ›´æ–°ç­”æ¡ˆå†…å®¹
                    await self.answer_repo.update(
                        answer_id, {"content": enhanced_content}
                    )
            except Exception as formula_err:
                logger.warning(f"[åå°] å…¬å¼å¢å¼ºå¤±è´¥: {str(formula_err)}")

            # 2. æ›´æ–°å­¦ä¹ åˆ†æ
            try:
                answer = await self.answer_repo.get_by_id(answer_id)
                await self._update_learning_analytics(user_id, question, answer)
                logger.info("âœ… [åå°] å­¦ä¹ åˆ†ææ›´æ–°å®Œæˆ")
            except Exception as analytics_err:
                logger.warning(f"[åå°] å­¦ä¹ åˆ†ææ›´æ–°å¤±è´¥: {str(analytics_err)}")

            # 3. ä½œä¸šæ‰¹æ”¹å’Œé”™é¢˜åˆ›å»º
            try:
                is_correction_scenario = self._is_homework_correction_scenario(
                    request.question_type.value if request.question_type else None,
                    extract_orm_str(question, "content") or "",
                    request.image_urls,
                )

                logger.info(
                    f"ğŸ” [åå°] æ‰¹æ”¹åœºæ™¯æ£€æµ‹: is_correction={is_correction_scenario}"
                )

                if is_correction_scenario:
                    logger.info("ğŸ“ [åå°] å¯åŠ¨ä½œä¸šæ‰¹æ”¹é€»è¾‘")

                    subject = extract_orm_str(request, "subject") or "math"
                    user_hint = extract_orm_str(question, "content")

                    correction_result = await self._call_ai_for_homework_correction(
                        image_urls=request.image_urls or [],
                        subject=subject,
                        user_hint=user_hint,
                    )

                    if correction_result:
                        (
                            mistakes_created_count,
                            _,
                        ) = await self._create_mistakes_from_correction(
                            user_id=user_id,
                            correction_result=correction_result,
                            subject=subject,
                            image_urls=request.image_urls or [],
                        )
                        await self.db.commit()
                        logger.info(
                            f"âœ… [åå°] ä½œä¸šæ‰¹æ”¹å®Œæˆ: åˆ›å»º {mistakes_created_count} ä¸ªé”™é¢˜"
                        )
                else:
                    # 4. éæ‰¹æ”¹åœºæ™¯çš„é”™é¢˜è‡ªåŠ¨åˆ›å»º
                    try:
                        answer = await self.answer_repo.get_by_id(answer_id)
                        mistake_result = await self._auto_create_mistake_if_needed(
                            user_id, question, answer, request
                        )
                        if mistake_result:
                            logger.info(
                                f"âœ… [åå°] é”™é¢˜è‡ªåŠ¨åˆ›å»ºæˆåŠŸ: "
                                f"mistake_id={mistake_result.get('id')}"
                            )
                    except Exception as mistake_err:
                        logger.warning(f"[åå°] é”™é¢˜åˆ›å»ºå¤±è´¥: {str(mistake_err)}")
            except Exception as correction_err:
                logger.warning(f"[åå°] ä½œä¸šæ‰¹æ”¹å¤±è´¥: {str(correction_err)}")

            logger.info(f"âœ… [åå°] åå¤„ç†å®Œæˆ: answer_id={answer_id}")

        except Exception as e:
            logger.error(f"âŒ [åå°] åå¤„ç†å¤±è´¥: {str(e)}", exc_info=True)

    async def _get_or_create_session(
        self, user_id: str, request: AskQuestionRequest
    ) -> ChatSession:
        """è·å–æˆ–åˆ›å»ºä¼šè¯"""
        if request.session_id:
            # å°è¯•è·å–ç°æœ‰ä¼šè¯
            try:
                session = await self.session_repo.get_by_id(request.session_id)
                if session and extract_orm_uuid_str(session, "user_id") == user_id:
                    # ä¼šè¯å­˜åœ¨ä¸”å±äºå½“å‰ç”¨æˆ·
                    if (
                        extract_orm_bool(session, "status")
                        == SessionStatus.ACTIVE.value
                    ):
                        # æ›´æ–°æœ€åæ´»è·ƒæ—¶é—´
                        await self.session_repo.update(
                            extract_orm_uuid_str(session, "id"),
                            {"last_active_at": datetime.now().isoformat()},
                        )
                    return session
            except Exception as e:
                # è·å–ä¼šè¯å¤±è´¥ï¼Œè®°å½•æ—¥å¿—ä½†ä¸ä¸­æ–­ï¼Œç»§ç»­åˆ›å»ºæ–°ä¼šè¯
                print(
                    f"è·å–ä¼šè¯å¤±è´¥ï¼Œå°†åˆ›å»ºæ–°ä¼šè¯: session_id={request.session_id}, error={str(e)}"
                )

        # åˆ›å»ºæ–°ä¼šè¯ï¼ˆåŸæ¥çš„session_idä¸å­˜åœ¨æˆ–è·å–å¤±è´¥ï¼‰
        session_title = await self._generate_session_title(request.content)
        session_data = {
            "user_id": user_id,
            "title": session_title,
            "subject": request.subject.value if request.subject else None,
            "status": SessionStatus.ACTIVE.value,
            "context_enabled": request.use_context,
            "last_active_at": datetime.utcnow().isoformat(),
        }
        return await self.session_repo.create(session_data)

    async def _generate_session_title(self, first_question: str) -> str:
        """ç”Ÿæˆä¼šè¯æ ‡é¢˜"""
        # ç®€å•çš„æ ‡é¢˜ç”Ÿæˆé€»è¾‘ï¼Œå–é—®é¢˜å‰30ä¸ªå­—ç¬¦
        title = first_question[:30]
        if len(first_question) > 30:
            title += "..."
        return title

    async def _save_question(
        self, user_id: str, session_id: str, request: AskQuestionRequest
    ) -> Question:
        """ä¿å­˜é—®é¢˜"""
        question_data = {
            "session_id": session_id,
            "user_id": user_id,
            "content": request.content,
            "question_type": (
                request.question_type.value if request.question_type else None
            ),
            "subject": request.subject.value if request.subject else None,
            "topic": request.topic,
            "difficulty_level": (
                request.difficulty_level.value if request.difficulty_level else None
            ),
            "context_data": (
                json.dumps(request.context_data) if request.context_data else None
            ),
            "has_images": bool(request.image_urls),
            "image_urls": (
                json.dumps(request.image_urls) if request.image_urls else None
            ),
            "is_processed": False,
        }
        return await self.question_repo.create(question_data)

    async def _build_ai_context(
        self, user_id: str, session: ChatSession, use_context: bool = True
    ) -> AIContext:
        """æ„å»ºAIè°ƒç”¨ä¸Šä¸‹æ–‡ï¼Œé›†æˆMCPä¸ªæ€§åŒ–å­¦æƒ…åˆ†æ"""
        context = AIContext(
            user_id=user_id,
            subject=extract_orm_str(session, "subject"),
            session_id=extract_orm_uuid_str(session, "id"),
        )

        if use_context:
            # è·å–ç”¨æˆ·ä¿¡æ¯
            user_stmt = select(User).where(User.id == user_id)
            user_result = await self.db.execute(user_stmt)
            user = user_result.scalar_one_or_none()

            if user:
                context.grade_level = self._parse_grade_level(
                    extract_orm_str(user, "grade_level")
                )
                context.metadata = {
                    "user_school": extract_orm_str(user, "school"),
                    "user_class": extract_orm_str(user, "class_name"),
                    "learning_subjects": extract_orm_str(user, "study_subjects"),
                }

            # ğŸš€ NEW: é›†æˆ MCP ä¸ªæ€§åŒ–å­¦æƒ…ä¸Šä¸‹æ–‡
            try:
                from src.services.knowledge_context_builder import (
                    knowledge_context_builder,
                )

                # æ„å»ºç”¨æˆ·å­¦æƒ…ä¸Šä¸‹æ–‡
                learning_context = await knowledge_context_builder.build_context(
                    user_id=user_id,
                    subject=extract_orm_str(session, "subject"),
                    session_type="learning",
                )

                # å°†å­¦æƒ…åˆ†æç»“æœæ·»åŠ åˆ°AIä¸Šä¸‹æ–‡ä¸­
                if learning_context.weak_knowledge_points:
                    weak_points_summary = []
                    for point in learning_context.weak_knowledge_points[
                        :5
                    ]:  # å–å‰5ä¸ªæœ€ä¸¥é‡çš„
                        weak_points_summary.append(
                            {
                                "knowledge": point.knowledge_name,
                                "subject": point.subject,
                                "error_rate": round(point.error_rate * 100, 1),
                                "severity": round(point.severity_score * 100, 1),
                            }
                        )

                    context.metadata = context.metadata or {}
                    context.metadata.update(
                        {
                            "weak_knowledge_points": weak_points_summary,
                            "learning_pace": learning_context.learning_preferences.learning_pace,
                            "focus_duration": learning_context.learning_preferences.focus_duration,
                            "current_level": learning_context.context_summary.current_level,
                            "total_questions": learning_context.context_summary.total_questions,
                            "learning_streak": learning_context.context_summary.learning_streak,
                            "mcp_context_generated": True,
                        }
                    )

                    logger.info(
                        f"MCPä¸Šä¸‹æ–‡å·²æ„å»º - ç”¨æˆ·: {user_id}, è–„å¼±çŸ¥è¯†ç‚¹: {len(learning_context.weak_knowledge_points)}"
                    )
                else:
                    # æ–°ç”¨æˆ·æˆ–æ²¡æœ‰è¶³å¤Ÿæ•°æ®ï¼Œæ ‡è®°ä¸ºé¦–æ¬¡å­¦ä¹ 
                    context.metadata = context.metadata or {}
                    context.metadata.update(
                        {
                            "mcp_context_generated": True,
                            "is_new_learner": True,
                            "current_level": "beginner",
                        }
                    )
                    logger.info(f"MCPä¸Šä¸‹æ–‡å·²æ„å»º - æ–°å­¦ä¹ è€…: {user_id}")

            except Exception as e:
                logger.warning(f"MCPä¸Šä¸‹æ–‡æ„å»ºå¤±è´¥ï¼Œå›é€€åˆ°ä¼ ç»Ÿæ¨¡å¼: {str(e)}")
                # ç»§ç»­ä½¿ç”¨ä¼ ç»Ÿä¸Šä¸‹æ–‡ï¼Œä¸å½±å“ä¸»æµç¨‹
                context.metadata = context.metadata or {}
                context.metadata["mcp_context_failed"] = True

            # è·å–ç›¸å…³ä½œä¸šå†å²ï¼ˆä¿æŒåŸæœ‰é€»è¾‘ï¼‰
            homework_context = await self._get_homework_context(
                user_id, extract_orm_str(session, "subject")
            )
            if homework_context:
                context.metadata = context.metadata or {}
                context.metadata.update(homework_context)

        return context

    def _parse_grade_level(self, grade_level: Optional[str]) -> Optional[int]:
        """è§£æå­¦æ®µä¸ºæ•°å­—"""
        if not grade_level:
            return None

        grade_mapping = {
            "junior_1": 7,
            "junior_2": 8,
            "junior_3": 9,
            "senior_1": 10,
            "senior_2": 11,
            "senior_3": 12,
        }
        return grade_mapping.get(grade_level)

    async def _get_homework_context(
        self, user_id: str, subject: Optional[str]
    ) -> Optional[Dict[str, Any]]:
        """è·å–ä½œä¸šç›¸å…³ä¸Šä¸‹æ–‡"""
        try:
            # è·å–æœ€è¿‘çš„ä½œä¸šè®°å½•
            stmt = (
                select(HomeworkSubmission)
                .options(selectinload(HomeworkSubmission.reviews))
                .where(HomeworkSubmission.student_id == user_id)
                .order_by(desc(HomeworkSubmission.created_at))
                .limit(5)
            )

            if subject:
                stmt = stmt.where(HomeworkSubmission.subject == subject)

            result = await self.db.execute(stmt)
            submissions = result.scalars().all()

            if not submissions:
                return None

            # åˆ†æé”™é¢˜å’ŒçŸ¥è¯†ç‚¹
            wrong_topics = []
            mastered_topics = []

            for submission in submissions:
                for review in submission.reviews:
                    if hasattr(review, "knowledge_points") and review.knowledge_points:
                        points = json.loads(review.knowledge_points)
                        if review.score and review.score < 80:  # å‡è®¾80åˆ†ä»¥ä¸‹ä¸ºé”™é¢˜
                            wrong_topics.extend(points)
                        else:
                            mastered_topics.extend(points)

            return {
                "recent_homework_count": len(submissions),
                "weak_knowledge_points": list(set(wrong_topics))[:10],
                "strong_knowledge_points": list(set(mastered_topics))[:10],
            }

        except Exception as e:
            logger.warning(f"è·å–ä½œä¸šä¸Šä¸‹æ–‡å¤±è´¥: {str(e)}")
            return None

    async def _build_conversation_messages(
        self,
        session_id: str,
        request: AskQuestionRequest,
        context: AIContext,
        include_history: bool = True,
        max_history: int = 10,
    ) -> List[ChatMessage]:
        """æ„å»ºå¯¹è¯æ¶ˆæ¯"""
        messages = []

        # 1. ç³»ç»Ÿæç¤ºè¯
        system_prompt = await self._build_system_prompt(context)
        messages.append(ChatMessage(role=MessageRole.SYSTEM, content=system_prompt))

        # 2. å†å²å¯¹è¯
        if include_history and max_history > 0:
            history_messages = await self._get_conversation_history(
                session_id, max_history
            )
            messages.extend(history_messages)

        # 3. å½“å‰é—®é¢˜
        user_message = request.content

        # æ„å»ºç”¨æˆ·æ¶ˆæ¯ï¼Œæ”¯æŒå›¾ç‰‡
        if request.image_urls and len(request.image_urls) > 0:
            # æœ‰å›¾ç‰‡æ—¶ï¼Œåˆ›å»ºåŒ…å«å›¾ç‰‡çš„å¤šæ¨¡æ€æ¶ˆæ¯
            user_chat_message = ChatMessage(
                role=MessageRole.USER,
                content=user_message,
                image_urls=request.image_urls,
            )

            # æ·»åŠ å›¾ç‰‡æç¤ºåˆ°æ–‡æœ¬å†…å®¹
            user_message += f"\n\n[ç”¨æˆ·ä¸Šä¼ äº†{len(request.image_urls)}å¼ å›¾ç‰‡ï¼Œè¯·åˆ†æå›¾ç‰‡å†…å®¹å¹¶å›ç­”é—®é¢˜]"
            user_chat_message.content = user_message

            # ğŸ” è°ƒè¯•æ—¥å¿—ï¼šè®°å½•å›¾ç‰‡URL
            logger.info(
                f"æ„å»ºå¤šæ¨¡æ€æ¶ˆæ¯: session_id={session_id}, image_count={len(request.image_urls)}",
                extra={
                    "session_id": session_id,
                    "image_urls": request.image_urls,
                    "message_preview": user_message[:100],
                },
            )

        else:
            # çº¯æ–‡æœ¬æ¶ˆæ¯
            user_chat_message = ChatMessage(role=MessageRole.USER, content=user_message)

        messages.append(user_chat_message)

        return messages

    async def _build_system_prompt(self, context: AIContext) -> str:
        """
        æ„å»ºç³»ç»Ÿæç¤ºè¯ï¼ˆç®€åŒ–ç‰ˆï¼‰

        æ›´å¤æ‚çš„æç¤ºè¯é…ç½®è¯·åœ¨ç™¾ç‚¼å¹³å°çš„æ™ºèƒ½ä½“"ç³»ç»ŸæŒ‡ä»¤"ä¸­è®¾ç½®
        """
        prompt_parts = [
            "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„K12å­¦ä¹ åŠ©æ•™ï¼Œåå«'äº”å¥½åŠ©æ•™'ï¼Œä¸“é—¨å¸®åŠ©å°åˆé«˜ä¸­å­¦ç”Ÿè§£å†³å­¦ä¹ é—®é¢˜ã€‚",
            "",
            "ä½ çš„èŒè´£åŒ…æ‹¬ï¼š",
            "1. åªèƒ½å›ç­”å­¦ä¹ é—®é¢˜ï¼Œæä¾›æ¸…æ™°æ˜“æ‡‚çš„è§£é‡Š",
            "2. åˆ†æé¢˜ç›®ï¼Œæä¾›è¯¦ç»†çš„è§£é¢˜æ­¥éª¤",
            "3. é¼“åŠ±å­¦ç”Ÿç§¯æå­¦ä¹ ï¼Œå»ºç«‹å­¦ä¹ ä¿¡å¿ƒ",
        ]

        # æ·»åŠ ç”¨æˆ·ä¸Šä¸‹æ–‡ï¼ˆä¿ç•™ä¸ªæ€§åŒ–åŠŸèƒ½ï¼‰
        if context.grade_level:
            grade_name = self._get_grade_name(context.grade_level)
            prompt_parts.append(f"\nå­¦ç”Ÿå½“å‰å­¦æ®µï¼š{grade_name}")

        if context.subject:
            subject_name = self._get_subject_name(context.subject)
            prompt_parts.append(f"å½“å‰å­¦ç§‘ï¼š{subject_name}")

        if context.metadata:
            if context.metadata.get("user_school"):
                prompt_parts.append(f"å­¦ç”Ÿå­¦æ ¡ï¼š{context.metadata['user_school']}")

            if context.metadata.get("weak_knowledge_points"):
                weak_points = context.metadata["weak_knowledge_points"][:3]  # å–å‰3ä¸ª
                # weak_points æ˜¯ WeakKnowledgePoint å¯¹è±¡æˆ–å­—å…¸åˆ—è¡¨,éœ€è¦æå– knowledge_name
                if weak_points:
                    point_names = []
                    for point in weak_points:
                        if isinstance(point, dict):
                            point_names.append(point.get("knowledge_name", str(point)))
                        elif hasattr(point, "knowledge_name"):
                            point_names.append(point.knowledge_name)
                        else:
                            point_names.append(str(point))
                    if point_names:
                        prompt_parts.append(f"å­¦ç”Ÿè–„å¼±çŸ¥è¯†ç‚¹ï¼š{', '.join(point_names)}")

        prompt_parts.append("\nè¯·åŸºäºä»¥ä¸Šä¿¡æ¯ï¼Œä¸ºå­¦ç”Ÿæä¾›ä¸ªæ€§åŒ–çš„å­¦ä¹ æŒ‡å¯¼ã€‚")

        return "\n".join(prompt_parts)

    def _get_grade_name(self, grade_level: int) -> str:
        """è·å–å­¦æ®µåç§°"""
        grade_mapping = {
            7: "åˆä¸€",
            8: "åˆäºŒ",
            9: "åˆä¸‰",
            10: "é«˜ä¸€",
            11: "é«˜äºŒ",
            12: "é«˜ä¸‰",
        }
        return grade_mapping.get(grade_level, f"å­¦æ®µ{grade_level}")

    def _get_subject_name(self, subject: str) -> str:
        """è·å–å­¦ç§‘åç§°"""
        subject_mapping = {
            "math": "æ•°å­¦",
            "chinese": "è¯­æ–‡",
            "english": "è‹±è¯­",
            "physics": "ç‰©ç†",
            "chemistry": "åŒ–å­¦",
            "biology": "ç”Ÿç‰©",
            "history": "å†å²",
            "geography": "åœ°ç†",
            "politics": "æ”¿æ²»",
        }
        return subject_mapping.get(subject, subject)

    async def _get_conversation_history(
        self, session_id: str, max_count: int
    ) -> List[ChatMessage]:
        """è·å–å¯¹è¯å†å²"""
        try:
            # è·å–æœ€è¿‘çš„é—®ç­”å¯¹
            stmt = (
                select(Question)
                .options(selectinload(Question.answer))
                .where(Question.session_id == session_id, Question.is_processed)
                .order_by(desc(Question.created_at))
                .limit(max_count)
            )

            result = await self.db.execute(stmt)
            questions = result.scalars().all()

            messages = []

            # æŒ‰æ—¶é—´æ­£åºæ’åˆ—ï¼ˆæ—§çš„åœ¨å‰ï¼‰
            for question in reversed(questions):
                messages.append(
                    ChatMessage(
                        role=MessageRole.USER,
                        content=extract_orm_str(question, "content"),
                    )
                )

                if question.answer:
                    messages.append(
                        ChatMessage(
                            role=MessageRole.ASSISTANT,
                            content=extract_orm_str(question.answer, "content"),
                        )
                    )

            return messages

        except Exception as e:
            logger.warning(f"è·å–å¯¹è¯å†å²å¤±è´¥: {str(e)}")
            return []

    async def _save_answer(self, question_id: str, ai_response) -> Answer:
        """ä¿å­˜AIç­”æ¡ˆ"""
        # åˆ†æç­”æ¡ˆç”Ÿæˆæ¨èå†…å®¹
        related_topics, suggested_questions = await self._analyze_answer_content(
            ai_response.content
        )

        answer_data = {
            "question_id": question_id,
            "content": ai_response.content,
            "model_name": ai_response.model,
            "tokens_used": ai_response.tokens_used,
            "generation_time": int(ai_response.processing_time * 1000),
            "confidence_score": 85,  # é»˜è®¤ç½®ä¿¡åº¦ï¼Œåç»­å¯é€šè¿‡åˆ†ææ”¹è¿›
            "related_topics": json.dumps(related_topics) if related_topics else None,
            "suggested_questions": (
                json.dumps(suggested_questions) if suggested_questions else None
            ),
        }

        answer = await self.answer_repo.create(answer_data)

        # æ›´æ–°é—®é¢˜çŠ¶æ€
        await self.question_repo.update(question_id, {"is_processed": True})

        return answer

    async def _analyze_answer_content(
        self, content: str
    ) -> Tuple[List[str], List[str]]:
        """åˆ†æç­”æ¡ˆå†…å®¹ï¼Œæå–ç›¸å…³è¯é¢˜å’Œæ¨èé—®é¢˜"""
        # è¿™é‡Œæ˜¯ç®€åŒ–çš„åˆ†æé€»è¾‘ï¼Œå®é™…å¯ä»¥ä½¿ç”¨NLPæŠ€æœ¯æ”¹è¿›
        related_topics = []
        suggested_questions = []

        # ç®€å•çš„å…³é”®è¯æå–ï¼ˆå¯ä»¥åç»­æ”¹è¿›ï¼‰
        if "äºŒæ¬¡å‡½æ•°" in content:
            related_topics.extend(["äºŒæ¬¡å‡½æ•°", "å‡½æ•°å›¾è±¡", "é…æ–¹æ³•"])
            suggested_questions.extend(
                ["å¦‚ä½•æ±‚äºŒæ¬¡å‡½æ•°çš„å¯¹ç§°è½´ï¼Ÿ", "äºŒæ¬¡å‡½æ•°çš„æœ€å€¼æ€ä¹ˆæ±‚ï¼Ÿ"]
            )
        elif "åŒ–å­¦æ–¹ç¨‹å¼" in content:
            related_topics.extend(["åŒ–å­¦æ–¹ç¨‹å¼", "åŒ–å­¦ååº”", "é…å¹³"])
            suggested_questions.extend(["å¦‚ä½•é…å¹³åŒ–å­¦æ–¹ç¨‹å¼ï¼Ÿ", "åŒ–å­¦ååº”ç±»å‹æœ‰å“ªäº›ï¼Ÿ"])

        return related_topics[:5], suggested_questions[:3]  # é™åˆ¶æ•°é‡

    async def _update_session_stats(self, session_id: str, tokens_used: int) -> None:
        """
        æ›´æ–°ä¼šè¯ç»Ÿè®¡ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰

        ä½¿ç”¨åŸå§‹ SQL æ›´æ–°ä»¥å‡å°‘æ•°æ®åº“å¾€è¿”ï¼Œé¿å…å…ˆè¯»åå†™å¯¼è‡´çš„é”äº‰ç”¨
        ç‰¹åˆ«æ˜¯åœ¨é«˜å¹¶å‘æµå¼è¯·æ±‚ä¸­
        """
        try:
            from sqlalchemy import text

            # ä½¿ç”¨åŸå§‹ SQL è¿›è¡ŒåŸå­æ€§æ›´æ–°ï¼Œé¿å…ç«æ€æ¡ä»¶
            update_query = text("""
                UPDATE chat_session
                SET
                    total_tokens = COALESCE(total_tokens, 0) + :tokens_used,
                    question_count = COALESCE(question_count, 0) + 1,
                    last_active_at = :now
                WHERE id = :session_id
            """)

            await self.db.execute(
                update_query,
                {
                    "tokens_used": tokens_used,
                    "now": datetime.now().isoformat(),
                    "session_id": session_id,
                },
            )

            logger.debug(
                f"âš¡ ä¼šè¯ç»Ÿè®¡å·²æ›´æ–°ï¼ˆåŸå­æ“ä½œï¼‰: session_id={session_id}, tokens={tokens_used}"
            )
        except Exception as e:
            logger.warning(f"æ›´æ–°ä¼šè¯ç»Ÿè®¡å¤±è´¥: {e}")
            # ç»§ç»­å¤„ç†ï¼Œä¸é˜»å¡æµå¼å“åº”

    async def _update_learning_analytics(
        self, user_id: str, question: Question, answer: Answer
    ) -> None:
        """æ›´æ–°ç”¨æˆ·å­¦ä¹ åˆ†æ"""
        try:
            # è·å–æˆ–åˆ›å»ºå­¦ä¹ åˆ†æè®°å½•
            analytics = await self.analytics_repo.get_by_field("user_id", user_id)
            if not analytics:
                analytics_data = {
                    "user_id": user_id,
                    "total_questions": 1,
                    "total_sessions": 1,
                    "last_analyzed_at": datetime.utcnow().isoformat(),
                }
                await self.analytics_repo.create(analytics_data)
            else:
                # æ›´æ–°ç»Ÿè®¡
                await self.analytics_repo.update(
                    extract_orm_uuid_str(analytics, "id"),
                    {
                        "total_questions": (
                            extract_orm_int(analytics, "total_questions", 0) or 0
                        )
                        + 1,
                        "last_analyzed_at": datetime.utcnow().isoformat(),
                    },
                )

        except Exception as e:
            logger.warning(f"æ›´æ–°å­¦ä¹ åˆ†æå¤±è´¥: {str(e)}")

    # ========== ä¼šè¯ç®¡ç†åŠŸèƒ½ ==========

    async def create_session(
        self, user_id: str, request: CreateSessionRequest
    ) -> SessionResponse:
        """åˆ›å»ºæ–°ä¼šè¯"""
        session_data = {
            "user_id": user_id,
            "title": request.title,
            "subject": request.subject.value if request.subject else None,
            "grade_level": request.grade_level,
            "status": SessionStatus.ACTIVE.value,
            "context_enabled": request.context_enabled,
            "last_active_at": datetime.utcnow().isoformat(),
        }

        session = await self.session_repo.create(session_data)

        # å¦‚æœæœ‰åˆå§‹é—®é¢˜ï¼Œå¤„ç†ç¬¬ä¸€ä¸ªé—®é¢˜
        if request.initial_question:
            ask_request = AskQuestionRequest(
                content=request.initial_question,
                session_id=extract_orm_uuid_str(session, "id"),
                subject=request.subject,
                topic=None,
                difficulty_level=None,
            )
            await self.ask_question(user_id, ask_request)

        return SessionResponse.model_validate(session)

    async def get_session_list(
        self, user_id: str, query: SessionListQuery
    ) -> Dict[str, Any]:
        """è·å–ä¼šè¯åˆ—è¡¨"""
        # æ„å»ºæŸ¥è¯¢æ¡ä»¶
        conditions = [ChatSession.user_id == user_id]

        if query.status:
            conditions.append(ChatSession.status == safe_str(query.status))

        if query.subject:
            conditions.append(ChatSession.subject == safe_str(query.subject))

        if query.search:
            conditions.append(ChatSession.title.contains(query.search))

        # è®¡ç®—æ€»æ•°
        count_stmt = select(func.count(ChatSession.id)).where(and_(*conditions))
        total_result = await self.db.execute(count_stmt)
        total = total_result.scalar()

        # æŸ¥è¯¢æ•°æ®
        stmt = (
            select(ChatSession)
            .where(and_(*conditions))
            .order_by(desc(ChatSession.last_active_at))
            .offset((query.page - 1) * query.size)
            .limit(query.size)
        )

        result = await self.db.execute(stmt)
        sessions = result.scalars().all()

        return {
            "total": total,
            "page": query.page,
            "size": query.size,
            "pages": (
                (total + query.size - 1) // query.size if total and query.size else 0
            ),
            "items": [SessionResponse.model_validate(session) for session in sessions],
        }

    async def get_question_history(
        self, user_id: str, query: QuestionHistoryQuery
    ) -> Dict[str, Any]:
        """è·å–é—®é¢˜å†å²"""
        # æ„å»ºæŸ¥è¯¢æ¡ä»¶
        conditions = [Question.user_id == user_id]

        if query.session_id:
            conditions.append(Question.session_id == query.session_id)

        if query.subject:
            conditions.append(Question.subject == safe_str(query.subject))

        if query.question_type:
            conditions.append(Question.question_type == safe_str(query.question_type))

        if query.start_date:
            conditions.append(Question.created_at >= query.start_date.isoformat())

        if query.end_date:
            conditions.append(Question.created_at <= query.end_date.isoformat())

        # è®¡ç®—æ€»æ•°
        count_stmt = select(func.count(Question.id)).where(and_(*conditions))
        total_result = await self.db.execute(count_stmt)
        total = total_result.scalar()

        # æŸ¥è¯¢æ•°æ®
        stmt = (
            select(Question)
            .options(selectinload(Question.answer))
            .where(and_(*conditions))
            .order_by(desc(Question.created_at))
            .offset((query.page - 1) * query.size)
            .limit(query.size)
        )

        result = await self.db.execute(stmt)
        questions = result.scalars().all()

        # æ„å»ºé—®ç­”å¯¹
        items = []
        for question in questions:
            item = {
                "question": QuestionResponse.model_validate(question),
                "answer": (
                    AnswerResponse.model_validate(question.answer)
                    if question.answer
                    else None
                ),
            }
            items.append(item)

        return {
            "total": total,
            "page": query.page,
            "size": query.size,
            "pages": (
                (total + query.size - 1) // query.size if total and query.size else 0
            ),
            "items": items,
        }

    # ========== åé¦ˆå’Œè¯„ä»·åŠŸèƒ½ ==========

    async def submit_feedback(self, user_id: str, request: FeedbackRequest) -> bool:
        """æäº¤ç”¨æˆ·åé¦ˆ"""
        # éªŒè¯é—®é¢˜å½’å±
        question = await self.question_repo.get_by_id(request.question_id)
        if not question or extract_orm_uuid_str(question, "user_id") != user_id:
            raise NotFoundError("é—®é¢˜ä¸å­˜åœ¨æˆ–æ— æƒé™è®¿é—®")

        if not getattr(question, "answer", None):
            raise ValidationError("é—®é¢˜å°šæœªå›ç­”ï¼Œæ— æ³•æäº¤åé¦ˆ")

        # æ›´æ–°ç­”æ¡ˆåé¦ˆ
        answer_id = extract_orm_uuid_str(question.answer, "id")
        await self.answer_repo.update(
            answer_id,
            {
                "user_rating": request.rating,
                "user_feedback": request.feedback,
                "is_helpful": request.is_helpful,
            },
        )

        logger.info(
            "ç”¨æˆ·åé¦ˆå·²ä¿å­˜",
            extra={
                "user_id": user_id,
                "question_id": request.question_id,
                "rating": request.rating,
            },
        )

        return True

    # ========== å­¦ä¹ åˆ†æåŠŸèƒ½ ==========

    @cache_result(ttl=3600)  # ç¼“å­˜1å°æ—¶
    async def get_learning_analytics(
        self, user_id: str
    ) -> Optional[LearningAnalyticsResponse]:
        """è·å–å­¦ä¹ åˆ†æ"""
        analytics = await self.analytics_repo.get_by_field("user_id", user_id)
        if not analytics:
            return None

        # è·å–è¯¦ç»†ç»Ÿè®¡æ•°æ®
        subject_stats = await self._calculate_subject_stats(user_id)
        learning_pattern = await self._analyze_learning_pattern(user_id)

        # è®¡ç®—å¹³å‡è¯„åˆ†
        avg_rating_stmt = (
            select(func.avg(Answer.user_rating))
            .select_from(join(Answer, Question, Answer.question_id == Question.id))
            .where(Question.user_id == user_id, Answer.user_rating.isnot(None))
        )

        avg_rating_result = await self.db.execute(avg_rating_stmt)
        _avg_rating = avg_rating_result.scalar() or 0.0  # ç”¨äºæœªæ¥çš„è¯„åˆ†ç»Ÿè®¡

        # è®¡ç®—æ­£é¢åé¦ˆç‡
        _positive_feedback_rate = await self._calculate_positive_feedback_rate(
            user_id
        )  # ç”¨äºæœªæ¥çš„åé¦ˆåˆ†æ

        # ç”Ÿæˆæ”¹è¿›å»ºè®®
        _improvement_suggestions = await self._generate_improvement_suggestions(
            user_id, subject_stats
        )  # ç”¨äºæœªæ¥çš„å»ºè®®åŠŸèƒ½

        # è¯†åˆ«çŸ¥è¯†ç¼ºå£
        _knowledge_gaps = await self._identify_knowledge_gaps(
            user_id
        )  # ç”¨äºæœªæ¥çš„ç¼ºå£åˆ†æ

        # Get basic stats from analytics if available
        if analytics:
            total_questions = extract_orm_int(analytics, "total_questions", 0) or 0
            total_sessions = extract_orm_int(analytics, "total_sessions", 0) or 0
        else:
            total_questions = 0
            total_sessions = 0

        # Create a simple learning pattern
        from src.schemas.learning import DifficultyLevel, LearningPattern

        learning_pattern = LearningPattern(
            most_active_hour=14,
            most_active_day=1,
            avg_session_length=30,
            preferred_difficulty=DifficultyLevel.MEDIUM,
        )

        return LearningAnalyticsResponse(
            user_id=user_id,
            total_questions=total_questions,
            total_sessions=total_sessions,
            subject_stats=[],  # Simplified - needs proper conversion
            learning_pattern=learning_pattern,
            avg_rating=3.5,
            positive_feedback_rate=75,
            improvement_suggestions=["éœ€è¦æ›´å¤šç»ƒä¹ "],  # Simplified
            knowledge_gaps=["åŸºç¡€æ¦‚å¿µ"],  # Simplified
            last_analyzed_at=datetime.now(),
        )

    async def _calculate_subject_stats(self, user_id: str) -> List[Dict[str, Any]]:
        """è®¡ç®—å„å­¦ç§‘ç»Ÿè®¡"""
        # ç®€åŒ–å®ç°ï¼Œè¿”å›åŸºæœ¬ç»Ÿè®¡
        stmt = (
            select(
                Question.subject,
                func.count(Question.id).label("question_count"),
                func.avg(Question.difficulty_level).label("avg_difficulty"),
            )
            .where(Question.user_id == user_id, Question.subject.isnot(None))
            .group_by(Question.subject)
        )

        result = await self.db.execute(stmt)
        stats = []

        for row in result:
            stats.append(
                {
                    "subject": row.subject,
                    "question_count": row.question_count,
                    "avg_difficulty": float(row.avg_difficulty or 3.0),
                    "mastery_level": 75,  # é»˜è®¤æŒæ¡åº¦ï¼Œå¯ä»¥åç»­æ”¹è¿›
                }
            )

        return stats

    async def _analyze_learning_pattern(self, user_id: str) -> Dict[str, Any]:
        """åˆ†æå­¦ä¹ æ¨¡å¼"""
        return {
            "most_active_hour": 20,  # æ™šä¸Š8ç‚¹
            "most_active_day": 0,  # å‘¨æ—¥
            "avg_session_length": 30,  # 30åˆ†é’Ÿ
            "preferred_difficulty": 3,  # ä¸­ç­‰éš¾åº¦
        }

    async def _calculate_positive_feedback_rate(self, user_id: str) -> int:
        """è®¡ç®—æ­£é¢åé¦ˆç‡"""
        total_stmt = (
            select(func.count(Answer.id))
            .select_from(join(Answer, Question, Answer.question_id == Question.id))
            .where(Question.user_id == user_id, Answer.is_helpful.isnot(None))
        )

        positive_stmt = (
            select(func.count(Answer.id))
            .select_from(join(Answer, Question, Answer.question_id == Question.id))
            .where(Question.user_id == user_id, Answer.is_helpful)
        )

        total_result = await self.db.execute(total_stmt)
        positive_result = await self.db.execute(positive_stmt)

        total = total_result.scalar() or 0
        positive = positive_result.scalar() or 0

        return int((positive / total * 100) if total > 0 else 0)

    async def _generate_improvement_suggestions(
        self, user_id: str, subject_stats: List[Dict[str, Any]]
    ) -> List[str]:
        """ç”Ÿæˆæ”¹è¿›å»ºè®®"""
        suggestions = []

        # åŸºäºå­¦ç§‘ç»Ÿè®¡ç”Ÿæˆå»ºè®®
        for stat in subject_stats:
            if stat["question_count"] < 5:
                suggestions.append(
                    f"å»ºè®®å¢åŠ {self._get_subject_name(stat['subject'])}å­¦ç§‘çš„ç»ƒä¹ "
                )

            if stat["avg_difficulty"] < 2.5:
                suggestions.append(
                    f"å¯ä»¥å°è¯•{self._get_subject_name(stat['subject'])}æ›´æœ‰æŒ‘æˆ˜æ€§çš„é—®é¢˜"
                )

        return suggestions[:5]  # æœ€å¤š5ä¸ªå»ºè®®

    async def _identify_knowledge_gaps(self, user_id: str) -> List[str]:
        """è¯†åˆ«çŸ¥è¯†ç¼ºå£"""
        # åŸºäºé”™é¢˜å’Œä½åˆ†ä½œä¸šè¯†åˆ«çŸ¥è¯†ç¼ºå£
        # ä»é—®é¢˜è¯é¢˜ä¸­åˆ†æ
        stmt = (
            select(Question.topic)
            .where(Question.user_id == user_id, Question.topic.isnot(None))
            .distinct()
        )

        result = await self.db.execute(stmt)
        topics = [row[0] for row in result]

        # ç®€åŒ–é€»è¾‘ï¼šå¦‚æœæŸä¸ªè¯é¢˜é—®å¾—æ¯”è¾ƒå¤šï¼Œå¯èƒ½æ˜¯è–„å¼±ç¯èŠ‚
        return topics[:5]

    # ========== é”™é¢˜æœ¬åŠŸèƒ½ ==========

    async def add_question_to_mistakes(
        self,
        user_id: str,
        question_id: str,
        student_answer: Optional[str] = None,
    ) -> Dict[str, Any]:
        """
        å°†å­¦ä¹ é—®ç­”ä¸­çš„é¢˜ç›®åŠ å…¥é”™é¢˜æœ¬ï¼ˆä¼˜åŒ–ç‰ˆ - ä½¿ç”¨AIç»“æ„åŒ–æå–ï¼‰

        Args:
            user_id: ç”¨æˆ·ID
            question_id: é—®é¢˜ID
            student_answer: å­¦ç”Ÿç­”æ¡ˆï¼ˆå¯é€‰ï¼Œç”¨äºæ ‡è®°ç­”é”™ï¼‰

        Returns:
            Dict: åˆ›å»ºçš„é”™é¢˜è¯¦æƒ…

        Raises:
            NotFoundError: é—®é¢˜ä¸å­˜åœ¨
            ServiceError: åˆ›å»ºå¤±è´¥
        """
        try:
            # 1. è·å–é—®é¢˜å’Œç­”æ¡ˆ
            question = await self.question_repo.get_by_id(question_id)
            if not question or str(question.user_id) != user_id:
                raise NotFoundError(f"é—®é¢˜ {question_id} ä¸å­˜åœ¨")

            # ä½¿ç”¨ get_by_field æ–¹æ³•è·å–ç­”æ¡ˆï¼ˆBaseRepository çš„æ ‡å‡†æ–¹æ³•ï¼‰
            answer = await self.answer_repo.get_by_field("question_id", question_id)
            if not answer:
                raise NotFoundError(f"é—®é¢˜ {question_id} æš‚æ— ç­”æ¡ˆ")

            # 2. ğŸ¯ ä½¿ç”¨AIç»“æ„åŒ–æå–é¢˜ç›®ä¿¡æ¯
            question_content = getattr(question, "content", "")
            answer_content = getattr(answer, "content", "")
            question_subject = getattr(question, "subject", None)

            # è§£æå›¾ç‰‡URL
            image_urls = []
            question_has_images = getattr(question, "has_images", False)
            question_image_urls = getattr(question, "image_urls", None)
            if question_has_images and question_image_urls:
                try:
                    image_urls = (
                        json.loads(question_image_urls)
                        if isinstance(question_image_urls, str)
                        else question_image_urls
                    )
                except (json.JSONDecodeError, TypeError, ValueError) as parse_error:
                    logger.warning(f"è§£æå›¾ç‰‡URLå¤±è´¥: {parse_error}")
                    image_urls = []

            logger.info(
                f"ğŸ” å¼€å§‹AIç»“æ„åŒ–æå–: question_id={question_id}, "
                f"has_images={len(image_urls) > 0}, subject={question_subject}"
            )

            # è°ƒç”¨AIç»“æ„åŒ–æå–
            structured_data = await self._extract_structured_question(
                user_question=question_content,
                ai_answer=answer_content,
                image_urls=image_urls,
                subject=question_subject,
            )

            logger.info(
                f"âœ… AIæå–å®Œæˆ: success={structured_data.get('extraction_success')}, "
                f"confidence={structured_data.get('confidence')}, "
                f"knowledge_points={structured_data.get('knowledge_points')}"
            )

            # 3. æ„é€ é”™é¢˜æ•°æ®ï¼ˆä½¿ç”¨ç»“æ„åŒ–æå–çš„æ•°æ®ï¼‰
            from src.models.study import MistakeRecord
            from src.repositories.mistake_repository import MistakeRepository
            from src.services.knowledge_graph_service import (
                normalize_subject,
            )

            mistake_repo = MistakeRepository(MistakeRecord, self.db)

            # ç”Ÿæˆæ ‡é¢˜ï¼ˆä½¿ç”¨æå–çš„çº¯å‡€é¢˜ç›®å†…å®¹ï¼‰
            clean_question = structured_data.get("question_content", question_content)
            title = self._generate_mistake_title(clean_question)

            # ğŸ”§ å…³é”®ä¿®å¤ï¼šå°†è‹±æ–‡ subject è½¬æ¢ä¸ºä¸­æ–‡ï¼Œç¡®ä¿æ•°æ®åº“ä¸€è‡´æ€§
            # å‰ç«¯å‘é€ subject="math"ï¼ˆè‹±æ–‡ï¼‰ï¼Œéœ€è¦è½¬æ¢ä¸º "æ•°å­¦"ï¼ˆä¸­æ–‡ï¼‰ä¿å­˜åˆ° MistakeRecord
            normalized_subject = (
                normalize_subject(question_subject) if question_subject else "å…¶ä»–"
            )

            logger.info(
                f"ğŸ“ å­¦ç§‘è½¬æ¢: question_subject={question_subject} â†’ normalized_subject={normalized_subject}"
            )

            mistake_data = {
                "user_id": user_id,
                "subject": normalized_subject,
                "title": title,
                "ocr_text": clean_question,  # ğŸ¯ ä½¿ç”¨çº¯å‡€çš„é¢˜ç›®å†…å®¹
                "image_urls": image_urls,
                "difficulty_level": structured_data.get("difficulty_level", 2),
                "knowledge_points": structured_data.get(
                    "knowledge_points", []
                ),  # ğŸ¯ ä½¿ç”¨AIæå–çš„çŸ¥è¯†ç‚¹
                "ai_feedback": {
                    "model": (
                        getattr(answer, "model_name", "unknown")
                        if answer
                        else "unknown"
                    ),
                    "answer": answer_content,
                    "confidence": (
                        getattr(answer, "confidence_score", 0.0) if answer else 0.0
                    ),
                    "tokens_used": getattr(answer, "tokens_used", 0) if answer else 0,
                    # ğŸ¯ æ–°å¢ï¼šç»“æ„åŒ–æå–çš„æ•°æ®
                    "structured_extraction": {
                        "success": structured_data.get("extraction_success", False),
                        "confidence": structured_data.get("confidence", 0.0),
                        "question_type": structured_data.get("question_type", "æœªçŸ¥"),
                        "explanation": structured_data.get("explanation", ""),
                        "is_fallback": structured_data.get("fallback", False),
                    },
                },
                # ã€æ¥æºä¿¡æ¯ã€‘
                "source": "learning",
                "source_question_id": question_id,
                "student_answer": student_answer,
                "correct_answer": structured_data.get(
                    "correct_answer"
                ),  # ğŸ¯ ä½¿ç”¨AIæå–çš„ç­”æ¡ˆ
                # å¤ä¹ ç›¸å…³ï¼ˆä½¿ç”¨è‰¾å®¾æµ©æ–¯ç®—æ³•ï¼‰
                "mastery_status": "learning",
                "next_review_at": datetime.now() + timedelta(days=1),
                "review_count": 0,
                "correct_count": 0,
            }

            # 4. åˆ›å»ºé”™é¢˜è®°å½•
            mistake = await mistake_repo.create(mistake_data)

            logger.info(
                f"ğŸ“ ä»å­¦ä¹ é—®ç­”åˆ›å»ºé”™é¢˜: question_id={question_id}, mistake_id={mistake.id}, "
                f"ä½¿ç”¨AIæå–={structured_data.get('extraction_success')}"
            )

            # 5. ã€æ–°å¢ã€‘è‡ªåŠ¨å…³è”çŸ¥è¯†ç‚¹
            # æ³¨æ„ï¼šsubject å·²åœ¨ä¸Šé¢è½¬æ¢ä¸ºä¸­æ–‡ï¼Œæ— éœ€å†è½¬æ¢
            subject_for_kg = mistake_data.get("subject", "å…¶ä»–")
            try:
                from uuid import UUID

                from src.services.knowledge_graph_service import KnowledgeGraphService

                kg_service = KnowledgeGraphService(self.db, self.bailian_service)

                # å‡†å¤‡AIåé¦ˆæ•°æ®ï¼ˆåŒ…å«ç»“æ„åŒ–æå–çš„çŸ¥è¯†ç‚¹ï¼‰
                ai_feedback_for_kg = {
                    "knowledge_points": [
                        {
                            "name": kp,
                            "relevance": 0.9,
                            "extraction_method": "ai_structured",
                        }
                        for kp in structured_data.get("knowledge_points", [])
                    ],
                    "question": clean_question,
                    "explanation": structured_data.get("explanation", answer_content),
                }

                # è°ƒç”¨çŸ¥è¯†å›¾è°±æœåŠ¡åˆ†æå¹¶å…³è”çŸ¥è¯†ç‚¹
                kp_list = structured_data.get("knowledge_points", [])

                logger.debug(
                    f"ğŸ“Œ å‡†å¤‡å…³è”çŸ¥è¯†ç‚¹: mistake_id={mistake.id}, "
                    f"user_id={user_id}, subject={subject_for_kg}, "
                    f"knowledge_points_count={len(kp_list)}"
                )

                if not kp_list:
                    logger.warning(
                        f"âš ï¸ çŸ¥è¯†ç‚¹åˆ—è¡¨ä¸ºç©ºï¼Œè·³è¿‡å…³è”: mistake_id={mistake.id}, "
                        f"structured_data={structured_data.get('extraction_success', False)}"
                    )
                else:
                    await kg_service.analyze_and_associate_knowledge_points(
                        mistake_id=UUID(str(getattr(mistake, "id"))),
                        user_id=UUID(user_id),
                        subject=subject_for_kg,
                        ocr_text=clean_question,
                        ai_feedback=ai_feedback_for_kg,
                    )

                    logger.info(
                        f"âœ… å·²ä¸ºé”™é¢˜ {mistake.id} è‡ªåŠ¨å…³è”çŸ¥è¯†ç‚¹: "
                        f"{len(kp_list)} ä¸ªçŸ¥è¯†ç‚¹"
                    )
            except Exception as e:
                # çŸ¥è¯†ç‚¹å…³è”å¤±è´¥ä¸å½±å“é”™é¢˜åˆ›å»ºï¼Œä½†éœ€è¦è¯¦ç»†è®°å½•
                logger.error(
                    f"âŒ çŸ¥è¯†ç‚¹è‡ªåŠ¨å…³è”å¤±è´¥: mistake_id={mistake.id}, "
                    f"user_id={user_id}, subject={subject_for_kg}, error={e}",
                    exc_info=True,
                )

            # 6. è½¬æ¢ä¸ºå“åº”æ ¼å¼
            return {
                "id": str(mistake.id),
                "title": title,
                "subject": mistake.subject,
                "source": "learning",
                "source_question_id": question_id,
                "knowledge_points": structured_data.get("knowledge_points", []),
                "question_type": structured_data.get("question_type", "æœªçŸ¥"),
                "difficulty_level": structured_data.get("difficulty_level", 2),
                "ai_extraction_success": structured_data.get(
                    "extraction_success", False
                ),
                "next_review_date": (
                    next_review_at.isoformat()
                    if (next_review_at := getattr(mistake, "next_review_at", None))
                    else None
                ),
                "created_at": (
                    mistake.created_at.isoformat()
                    if hasattr(mistake, "created_at")
                    else None
                ),
            }

        except NotFoundError:
            raise
        except Exception as e:
            logger.error(f"åŠ å…¥é”™é¢˜æœ¬å¤±è´¥: {e}", exc_info=True)
            raise ServiceError(f"åŠ å…¥é”™é¢˜æœ¬å¤±è´¥: {str(e)}")

    def _generate_mistake_title(self, content: str) -> str:
        """ç”Ÿæˆé”™é¢˜æ ‡é¢˜ï¼ˆæˆªå–å‰30å­—ï¼‰"""
        if len(content) <= 30:
            return content
        return content[:30] + "..."

    def _extract_correct_answer(self, ai_answer: str) -> Optional[str]:
        """ä»AIå›ç­”ä¸­æå–æ­£ç¡®ç­”æ¡ˆ"""
        import re

        # ç®€å•è§„åˆ™ï¼šæŸ¥æ‰¾"ç­”æ¡ˆï¼š"ã€"æ­£ç¡®ç­”æ¡ˆï¼š"ç­‰å…³é”®è¯åçš„å†…å®¹
        patterns = [
            r"ç­”æ¡ˆ[ï¼š:]\s*(.+?)(?:\n|$)",
            r"æ­£ç¡®ç­”æ¡ˆ[ï¼š:]\s*(.+?)(?:\n|$)",
            r"è§£[ï¼š:]\s*(.+?)(?:\n|$)",
        ]

        for pattern in patterns:
            match = re.search(pattern, ai_answer)
            if match:
                return match.group(1).strip()

        # å¦‚æœæ²¡æ‰¾åˆ°ï¼Œè¿”å›AIå›ç­”çš„å‰100å­—
        return ai_answer[:100] if len(ai_answer) > 100 else ai_answer

    async def _extract_structured_question(
        self,
        user_question: str,
        ai_answer: str,
        image_urls: Optional[List[str]] = None,
        subject: Optional[str] = None,
    ) -> Dict[str, Any]:
        """
        ğŸ¯ æ ¸å¿ƒæ–¹æ³•ï¼šä½¿ç”¨ç™¾ç‚¼AIä»é—®ç­”å¯¹è¯ä¸­ç»“æ„åŒ–æå–é¢˜ç›®ä¿¡æ¯

        ç›®æ ‡ï¼š
        - åˆ†ç¦»å­¦ç”Ÿæé—®è¯­å¥å’Œé¢˜ç›®å†…å®¹
        - æå–é¢˜ç›®ä¸»ä½“ã€ç­”æ¡ˆã€è§£æã€çŸ¥è¯†ç‚¹
        - è¯†åˆ«é¢˜ç›®ç±»å‹å’Œéš¾åº¦

        Args:
            user_question: ç”¨æˆ·åŸå§‹æé—®ï¼ˆå¯èƒ½åŒ…å«"è€å¸ˆï¼Œæˆ‘ä¸ä¼š..."ç­‰è¯­å¥ï¼‰
            ai_answer: AIçš„å®Œæ•´å›ç­”
            image_urls: å›¾ç‰‡URLåˆ—è¡¨
            subject: å­¦ç§‘

        Returns:
            {
                'question_content': str,  # çº¯å‡€çš„é¢˜ç›®å†…å®¹
                'correct_answer': str,  # æ ‡å‡†ç­”æ¡ˆ
                'explanation': str,  # è¯¦ç»†è§£æ
                'knowledge_points': List[str],  # çŸ¥è¯†ç‚¹åˆ—è¡¨
                'difficulty_level': int,  # 1-5
                'question_type': str,  # é€‰æ‹©é¢˜/å¡«ç©ºé¢˜/è§£ç­”é¢˜ç­‰
                'has_image': bool,  # æ˜¯å¦å«å›¾ç‰‡
                'extraction_success': bool,  # æå–æ˜¯å¦æˆåŠŸ
                'confidence': float  # æå–ç½®ä¿¡åº¦
            }
        """
        try:
            # æ„å»ºæç¤ºè¯
            prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„K12æ•™è‚²é¢˜ç›®è§£æä¸“å®¶ã€‚è¯·ä»ä»¥ä¸‹å­¦ç”Ÿä¸è€å¸ˆçš„é—®ç­”å¯¹è¯ä¸­ï¼Œæå–å‡º**ç»“æ„åŒ–çš„é¢˜ç›®ä¿¡æ¯**ã€‚

**å­¦ç”Ÿæé—®ï¼š**
{user_question}

**è€å¸ˆå›ç­”ï¼š**
{ai_answer}

**ä»»åŠ¡è¦æ±‚ï¼š**
1. åˆ†ç¦»å­¦ç”Ÿçš„æé—®è¯­å¥ï¼ˆå¦‚"è€å¸ˆæˆ‘ä¸ä¼š"ã€"å¸®æˆ‘çœ‹çœ‹"ï¼‰å’ŒçœŸæ­£çš„é¢˜ç›®å†…å®¹
2. æå–é¢˜ç›®ä¸»ä½“ï¼ˆå¦‚æœå­¦ç”Ÿæ²¡æœ‰æ˜ç¡®ç»™å‡ºé¢˜ç›®ï¼Œä»è€å¸ˆå›ç­”ä¸­æ¨æ–­ï¼‰
3. æå–æ ‡å‡†ç­”æ¡ˆ
4. æå–è¯¦ç»†è§£æ
5. è¯†åˆ«æ¶‰åŠçš„çŸ¥è¯†ç‚¹ï¼ˆ2-5ä¸ªï¼‰
6. åˆ¤æ–­é¢˜ç›®ç±»å‹å’Œéš¾åº¦

**è¾“å‡ºæ ¼å¼ï¼ˆä¸¥æ ¼JSONï¼‰ï¼š**
```json
{{
  "question_content": "çº¯å‡€çš„é¢˜ç›®å†…å®¹ï¼ˆå»é™¤å­¦ç”Ÿçš„æ±‚åŠ©è¯­å¥ï¼‰",
  "correct_answer": "æ ‡å‡†ç­”æ¡ˆ",
  "explanation": "è¯¦ç»†è§£æè¿‡ç¨‹",
  "knowledge_points": ["çŸ¥è¯†ç‚¹1", "çŸ¥è¯†ç‚¹2"],
  "difficulty_level": 2,
  "question_type": "é€‰æ‹©é¢˜/å¡«ç©ºé¢˜/è§£ç­”é¢˜/åˆ¤æ–­é¢˜/åº”ç”¨é¢˜",
  "extraction_success": true,
  "confidence": 0.9
}}
```

**ç‰¹æ®Šæƒ…å†µå¤„ç†ï¼š**
- å¦‚æœå­¦ç”Ÿåªä¸Šä¼ å›¾ç‰‡æ²¡æœ‰æ–‡å­—ï¼Œquestion_contentå¡«å†™"å›¾ç‰‡é¢˜ç›®ï¼ˆéœ€OCRè¯†åˆ«ï¼‰"
- å¦‚æœæ— æ³•æå–å®Œæ•´é¢˜ç›®ï¼Œè®¾ç½® extraction_success=falseï¼Œconfidenceé™ä½
- çŸ¥è¯†ç‚¹å¿…é¡»å…·ä½“æ˜ç¡®ï¼Œä¸è¦ç”¨"æ•°å­¦çŸ¥è¯†"è¿™ç§æ³›æ³›çš„è¯´æ³•
- éš¾åº¦ç­‰çº§ï¼š1=åŸºç¡€ï¼Œ2=ä¸­ç­‰ï¼Œ3=å›°éš¾ï¼Œ4=æŒ‘æˆ˜ï¼Œ5=ç«èµ›"""

            if subject:
                prompt += f"\n\n**å­¦ç§‘ï¼š** {subject}"

            if image_urls and len(image_urls) > 0:
                prompt += f"\n\n**æ³¨æ„ï¼š** å­¦ç”Ÿä¸Šä¼ äº† {len(image_urls)} å¼ å›¾ç‰‡ï¼Œé¢˜ç›®å¯èƒ½åœ¨å›¾ç‰‡ä¸­"

            # è°ƒç”¨ç™¾ç‚¼AI
            if not self.bailian_service:
                logger.warning("ç™¾ç‚¼æœåŠ¡æœªåˆå§‹åŒ–ï¼Œä½¿ç”¨é™çº§é€»è¾‘")
                return self._fallback_extraction(user_question, ai_answer)

            try:
                response = await self.bailian_service.chat_completion(
                    messages=[{"role": "user", "content": prompt}],
                    temperature=0.3,  # é™ä½æ¸©åº¦ä¿è¯ç¨³å®šæ€§
                )

                # è§£æå“åº”
                response_text = (
                    response.content if hasattr(response, "content") else str(response)
                )

                # æå–JSONéƒ¨åˆ†
                import re

                json_match = re.search(
                    r"```json\s*(\{.*?\})\s*```", response_text, re.DOTALL
                )
                if json_match:
                    json_str = json_match.group(1)
                else:
                    # å°è¯•ç›´æ¥è§£ææ•´ä¸ªå“åº”
                    json_str = response_text.strip()

                result = json.loads(json_str)

                # éªŒè¯å¿…è¦å­—æ®µ
                if not result.get("question_content") or not result.get(
                    "extraction_success"
                ):
                    logger.warning("AIæå–ç»“æœä¸å®Œæ•´ï¼Œä½¿ç”¨é™çº§é€»è¾‘")
                    return self._fallback_extraction(user_question, ai_answer)

                logger.info(
                    f"âœ… AIç»“æ„åŒ–æå–æˆåŠŸ: confidence={result.get('confidence', 0)}, "
                    f"knowledge_points={len(result.get('knowledge_points', []))}"
                )

                return result

            except json.JSONDecodeError as e:
                logger.error(f"AIå“åº”JSONè§£æå¤±è´¥: {e}, ä½¿ç”¨é™çº§é€»è¾‘")
                return self._fallback_extraction(user_question, ai_answer)

        except Exception as e:
            logger.error(f"AIç»“æ„åŒ–æå–å¤±è´¥: {e}", exc_info=True)
            return self._fallback_extraction(user_question, ai_answer)

    def _fallback_extraction(
        self, user_question: str, ai_answer: str
    ) -> Dict[str, Any]:
        """
        é™çº§æ–¹æ¡ˆï¼šä½¿ç”¨è§„åˆ™æå–ï¼ˆå½“AIæå–å¤±è´¥æ—¶ï¼‰
        """
        import re

        # ç®€å•è§„åˆ™æå–çŸ¥è¯†ç‚¹
        knowledge_points = []
        kp_patterns = [
            r"æ¶‰åŠ[çŸ¥è¯†ç‚¹åˆ°äº†]?[:ï¼š]?([^ã€‚ï¼Œ\n]+)",
            r"è€ƒæŸ¥[çŸ¥è¯†ç‚¹åˆ°äº†]?[:ï¼š]?([^ã€‚ï¼Œ\n]+)",
        ]
        for pattern in kp_patterns:
            matches = re.findall(pattern, ai_answer)
            knowledge_points.extend([m.strip() for m in matches if len(m.strip()) > 2])

        # å»é‡
        knowledge_points = list(set(knowledge_points))[:5]

        # æå–ç­”æ¡ˆ
        correct_answer = self._extract_correct_answer(ai_answer)

        return {
            "question_content": user_question[:200],  # ä½¿ç”¨åŸå§‹æé—®
            "correct_answer": correct_answer or "è¯¦è§è§£æ",
            "explanation": ai_answer[:500],  # ä½¿ç”¨AIå›ç­”çš„å‰500å­—
            "knowledge_points": knowledge_points if knowledge_points else ["æœªè¯†åˆ«"],
            "difficulty_level": 2,  # é»˜è®¤ä¸­ç­‰
            "question_type": "è§£ç­”é¢˜",
            "extraction_success": False,  # æ ‡è®°ä¸ºé™çº§æå–
            "confidence": 0.5,  # ä½ç½®ä¿¡åº¦
            "fallback": True,
        }

    # ğŸ¯ é”™é¢˜è‡ªåŠ¨åˆ›å»ºé€»è¾‘ï¼ˆç®€åŒ–è§„åˆ™ç‰ˆï¼‰
    # ========== æ™ºèƒ½é”™é¢˜è¯†åˆ«è¾…åŠ©æ–¹æ³• ==========

    def _detect_mistake_keywords(self, question_content: str) -> Dict[str, Any]:
        """
        ç­–ç•¥1ï¼šå…³é”®è¯æ£€æµ‹

        Args:
            question_content: ç”¨æˆ·æé—®å†…å®¹

        Returns:
            {
                'is_mistake': bool,
                'confidence': float (0.0-1.0),
                'mistake_type': str,
                'reason': str,
                'matched_keywords': List[str]
            }
        """
        # ğŸ›¡ï¸ æ’é™¤å…³é”®è¯ï¼šæ˜ç¡®çš„éé”™é¢˜åœºæ™¯ï¼ˆçº¯çŸ¥è¯†æŸ¥è¯¢ã€é—²èŠï¼‰
        EXCLUSION_KEYWORDS = [
            "å‘Šè¯‰æˆ‘",
            "ä»€ä¹ˆæ˜¯",
            "ä»‹ç»ä¸€ä¸‹",
            "è®²è§£ä¸€ä¸‹",
            "è¯´è¯´",
            "è§£é‡Šä¸€ä¸‹",
            "æœ€é•¿çš„",
            "æœ€çŸ­çš„",
            "æœ€å¤§çš„",
            "æœ€å°çš„",
            "æœ‰å“ªäº›",
            "ä¸¾ä¾‹",
            "æ¯”å¦‚",
            "åŒºåˆ«",
            "è”ç³»",
            "å…³ç³»",
            "å®šä¹‰",
            "æ¦‚å¿µ",
            "ç‰¹ç‚¹",
            "ä¼˜ç‚¹",
            "ç¼ºç‚¹",
            "å¥½å¤„",
            "åå¤„",
        ]

        # ğŸ¯ é«˜ç½®ä¿¡åº¦å…³é”®è¯ï¼šå¼ºçƒˆæš—ç¤ºé”™é¢˜çš„è¯æ±‡
        HIGH_CONFIDENCE_KEYWORDS = [
            "ä¸ä¼šåš",
            "ä¸ä¼š",
            "ä¸æ‡‚",
            "ä¸ç†è§£",
            "ä¸æ˜ç™½",
            "æ€ä¹ˆåš",
            "å¦‚ä½•è§£ç­”",
            "æ€ä¹ˆè§£",
            "æ€ä¹ˆç®—",
            "åšé”™äº†",
            "ç­”é”™äº†",
            "é”™åœ¨å“ª",
            "çœ‹ä¸æ‡‚",
            "æ±‚è§£",
            "æ±‚ç­”æ¡ˆ",
            "å¸®æˆ‘åš",
            "å¸®æˆ‘çœ‹çœ‹è¿™é“é¢˜",  # æ›´å…·ä½“ï¼Œé¿å…è¯¯åˆ¤
        ]

        # ğŸ”¸ ä¸­ç½®ä¿¡åº¦å…³é”®è¯ï¼šå¯èƒ½æ˜¯é”™é¢˜ï¼Œä½†éœ€è¦æ›´å¤šè¯æ®ï¼ˆéœ€è¦â‰¥2ä¸ªæˆ–ä¸å›¾ç‰‡ç»“åˆï¼‰
        MEDIUM_CONFIDENCE_KEYWORDS = [
            "è§£é¢˜æ­¥éª¤",
            "è§£é¢˜æ€è·¯",
            "è§£é¢˜è¿‡ç¨‹",
            "è§£é¢˜æ–¹æ³•",
            "éš¾é¢˜",
            "æœ‰éš¾åº¦",
            "è§£ä¸å‡º",
            "æ²¡å­¦è¿‡",
        ]

        # ğŸ›¡ï¸ 1. å…ˆæ£€æŸ¥æ’é™¤å…³é”®è¯ï¼ˆä¼˜å…ˆçº§æœ€é«˜ï¼‰
        matched_exclusion = [kw for kw in EXCLUSION_KEYWORDS if kw in question_content]
        if matched_exclusion:
            logger.info(f"ğŸ›¡ï¸ æ£€æµ‹åˆ°éé”™é¢˜å…³é”®è¯ï¼Œè·³è¿‡é”™é¢˜è¯†åˆ«: {matched_exclusion[:2]}")
            return {
                "is_mistake": False,
                "confidence": 0.2,
                "mistake_type": None,
                "reason": f"æ£€æµ‹åˆ°éé”™é¢˜å…³é”®è¯: {', '.join(matched_exclusion[:2])}",
                "matched_keywords": [],
            }

        # 2. æ£€æŸ¥é«˜ç½®ä¿¡åº¦å…³é”®è¯
        matched_high = [kw for kw in HIGH_CONFIDENCE_KEYWORDS if kw in question_content]

        # 3. æ£€æŸ¥ä¸­ç½®ä¿¡åº¦å…³é”®è¯
        matched_medium = [
            kw for kw in MEDIUM_CONFIDENCE_KEYWORDS if kw in question_content
        ]

        # åˆ¤æ–­é”™é¢˜ç±»å‹
        mistake_type = "hard_question"  # é»˜è®¤
        if any(kw in question_content for kw in ["é”™", "åšé”™", "ç­”é”™"]):
            mistake_type = "wrong_answer"
        elif any(kw in question_content for kw in ["ä¸ä¼š", "ä¸æ‡‚", "çœ‹ä¸æ‡‚"]):
            mistake_type = "empty_question"

        # ğŸ¯ é«˜ç½®ä¿¡åº¦å…³é”®è¯ â†’ ç›´æ¥åˆ¤å®šä¸ºé”™é¢˜
        if matched_high:
            return {
                "is_mistake": True,
                "confidence": 0.9,
                "mistake_type": mistake_type,
                "reason": f"æ£€æµ‹åˆ°é«˜ç½®ä¿¡åº¦å…³é”®è¯: {', '.join(matched_high[:2])}",
                "matched_keywords": matched_high,
            }

        # ğŸ”¸ å¤šä¸ªä¸­ç½®ä¿¡åº¦å…³é”®è¯ï¼ˆâ‰¥2ä¸ªï¼‰â†’ åˆ¤å®šä¸ºé”™é¢˜ï¼ˆä½†ç½®ä¿¡åº¦è¾ƒä½ï¼‰
        if len(matched_medium) >= 2:
            return {
                "is_mistake": True,
                "confidence": 0.7,  # é™ä½ç½®ä¿¡åº¦ï¼Œä»0.75é™åˆ°0.7
                "mistake_type": mistake_type,
                "reason": f"æ£€æµ‹åˆ°å¤šä¸ªä¸­ç½®ä¿¡åº¦å…³é”®è¯: {', '.join(matched_medium[:2])}",
                "matched_keywords": matched_medium,
            }

        # ğŸ”¸ å•ä¸ªä¸­ç½®ä¿¡åº¦å…³é”®è¯ â†’ ä¸ç¡®å®šï¼ˆè¿”å›Noneï¼Œéœ€è¦å…¶ä»–è¯æ®ï¼‰
        if matched_medium:
            return {
                "is_mistake": None,  # âœ… ä¿®å¤ï¼šå•ä¸ªä¸­ç½®ä¿¡åº¦å…³é”®è¯ä¸è¶³ä»¥åˆ¤å®š
                "confidence": 0.5,  # é™ä½ç½®ä¿¡åº¦ï¼Œä»0.6é™åˆ°0.5
                "mistake_type": None,
                "reason": f"æ£€æµ‹åˆ°å•ä¸ªä¸­ç½®ä¿¡åº¦å…³é”®è¯ï¼ˆä¸è¶³ä»¥åˆ¤å®šï¼‰: {matched_medium[0]}",
                "matched_keywords": matched_medium,
            }

        return {
            "is_mistake": False,
            "confidence": 0.3,
            "mistake_type": None,
            "reason": "æœªæ£€æµ‹åˆ°é”™é¢˜å…³é”®è¯",
            "matched_keywords": [],
        }

    def _extract_ai_mistake_metadata(self, answer_content: str) -> Dict[str, Any]:
        """
        ç­–ç•¥2ï¼šAIæ„å›¾è¯†åˆ«

        ä»AIå›ç­”ä¸­æå–å…ƒæ•°æ®ï¼ˆå¦‚æœAIè¾“å‡ºäº†ç»“æ„åŒ–ä¿¡æ¯ï¼‰

        âš ï¸ æ³¨æ„ï¼šæ­¤æ–¹æ³•ä»…ç”¨äºæå–AIä¸»åŠ¨è¾“å‡ºçš„ç»“æ„åŒ–å…ƒæ•°æ®ï¼Œ
        ä¸åº”åŸºäºAIå›ç­”å†…å®¹åšå¯å‘å¼åˆ¤æ–­ï¼ˆä¼šå¯¼è‡´æ­£å¸¸é—®ç­”è¢«è¯¯åˆ¤ï¼‰

        Args:
            answer_content: AIå›ç­”å†…å®¹

        Returns:
            {
                'is_mistake': Optional[bool],
                'confidence': float,
                'mistake_type': Optional[str],
                'knowledge_points': List[str],
                'reason': str
            }
        """
        try:
            # å°è¯•ä»å›ç­”æœ«å°¾æå–JSONå…ƒæ•°æ®
            # æ ¼å¼ï¼š```json\n{...}\n```
            import re

            json_pattern = r"```json\s*(\{.*?\})\s*```"
            match = re.search(json_pattern, answer_content, re.DOTALL)

            if match:
                metadata = json.loads(match.group(1))
                return {
                    "is_mistake": metadata.get("is_mistake_question"),
                    "confidence": metadata.get("confidence", 0.8),
                    "mistake_type": metadata.get("mistake_type"),
                    "knowledge_points": metadata.get("knowledge_points", []),
                    "reason": "AIå…ƒæ•°æ®æå–æˆåŠŸ",
                }
        except Exception as e:
            logger.debug(f"AIå…ƒæ•°æ®æå–å¤±è´¥: {e}")

        # ğŸ› ï¸ ç§»é™¤é”™è¯¯çš„å¯å‘å¼åˆ†æï¼ˆä¼šè¯¯åˆ¤æ­£å¸¸é—®ç­”ï¼‰
        # åŸé€»è¾‘ï¼šæ£€æŸ¥AIå›ç­”ä¸­çš„"è¿™é“é¢˜"ç­‰è¯ â†’ è¯¯åˆ¤ä¸ºé”™é¢˜
        # ä¿®å¤ï¼šä»…å½“AIæ˜ç¡®è¾“å‡ºå…ƒæ•°æ®æ—¶æ‰åˆ¤æ–­ï¼Œå¦åˆ™è¿”å›ä¸ç¡®å®š

        return {
            "is_mistake": None,
            "confidence": 0.5,
            "mistake_type": None,
            "knowledge_points": [],
            "reason": "AIæœªæä¾›æ˜ç¡®çš„é”™é¢˜åˆ¤æ–­å…ƒæ•°æ®",
        }

    async def _analyze_question_images(
        self, image_urls: List[str], question_content: str
    ) -> Dict[str, Any]:
        """
        ç­–ç•¥3ï¼šå›¾ç‰‡å†…å®¹åˆ†æ

        åˆ©ç”¨Qwen-vl-maxçš„è§†è§‰èƒ½åŠ›åˆ¤æ–­å›¾ç‰‡æ˜¯å¦ä¸ºç©ºç™½é¢˜/é”™é¢˜

        Args:
            image_urls: å›¾ç‰‡URLåˆ—è¡¨
            question_content: ç”¨æˆ·æé—®æ–‡æœ¬

        Returns:
            {
                'is_mistake': Optional[bool],
                'confidence': float,
                'has_answer': Optional[bool],
                'is_question_image': bool,
                'reason': str
            }
        """
        if not image_urls:
            return {
                "is_mistake": None,
                "confidence": 0.5,
                "has_answer": None,
                "is_question_image": False,
                "reason": "æ— å›¾ç‰‡ä¸Šä¼ ",
            }

        try:
            # ä½¿ç”¨ç®€åŒ–çš„å¯å‘å¼è§„åˆ™ï¼ˆé¿å…é¢å¤–AIè°ƒç”¨ï¼‰
            # è§„åˆ™ï¼šæœ‰å›¾ç‰‡ + æé—®æ–‡æœ¬å¾ˆçŸ­ = å¾ˆå¯èƒ½æ˜¯æ‹ç…§æé—®
            is_short_question = len(question_content.strip()) < 20

            if is_short_question:
                return {
                    "is_mistake": True,
                    "confidence": 0.85,
                    "has_answer": False,  # å‡è®¾ç©ºç™½é¢˜
                    "is_question_image": True,
                    "reason": "æ£€æµ‹åˆ°å›¾ç‰‡ä¸Šä¼ ä¸”æé—®æ–‡æœ¬ç®€çŸ­ï¼Œæ¨æµ‹ä¸ºæ‹ç…§é¢˜ç›®",
                }
            else:
                return {
                    "is_mistake": True,
                    "confidence": 0.7,
                    "has_answer": None,
                    "is_question_image": True,
                    "reason": "æ£€æµ‹åˆ°å›¾ç‰‡ä¸Šä¼ ï¼Œå¯èƒ½ä¸ºé¢˜ç›®",
                }

        except Exception as e:
            logger.warning(f"å›¾ç‰‡åˆ†æå¤±è´¥: {e}")
            return {
                "is_mistake": None,
                "confidence": 0.5,
                "has_answer": None,
                "is_question_image": False,
                "reason": f"å›¾ç‰‡åˆ†æå¼‚å¸¸: {str(e)}",
            }

    def _combine_mistake_analysis(
        self,
        keyword_result: Dict[str, Any],
        ai_intent_result: Dict[str, Any],
        image_result: Dict[str, Any],
    ) -> Tuple[bool, Dict[str, Any]]:
        """
        ç­–ç•¥4ï¼šç»¼åˆåˆ¤æ–­

        ç»¼åˆå…³é”®è¯ã€AIæ„å›¾ã€å›¾ç‰‡åˆ†æçš„ç»“æœï¼Œåšå‡ºæœ€ç»ˆåˆ¤æ–­

        ğŸ¯ åˆ¤æ–­æ ‡å‡†ï¼ˆä¼˜åŒ–åï¼Œé¿å…è¯¯åˆ¤ï¼‰ï¼š
        - å…³é”®è¯é«˜ç½®ä¿¡åº¦è¯æ®(â‰¥0.9) å¯åˆ¤å®šä¸ºé”™é¢˜
        - æˆ–è€… å›¾ç‰‡é«˜ç½®ä¿¡åº¦(â‰¥0.85) + å…³é”®è¯ä¸­ç­‰ç½®ä¿¡åº¦
        - æˆ–è€… å¤šä¸ªé«˜ç½®ä¿¡åº¦è¯æ®(â‰¥2) ä¸”å¹³å‡ç½®ä¿¡åº¦â‰¥0.8

        Args:
            keyword_result: å…³é”®è¯æ£€æµ‹ç»“æœ
            ai_intent_result: AIæ„å›¾è¯†åˆ«ç»“æœ
            image_result: å›¾ç‰‡åˆ†æç»“æœ

        Returns:
            (is_mistake, metadata)
        """
        evidences = []
        total_confidence = 0
        vote_for_mistake = 0
        vote_total = 0
        high_confidence_count = 0  # é«˜ç½®ä¿¡åº¦è¯æ®æ•°é‡

        # æ”¶é›†è¯æ®
        if keyword_result["is_mistake"] is not None:
            vote_total += 1
            if keyword_result["is_mistake"]:
                vote_for_mistake += 1
                total_confidence += keyword_result["confidence"]
                evidences.append(f"å…³é”®è¯({keyword_result['confidence']:.2f})")
                # ç»Ÿè®¡é«˜ç½®ä¿¡åº¦è¯æ®ï¼ˆâ‰¥0.85ï¼‰
                if keyword_result["confidence"] >= 0.85:
                    high_confidence_count += 1

        if ai_intent_result["is_mistake"] is not None:
            vote_total += 1
            if ai_intent_result["is_mistake"]:
                vote_for_mistake += 1
                total_confidence += ai_intent_result["confidence"]
                evidences.append(f"AIæ„å›¾({ai_intent_result['confidence']:.2f})")
                if ai_intent_result["confidence"] >= 0.85:
                    high_confidence_count += 1

        if image_result["is_mistake"] is not None:
            vote_total += 1
            if image_result["is_mistake"]:
                vote_for_mistake += 1
                total_confidence += image_result["confidence"]
                evidences.append(f"å›¾ç‰‡({image_result['confidence']:.2f})")
                if image_result["confidence"] >= 0.85:
                    high_confidence_count += 1

        # è®¡ç®—å¹³å‡ç½®ä¿¡åº¦
        avg_confidence = (
            total_confidence / vote_for_mistake if vote_for_mistake > 0 else 0
        )

        # ğŸ¯ æœ€ç»ˆåˆ¤æ–­ï¼ˆæé«˜é—¨æ§›ï¼Œé™ä½è¯¯åˆ¤ç‡ï¼‰ï¼š
        is_mistake = False
        decision_reason = ""

        if vote_total > 0 and vote_for_mistake > 0:
            # åœºæ›¯1ï¼šå…³é”®è¯é«˜ç½®ä¿¡åº¦ï¼ˆâ‰¥0.9ï¼‰â†’ ç›´æ¥åˆ¤å®š
            if (
                keyword_result.get("is_mistake")
                and keyword_result.get("confidence", 0) >= 0.9
            ):
                is_mistake = True
                decision_reason = "å…³é”®è¯é«˜ç½®ä¿¡åº¦ï¼ˆâ‰¥0.9ï¼‰"

            # åœºæ›¯2ï¼šå›¾ç‰‡é«˜ç½®ä¿¡åº¦(â‰¥0.85) + å…³é”®è¯ä¸­ç­‰ç½®ä¿¡åº¦(â‰¥0.6)
            elif (
                image_result.get("is_mistake")
                and image_result.get("confidence", 0) >= 0.85
                and keyword_result.get("is_mistake") is not False  # å…è®¸None
                and keyword_result.get("confidence", 0) >= 0.6
            ):
                is_mistake = True
                decision_reason = "å›¾ç‰‡é«˜ç½®ä¿¡åº¦ + å…³é”®è¯æ”¯æŒ"

            # åœºæ›¯3ï¼šå¤šä¸ªé«˜ç½®ä¿¡åº¦è¯æ®(â‰¥2) ä¸”å¹³å‡ç½®ä¿¡åº¦â‰¥0.8
            elif high_confidence_count >= 2 and avg_confidence >= 0.8:
                is_mistake = True
                decision_reason = f"å¤šä¸ªé«˜ç½®ä¿¡åº¦è¯æ®({high_confidence_count}ä¸ª)"

            # åœºæ›¯4ï¼šå›¾ç‰‡ + AIæ„å›¾ + å…³é”®è¯ éƒ½æ”¯æŒï¼Œä¸”å¹³å‡ç½®ä¿¡åº¦â‰¥0.75
            elif vote_for_mistake >= 3 and avg_confidence >= 0.75:
                is_mistake = True
                decision_reason = "å¤šç»´åº¦è¯æ®æ”¯æŒï¼ˆâ‰¥3ä¸ªï¼‰"

            else:
                decision_reason = f"è¯æ®ä¸è¶³ï¼šé«˜ç½®ä¿¡åº¦è¯æ®{high_confidence_count}ä¸ªï¼Œå¹³å‡ç½®ä¿¡åº¦{avg_confidence:.2f}"

        # ç¡®å®šé”™é¢˜ç±»å‹ï¼ˆä¼˜å…ˆçº§ï¼šå…³é”®è¯ > AIæ„å›¾ > å›¾ç‰‡ï¼‰
        mistake_type = (
            keyword_result.get("mistake_type")
            or ai_intent_result.get("mistake_type")
            or image_result.get("mistake_type")
            or "empty_question"
        )

        return is_mistake, {
            "is_mistake": is_mistake,
            "confidence": avg_confidence,
            "mistake_type": mistake_type,
            "reason": f"ç»¼åˆåˆ¤æ–­: {decision_reason}, è¯æ®=[{', '.join(evidences)}]",
            "evidences": evidences,
            "vote_for_mistake": vote_for_mistake,
            "vote_total": vote_total,
            "high_confidence_count": high_confidence_count,
        }

    async def _auto_create_mistake_if_needed(
        self,
        user_id: str,
        question: Question,
        answer: Answer,
        request: AskQuestionRequest,
    ) -> Optional[Dict[str, Any]]:
        """
        æ™ºèƒ½åˆ¤æ–­æ˜¯å¦éœ€è¦åˆ›å»ºé”™é¢˜ï¼ˆå¢å¼ºç‰ˆ - 4ç­–ç•¥ç»¼åˆï¼‰

        ç­–ç•¥ï¼š
        1. å…³é”®è¯æ£€æµ‹ï¼šé«˜/ä¸­ç½®ä¿¡åº¦å…³é”®è¯åŒ¹é…
        2. AIæ„å›¾è¯†åˆ«ï¼šä»AIå›ç­”ä¸­æå–å…ƒæ•°æ®
        3. å›¾ç‰‡åˆ†æï¼šåˆ©ç”¨Qwen-vl-maxè§†è§‰èƒ½åŠ›
        4. ç»¼åˆåˆ¤æ–­ï¼šå¤šç»´åº¦è¯æ®èåˆ

        ä¿æŒå‘åå…¼å®¹ï¼šæ–°é€»è¾‘å¤±è´¥æ—¶é™çº§åˆ°ç®€åŒ–è§„åˆ™
        """
        try:
            content = extract_orm_str(question, "content") or ""
            answer_content = extract_orm_str(answer, "content") or ""
            has_images = bool(request.image_urls and len(request.image_urls) > 0)

            # ========== 4ç­–ç•¥ç»¼åˆåˆ¤æ–­ ==========
            try:
                # ç­–ç•¥1ï¼šå…³é”®è¯æ£€æµ‹
                keyword_result = self._detect_mistake_keywords(content)

                # ç­–ç•¥2ï¼šAIæ„å›¾è¯†åˆ«
                ai_intent_result = self._extract_ai_mistake_metadata(answer_content)

                # ç­–ç•¥3ï¼šå›¾ç‰‡åˆ†æ
                image_result = await self._analyze_question_images(
                    request.image_urls or [], content
                )

                # ç­–ç•¥4ï¼šç»¼åˆåˆ¤æ–­
                should_create, analysis_meta = self._combine_mistake_analysis(
                    keyword_result, ai_intent_result, image_result
                )

                category = analysis_meta.get("mistake_type", "empty_question")
                confidence = analysis_meta.get("confidence", 0.0)

                logger.info(
                    f"ğŸ§  æ™ºèƒ½é”™é¢˜è¯†åˆ«: should_create={should_create}, "
                    f"confidence={confidence:.2f}, category={category}, "
                    f"reason={analysis_meta.get('reason')}"
                )

                # ç½®ä¿¡åº¦é˜ˆå€¼æ£€æŸ¥ï¼ˆä»é…ç½®è¯»å–ï¼Œé»˜è®¤0.7ï¼‰
                min_confidence = getattr(settings, "AUTO_MISTAKE_MIN_CONFIDENCE", 0.7)
                if not should_create or confidence < min_confidence:
                    logger.info(
                        f"âŒ ä¸æ»¡è¶³é”™é¢˜åˆ›å»ºæ¡ä»¶: should_create={should_create}, "
                        f"confidence={confidence:.2f} < {min_confidence}"
                    )
                    return None

            except Exception as strategy_error:
                # æ–°ç­–ç•¥å¤±è´¥æ—¶é™çº§åˆ°åŸæœ‰ç®€åŒ–è§„åˆ™
                logger.warning(f"âš ï¸ 4ç­–ç•¥ç»¼åˆåˆ¤æ–­å¤±è´¥ï¼Œé™çº§åˆ°ç®€åŒ–è§„åˆ™: {strategy_error}")

                # === é™çº§ï¼šåŸæœ‰ç®€åŒ–è§„åˆ™ï¼ˆå‘åå…¼å®¹ï¼‰===
                mistake_keywords = [
                    "ä¸ä¼š",
                    "ä¸æ‡‚",
                    "ä¸çŸ¥é“",
                    "ä¸æ˜ç™½",
                    "ä¸æ¸…æ¥š",
                    "ä¸ä¼šåš",
                    "ä¸å¤ªä¼š",
                    "ä¸å¤ªæ‡‚",
                    "çœ‹ä¸æ‡‚",
                    "é”™äº†",
                    "åšé”™",
                    "ç­”é”™",
                    "éš¾é¢˜",
                    "æœ‰éš¾åº¦",
                    "è§£ä¸å‡º",
                    "æ²¡å­¦è¿‡",
                    "ä¸ç†è§£",
                    "å¸®æˆ‘çœ‹çœ‹",
                    "å¸®æˆ‘åš",
                    "æ€ä¹ˆåš",
                    "æ€ä¹ˆè§£",
                    "æƒ³é—®",
                ]

                should_create = False
                category = "empty_question"

                if has_images:
                    should_create = True
                    category = "empty_question"
                    logger.info("ğŸ–¼ï¸ [é™çº§è§„åˆ™] æ£€æµ‹åˆ°å›¾ç‰‡ä¸Šä¼ ï¼Œè‡ªåŠ¨åˆ›å»ºé”™é¢˜")
                elif any(keyword in content for keyword in mistake_keywords):
                    should_create = True
                    if "é”™" in content or "åšé”™" in content or "ç­”é”™" in content:
                        category = "wrong_answer"
                    elif "éš¾" in content or "è§£ä¸å‡º" in content:
                        category = "hard_question"
                    else:
                        category = "empty_question"
                    logger.info(f"ğŸ”‘ [é™çº§è§„åˆ™] æ£€æµ‹åˆ°å…³é”®è¯ï¼Œcategory={category}")

                if not should_create:
                    return None
                # === é™çº§è§„åˆ™ç»“æŸ ===

            # ğŸ¯ ä½¿ç”¨AIç»“æ„åŒ–æå–é¢˜ç›®ä¿¡æ¯ï¼ˆè‡ªåŠ¨åˆ›å»ºä¹Ÿåº”ç”¨ï¼‰
            logger.info("ğŸ” è‡ªåŠ¨åˆ›å»ºé”™é¢˜ - å¼€å§‹AIç»“æ„åŒ–æå–")

            structured_data = await self._extract_structured_question(
                user_question=content,
                ai_answer=answer_content,
                image_urls=request.image_urls,
                subject=extract_orm_str(question, "subject"),
            )

            logger.info(
                f"âœ… è‡ªåŠ¨åˆ›å»º - AIæå–: success={structured_data.get('extraction_success')}, "
                f"knowledge_points={len(structured_data.get('knowledge_points', []))}"
            )

            # åˆ›å»ºé”™é¢˜è®°å½•
            from src.models.study import MistakeRecord
            from src.repositories.base_repository import BaseRepository

            mistake_repo = BaseRepository(MistakeRecord, self.db)

            # ğŸ› ï¸ ç”Ÿæˆé”™é¢˜æ•°æ®ï¼ˆä½¿ç”¨ç»“æ„åŒ–æå–çš„æ•°æ®ï¼‰
            # ä¼˜å…ˆä½¿ç”¨AIæå–çš„çŸ¥è¯†ç‚¹ï¼Œé™çº§ä½¿ç”¨è§„åˆ™æå–
            knowledge_points_list = structured_data.get("knowledge_points", [])
            if not knowledge_points_list:
                try:
                    kp_from_rules = self._extract_knowledge_points_from_answer(
                        answer_content, extract_orm_str(question, "subject") or "å…¶ä»–"
                    )
                    knowledge_points_list = [
                        kp.get("name") for kp in kp_from_rules if kp.get("name")
                    ]
                except Exception as kp_err:
                    logger.warning(f"è§„åˆ™æå–çŸ¥è¯†ç‚¹å¤±è´¥: {kp_err}")
                    knowledge_points_list = []

            # åˆå§‹åŒ–confidenceç¡®ä¿åœ¨æ‰€æœ‰ä»£ç è·¯å¾„ä¸Šéƒ½å·²å®šä¹‰
            confidence = 0.8

            ai_feedback_data = {
                "category": category,
                "auto_created": True,
                "classification": {
                    "category": category,
                    "confidence": confidence,
                    "reasoning": "åŸºäºæ™ºèƒ½åˆ¤æ–­",
                },
                "auto_created_at": datetime.now().isoformat(),
                "knowledge_points": knowledge_points_list,
                "knowledge_points_extracted": len(knowledge_points_list) > 0,
                # ğŸ¯ ç»“æ„åŒ–æå–ä¿¡æ¯
                "structured_extraction": {
                    "success": structured_data.get("extraction_success", False),
                    "confidence": structured_data.get("confidence", 0.0),
                    "question_type": structured_data.get("question_type", "æœªçŸ¥"),
                    "explanation": structured_data.get("explanation", ""),
                    "is_fallback": structured_data.get("fallback", False),
                },
            }

            # ğŸ¯ æ ¹æ®é”™é¢˜ç±»å‹ç¡®å®š source å­—æ®µå€¼
            source_mapping = {
                "empty_question": "learning_empty",  # ä¸ä¼šåšçš„é¢˜
                "wrong_answer": "learning_wrong",  # ç­”é”™çš„é¢˜
                "hard_question": "learning_hard",  # æœ‰éš¾åº¦çš„é¢˜
            }
            source = source_mapping.get(category, "learning")  # é»˜è®¤ learning

            logger.info(f"ğŸ“‹ é”™é¢˜åˆ†ç±»: category={category}, source={source}")

            # ğŸ”§ [ä¿®å¤] æ™ºèƒ½æ¨æ–­ç§‘ç›®ï¼Œé¿å…é»˜è®¤"å…¶ä»–"å¯¼è‡´ç­›é€‰å¤±è´¥
            question_subject = extract_orm_str(question, "subject")
            if not question_subject:
                # å°è¯•ä»å†…å®¹æ¨æ–­ç§‘ç›®
                inferred_subject = self._infer_subject_from_content(content)
                logger.info(
                    f"ğŸ“š ç§‘ç›®æ¨æ–­: question.subjectä¸ºç©ºï¼Œä»å†…å®¹æ¨æ–­ä¸º '{inferred_subject}'"
                )
                question_subject = inferred_subject

            # ä½¿ç”¨ç»“æ„åŒ–æå–çš„çº¯å‡€é¢˜ç›®å†…å®¹
            clean_question = structured_data.get("question_content", content)

            mistake_data = {
                "user_id": user_id,
                "source": source,  # ğŸ¯ åŠ¨æ€è®¾ç½® source
                "source_question_id": str(extract_orm_uuid_str(question, "id")),
                # åŸºæœ¬ä¿¡æ¯
                "subject": question_subject,
                "title": self._generate_mistake_title(clean_question),
                "ocr_text": clean_question,  # ğŸ¯ ä½¿ç”¨çº¯å‡€çš„é¢˜ç›®å†…å®¹
                "image_urls": (
                    json.dumps(request.image_urls) if request.image_urls else None
                ),
                # AIåˆ†æä¿¡æ¯ï¼ˆåŒ…å«çŸ¥è¯†ç‚¹å’Œç»“æ„åŒ–æå–ç»“æœï¼‰
                "ai_feedback": json.dumps(ai_feedback_data),
                "knowledge_points": knowledge_points_list,  # ğŸ¯ ä½¿ç”¨æå–çš„çŸ¥è¯†ç‚¹
                # å­¦ç”Ÿç­”æ¡ˆï¼ˆå¯é€‰ï¼‰
                "student_answer": None,
                "correct_answer": structured_data.get("correct_answer")
                or self._extract_correct_answer(
                    answer_content
                ),  # ğŸ¯ ä¼˜å…ˆä½¿ç”¨AIæå–çš„ç­”æ¡ˆ
                # å¤ä¹ ç›¸å…³
                "mastery_status": "learning",  # ğŸ› ï¸ ä½¿ç”¨æ¨¡å‹ä¸­å®šä¹‰çš„å€¼
                "next_review_at": datetime.now() + timedelta(days=1),
                "review_count": 0,
                "correct_count": 0,
                "difficulty_level": structured_data.get(
                    "difficulty_level", 2
                ),  # ğŸ¯ ä½¿ç”¨AIåˆ¤æ–­çš„éš¾åº¦
            }

            # åˆ›å»ºé”™é¢˜
            mistake = await mistake_repo.create(mistake_data)
            mistake_id_str = extract_orm_uuid_str(mistake, "id")  # ğŸ”§ ç«‹å³æå–ID

            # ğŸ¯ åˆ›å»ºé”™é¢˜åç«‹å³å…³è”çŸ¥è¯†ç‚¹
            try:
                mistake_id = UUID(mistake_id_str)
                # ğŸ¯ ä½¿ç”¨ç»“æ„åŒ–æå–çš„çŸ¥è¯†ç‚¹è¿›è¡Œå…³è”
                await self._trigger_knowledge_association(
                    mistake_id=mistake_id,
                    user_id=UUID(user_id),
                    subject=mistake_data["subject"],
                    ocr_text=clean_question,  # ğŸ¯ ä½¿ç”¨çº¯å‡€çš„é¢˜ç›®å†…å®¹
                    ai_feedback=ai_feedback_data,
                )
                logger.info(f"ğŸ”— çŸ¥è¯†ç‚¹å…³è”å·²è§¦å‘: mistake_id={mistake_id}")
            except Exception as ka_err:
                logger.warning(f"è§¦å‘çŸ¥è¯†ç‚¹å…³è”å¤±è´¥ï¼Œä½†ä¸å½±å“é”™é¢˜åˆ›å»º: {ka_err}")

            # è¿”å›é”™é¢˜ä¿¡æ¯
            return {
                "id": mistake_id_str,
                "category": category,
                "next_review_date": (datetime.now() + timedelta(days=1)).isoformat(),
                "subject": mistake_data["subject"],
                "auto_created": True,
            }

        except Exception as e:
            logger.error(f"é”™é¢˜è‡ªåŠ¨åˆ›å»ºå¤±è´¥: {str(e)}", exc_info=True)
            return None

    def _extract_knowledge_points_from_answer(
        self, answer_content: str, subject: str
    ) -> List[Dict[str, Any]]:
        """
        ä» AI å›ç­”ä¸­æå–çŸ¥è¯†ç‚¹

        ç­–ç•¥ï¼š
        1. å…³é”®è¯åŒ¹é…ï¼šæŸ¥æ‰¾å¸¸è§çŸ¥è¯†ç‚¹å…³é”®è¯
        2. æ¨¡å¼åŒ¹é…ï¼šæå–â€œæ¶‰åŠçŸ¥è¯†ç‚¹â€ã€â€œè€ƒæŸ¥â€ç­‰åé¢çš„å†…å®¹
        3. å­¦ç§‘ç‰¹å®šçŸ¥è¯†ç‚¹åº“
        """
        knowledge_points = []

        # å­¦ç§‘çŸ¥è¯†ç‚¹åº“ï¼ˆå¯æ‰©å±•ï¼‰
        knowledge_keywords_db = {
            "æ•°å­¦": [
                "å‡½æ•°",
                "æ–¹ç¨‹",
                "ä¸ç­‰å¼",
                "å‡ ä½•",
                "ä¸‰è§’å½¢",
                "åœ†",
                "äºŒæ¬¡å‡½æ•°",
                "ä¸€æ¬¡å‡½æ•°",
                "ä¸€å…ƒäºŒæ¬¡æ–¹ç¨‹",
                "å› å¼åˆ†è§£",
                "å¹³é¢ç›´è§’åæ ‡ç³»",
                "ç›´çº¿",
                "åœ†çš„æ–¹ç¨‹",
                "è§£ä¸‰è§’å½¢",
                "æ¦‚ç‡",
                "ç»Ÿè®¡",
                "å‹¾è‚¡å®šç†",
                "ç›¸ä¼¼ä¸‰è§’å½¢",
                "å…¨ç­‰ä¸‰è§’å½¢",
                "äºŒæ¬¡å‡½æ•°å›¾åƒ",
                "å¯¹ç§°è½´",
                "é¡¶ç‚¹åæ ‡",
                "äºŒæ¬¡å‡½æ•°æ€§è´¨",
            ],
            "è‹±è¯­": [
                "è¯­æ³•",
                "è¯æ±‡",
                "é˜…è¯»ç†è§£",
                "å†™ä½œ",
                "å¬åŠ›",
                "å£è¯­",
                "æ—¶æ€",
                "ä»å¥",
                "éè°“è¯­åŠ¨è¯",
                "å®šè¯­ä»å¥",
            ],
            "è¯­æ–‡": [
                "é˜…è¯»ç†è§£",
                "ä½œæ–‡",
                "å¤è¯—è¯",
                "æ–‡è¨€æ–‡",
                "è¯­æ³•",
                "ä¿®è¾æ‰‹æ³•",
                "è¯è¯­ç§¯ç´¯",
                "è¯­å¥ç†è§£",
            ],
            "ç‰©ç†": [
                "åŠ›å­¦",
                "ç”µå­¦",
                "å…‰å­¦",
                "çƒ­å­¦",
                "æœºæ¢°è¿åŠ¨",
                "ç‰›é¡¿è¿åŠ¨å®šå¾‹",
                "æ¬§å§†å®šå¾‹",
                "ç”µè·¯åˆ†æ",
            ],
            "åŒ–å­¦": [
                "åŒ–å­¦æ–¹ç¨‹å¼",
                "æ°§åŒ–è¿˜åŸ",
                "é…¸ç¢±ç›",
                "å…ƒç´ å‘¨æœŸè¡¨",
                "åŒ–å­¦é”®",
                "æœ‰æœºåŒ–å­¦",
                "åŒ–å­¦å¹³è¡¡",
            ],
        }

        keywords = knowledge_keywords_db.get(subject, [])

        # ç­–ç•¥ 1ï¼šå…³é”®è¯åŒ¹é…
        for keyword in keywords:
            if keyword in answer_content:
                knowledge_points.append(
                    {
                        "name": keyword,
                        "relevance": 0.8,
                        "error_type": "concept_misunderstanding",
                        "extraction_method": "keyword_match",
                    }
                )

        # ç­–ç•¥ 2ï¼šæ¨¡å¼åŒ¹é…
        import re

        patterns = [
            r"æ¶‰åŠ[çŸ¥è¯†ç‚¹åˆ°äº†]?[:ï¼š]?([^ã€‚ï¼Œï¼Œã€\n]+)",
            r"è€ƒæŸ¥[çŸ¥è¯†ç‚¹åˆ°äº†]?[:ï¼š]?([^ã€‚ï¼Œï¼Œã€\n]+)",
            r"ä½¿ç”¨[çŸ¥è¯†ç‚¹åˆ°äº†]?[:ï¼š]?([^ã€‚ï¼Œï¼Œã€\n]+)",
            r"åº”ç”¨[çŸ¥è¯†ç‚¹åˆ°äº†]?[:ï¼š]?([^ã€‚ï¼Œï¼Œã€\n]+)",
        ]

        for pattern in patterns:
            matches = re.findall(pattern, answer_content)
            for match in matches:
                # æ¸…ç†æå–çš„æ–‡æœ¬
                kp_name = match.strip()
                if len(kp_name) > 2 and len(kp_name) < 20:  # è¿‡æ»¤å¤ªçŸ­æˆ–å¤ªé•¿çš„
                    knowledge_points.append(
                        {
                            "name": kp_name,
                            "relevance": 0.9,
                            "error_type": "concept_misunderstanding",
                            "extraction_method": "pattern_match",
                        }
                    )

        # å»é‡ï¼ˆæ ¹æ® name å­—æ®µï¼‰
        seen = set()
        unique_kps = []
        for kp in knowledge_points:
            if kp["name"] not in seen:
                seen.add(kp["name"])
                unique_kps.append(kp)

        return unique_kps[:5]  # æœ€å¤šè¿•å› 5 ä¸ªçŸ¥è¯†ç‚¹

    async def _trigger_knowledge_association(
        self,
        mistake_id: UUID,
        user_id: UUID,
        subject: str,
        ocr_text: Optional[str],
        ai_feedback: Dict[str, Any],
    ) -> None:
        """
        è§¦å‘çŸ¥è¯†å›¾è°±æœåŠ¡è¿›è¡ŒçŸ¥è¯†ç‚¹å…³è” + å®æ—¶å¿«ç…§æ›´æ–°
        
        Phase 1: å®æ—¶åŒæ­¥æœºåˆ¶ä¿®å¤
        - åˆ›å»ºçŸ¥è¯†ç‚¹å…³è”åç«‹å³æ›´æ–°çŸ¥è¯†å›¾è°±å¿«ç…§
        - å¿«ç…§æ›´æ–°å¤±è´¥ä¸å½±å“é”™é¢˜åˆ›å»ºæµç¨‹
        """
        try:
            from src.services.knowledge_graph_service import KnowledgeGraphService

            kg_service = KnowledgeGraphService(self.db, self.bailian_service)

            # 1. è°ƒç”¨çŸ¥è¯†å›¾è°±æœåŠ¡è¿›è¡Œå…³è”
            associations = await kg_service.analyze_and_associate_knowledge_points(
                mistake_id=mistake_id,
                user_id=user_id,
                subject=subject,
                ocr_text=ocr_text,
                ai_feedback=ai_feedback,
            )

            if associations:
                logger.info(
                    f"âœ… çŸ¥è¯†ç‚¹å…³è”æˆåŠŸ: mistake_id={mistake_id}, "
                    f"å…³è”æ•°é‡={len(associations)}"
                )
                
                # ğŸ†• Phase 1: ç«‹å³æ›´æ–°çŸ¥è¯†å›¾è°±å¿«ç…§
                try:
                    await kg_service.create_knowledge_graph_snapshot(
                        user_id=user_id,
                        subject=subject,
                        period_type="realtime_update",
                        auto_commit=False  # ä½¿ç”¨å·²æœ‰äº‹åŠ¡,ç»Ÿä¸€æäº¤
                    )
                    logger.info(
                        f"âœ… çŸ¥è¯†å›¾è°±å¿«ç…§å®æ—¶æ›´æ–°æˆåŠŸ: user={user_id}, "
                        f"subject={subject}, trigger=mistake_create"
                    )
                except Exception as snapshot_error:
                    # å¿«ç…§æ›´æ–°å¤±è´¥ä¸å½±å“é”™é¢˜åˆ›å»º
                    logger.warning(
                        f"âš ï¸ çŸ¥è¯†å›¾è°±å¿«ç…§å®æ—¶æ›´æ–°å¤±è´¥(ä¸å½±å“é”™é¢˜åˆ›å»º): "
                        f"user={user_id}, subject={subject}, error={snapshot_error}"
                    )
            else:
                logger.warning(f"âš ï¸ æœªèƒ½ä¸ºé”™é¢˜ {mistake_id} å…³è”çŸ¥è¯†ç‚¹")

        except Exception as e:
            logger.error(f"çŸ¥è¯†ç‚¹å…³è”å¤±è´¥: {e}", exc_info=True)
            raise

    def _infer_subject_from_content(self, content: str) -> str:
        """
        ä»å†…å®¹æ™ºèƒ½æ¨æ–­ç§‘ç›®

        åŸºäºå…³é”®è¯åŒ¹é…æ¨æ–­ï¼Œé»˜è®¤æ•°å­¦ï¼ˆK12æœ€å¸¸è§ç§‘ç›®ï¼‰

        Args:
            content: OCRè¯†åˆ«çš„é—®é¢˜å†…å®¹

        Returns:
            æ¨æ–­çš„ç§‘ç›®åç§°
        """
        if not content:
            return "æ•°å­¦"

        content_lower = content.lower()

        # ç§‘ç›®å…³é”®è¯åº“
        subject_keywords = {
            "æ•°å­¦": [
                "æ–¹ç¨‹",
                "å‡½æ•°",
                "å‡ ä½•",
                "ä¸‰è§’",
                "ä»£æ•°",
                "å¾®ç§¯åˆ†",
                "å¯¼æ•°",
                "ç§¯åˆ†",
                "åœ†",
                "ç›´çº¿",
                "æŠ›ç‰©çº¿",
                "æ¤­åœ†",
                "åŒæ›²çº¿",
                "æ­£å¼¦",
                "ä½™å¼¦",
                "æ­£åˆ‡",
                "sin",
                "cos",
                "tan",
                "x",
                "y",
                "z",
                "f(x)",
                "Ï€",
                "âˆ«",
                "âˆ‘",
                "æ±‚è§£",
                "è®¡ç®—",
                "è¯æ˜",
                "é¢ç§¯",
                "ä½“ç§¯",
                "é•¿åº¦",
                "çƒ",
                "åœ†æŸ±",
                "æ£±é”¥",
                "ç«‹æ–¹ä½“",
            ],
            "ç‰©ç†": [
                "åŠ›",
                "é€Ÿåº¦",
                "åŠ é€Ÿåº¦",
                "è´¨é‡",
                "èƒ½é‡",
                "åŠŸ",
                "åŠŸç‡",
                "ç‰›é¡¿",
                "ç„¦è€³",
                "ç“¦ç‰¹",
                "æ¬§å§†",
                "ä¼ç‰¹",
                "å®‰åŸ¹",
                "ç”µè·¯",
                "ç£åœº",
                "ç”µåœº",
                "ç”µæµ",
                "ç”µå‹",
                "ç”µé˜»",
                "å…‰",
                "æ³¢",
                "å£°",
                "çƒ­",
                "æ¸©åº¦",
                "å‹å¼º",
                "F=",
                "W=",
                "P=",
                "E=",
                "v=",
                "a=",
            ],
            "åŒ–å­¦": [
                "åŒ–å­¦å¼",
                "åŒ–å­¦ååº”",
                "åˆ†å­",
                "åŸå­",
                "ç¦»å­",
                "å…ƒç´ ",
                "æ°§åŒ–",
                "è¿˜åŸ",
                "é…¸",
                "ç¢±",
                "ç›",
                "pH",
                "Hâ‚‚O",
                "COâ‚‚",
                "Oâ‚‚",
                "Hâ‚‚",
                "Na",
                "Cl",
                "æ‘©å°”",
                "æº¶æ¶²",
                "æµ“åº¦",
                "è´¨é‡åˆ†æ•°",
                "ååº”æ–¹ç¨‹å¼",
                "åŒ–åˆç‰©",
                "å•è´¨",
            ],
            "è‹±è¯­": [
                "grammar",
                "vocabulary",
                "tense",
                "sentence",
                "translate",
                "reading",
                "writing",
                "speaking",
                "verb",
                "noun",
                "adjective",
                "adverb",
                "past",
                "present",
                "future",
                "passive",
                "what",
                "where",
                "when",
                "who",
                "how",
                "why",
            ],
            "è¯­æ–‡": [
                "ä½œæ–‡",
                "é˜…è¯»ç†è§£",
                "å¤è¯—",
                "æ–‡è¨€æ–‡",
                "ç°ä»£æ–‡",
                "ä½œè€…",
                "ä¸»é¢˜",
                "æ‰‹æ³•",
                "ä¿®è¾",
                "æ¯”å–»",
                "æ‹Ÿäºº",
                "æ®µè½",
                "ä¸­å¿ƒæ€æƒ³",
                "å†™ä½œ",
                "æ–‡ç« ",
                "æœ—è¯µ",
                "èƒŒè¯µ",
                "é»˜å†™",
                "å¤æ–‡",
                "è¯—è¯",
            ],
            "ç”Ÿç‰©": [
                "ç»†èƒ",
                "åŸºå› ",
                "é—ä¼ ",
                "æŸ“è‰²ä½“",
                "DNA",
                "RNA",
                "å…‰åˆä½œç”¨",
                "å‘¼å¸ä½œç”¨",
                "æ–°é™ˆä»£è°¢",
                "ç”Ÿæ€",
                "ç¯å¢ƒ",
                "ç‰©ç§",
                "è¿›åŒ–",
                "å™¨å®˜",
                "ç»„ç»‡",
                "ç³»ç»Ÿ",
                "è¡€æ¶²",
                "ç¥ç»",
            ],
        }

        # ç»Ÿè®¡æ¯ä¸ªç§‘ç›®çš„å…³é”®è¯åŒ¹é…æ•°
        scores = {}
        for subject, keywords in subject_keywords.items():
            count = sum(1 for kw in keywords if kw in content_lower)
            if count > 0:
                scores[subject] = count

        # è¿”å›åŒ¹é…æœ€å¤šçš„ç§‘ç›®ï¼Œå¦‚æœæ²¡æœ‰åŒ¹é…åˆ™é»˜è®¤æ•°å­¦
        if scores:
            inferred = max(scores, key=lambda x: scores[x])
            logger.debug(f"ç§‘ç›®æ¨æ–­: {inferred} (åŒ¹é…åˆ†æ•°: {scores})")
            return inferred

        logger.debug("ç§‘ç›®æ¨æ–­: æ— æ˜æ˜¾å…³é”®è¯ï¼Œé»˜è®¤æ•°å­¦")
        return "æ•°å­¦"

    # ========== ä½œä¸šæ‰¹æ”¹æ ¸å¿ƒæ–¹æ³• ==========

    def _is_homework_correction_scenario(
        self,
        question_type: Optional[str],
        content: str,
        image_urls: Optional[List[str]],
    ) -> bool:
        """
        åˆ¤æ–­æ˜¯å¦ä¸ºä½œä¸šæ‰¹æ”¹åœºæ™¯

        Args:
            question_type: é—®é¢˜ç±»å‹ï¼ˆå­—ç¬¦ä¸²å€¼æˆ–Noneï¼‰
            content: é—®é¢˜å†…å®¹
            image_urls: å›¾ç‰‡åˆ—è¡¨

        Returns:
            bool: æ˜¯å¦ä¸ºæ‰¹æ”¹åœºæ™¯
        """
        # æ£€æŸ¥é—®é¢˜ç±»å‹
        if question_type == "homework_help":
            logger.info("ğŸ” æ‰¹æ”¹åœºæ™¯æ£€æµ‹: question_type=HOMEWORK_HELP â†’ True")
            return True

        # æ£€æŸ¥å†…å®¹ä¸­çš„å…³é”®è¯
        correction_keywords = [
            "æ‰¹æ”¹",
            "æ”¹é”™",
            "ä½œä¸š",
            "é¢˜ç›®",
            "ç­”æ¡ˆ",
            "å¯¹ä¸å¯¹",
            "è¿™é“é¢˜",
            "å¸®æˆ‘æ£€æŸ¥",
            "çœ‹çœ‹å¯¹ä¸å¯¹",
            "è¿™ä»½ä½œä¸š",
            "é€é¢˜",
            "é€ä¸ª",
        ]

        content_lower = content.lower()
        matched_keywords = [kw for kw in correction_keywords if kw in content_lower]
        has_correction_keyword = len(matched_keywords) > 0

        # æœ‰å›¾ç‰‡ + åŒ…å«æ‰¹æ”¹å…³é”®è¯ = æ‰¹æ”¹åœºæ™¯
        has_images = bool(image_urls and len(image_urls) > 0)

        # ğŸ” [è°ƒè¯•] è¯¦ç»†åˆ¤æ–­æ—¥å¿—
        logger.info(
            f"ğŸ” æ‰¹æ”¹åœºæ™¯æ£€æµ‹è¯¦æƒ…: "
            f"has_images={has_images}, "
            f"image_count={len(image_urls or [])}, "
            f"has_keyword={has_correction_keyword}, "
            f"matched_keywords={matched_keywords}, "
            f"content='{content[:50]}...'"
        )

        if has_images and has_correction_keyword:
            logger.info("ğŸ” æ‰¹æ”¹åœºæ™¯æ£€æµ‹: has_images AND has_keyword â†’ True")
            return True

        logger.info("ğŸ” æ‰¹æ”¹åœºæ™¯æ£€æµ‹: æ¡ä»¶ä¸æ»¡è¶³ â†’ False")
        return False

    async def _call_ai_for_homework_correction(
        self,
        image_urls: List[str],
        subject: str,
        user_hint: Optional[str] = None,
    ) -> Optional[HomeworkCorrectionResult]:
        """
        è°ƒç”¨ AI è¿›è¡Œä½œä¸šæ‰¹æ”¹

        Args:
            image_urls: ä½œä¸šå›¾ç‰‡ URLs
            subject: å­¦ç§‘
            user_hint: ç”¨æˆ·æç¤ºä¿¡æ¯

        Returns:
            HomeworkCorrectionResult: æ‰¹æ”¹ç»“æœï¼Œå¤±è´¥æ—¶è¿”å› None
        """
        if not image_urls:
            logger.warning("æ‰¹æ”¹å¤±è´¥ï¼šæ²¡æœ‰æä¾›å›¾ç‰‡")
            return None

        try:
            # æ„å»º Prompt
            prompt = HOMEWORK_CORRECTION_PROMPT.format(subject=subject)
            if user_hint:
                prompt += f"\n\nå­¦ç”Ÿæç¤ºï¼š{user_hint}"
                logger.debug(f"ğŸ“Œ æ·»åŠ ç”¨æˆ·æç¤º: {user_hint[:50]}...")

            # æ„å»ºæ¶ˆæ¯
            messages = [
                {
                    "role": "user",
                    "content": prompt,
                    "image_urls": image_urls,
                }
            ]

            logger.info(
                f"ğŸ“ [ä½œä¸šæ‰¹æ”¹] å¼€å§‹: subject={subject}, "
                f"image_count={len(image_urls)}, "
                f"prompt_length={len(prompt)}"
            )
            logger.debug(f"ğŸ“„ Promptå†…å®¹: {prompt[:200]}...")

            # è°ƒç”¨ AI
            start_time = time.time()
            logger.info("ğŸš€ [AIè°ƒç”¨] è°ƒç”¨ç™¾ç‚¼AIæ‰¹æ”¹æœåŠ¡...")

            ai_response = await self.bailian_service.chat_completion(
                messages=messages,
                max_tokens=2000,  # æ‰¹æ”¹å¯èƒ½éœ€è¦æ›´å¤š tokens
                temperature=0.3,  # é™ä½æ¸©åº¦ä»¥è·å¾—æ›´å‡†ç¡®çš„ç»“æœ
                top_p=0.8,
            )

            elapsed_time = time.time() - start_time
            logger.info(
                f"â±ï¸ [AIå“åº”] è€—æ—¶: {elapsed_time:.2f}s, "
                f"tokens_used={ai_response.tokens_used if hasattr(ai_response, 'tokens_used') else 'N/A'}"
            )

            if not ai_response.success:
                logger.error(
                    f"âŒ [AIå¤±è´¥] æ‰¹æ”¹å¤±è´¥: {ai_response.error_message}, "
                    f"è€—æ—¶: {elapsed_time:.2f}s"
                )
                return None

            # è§£æ AI å“åº”
            response_content = ai_response.content or ""
            logger.info(
                f"ğŸ“¥ [AIå“åº”] æ¥æ”¶å†…å®¹: length={len(response_content)}, "
                f"preview={response_content[:100]}..."
            )
            logger.debug(f"ğŸ“„ å®Œæ•´å“åº”: {response_content}")

            # å°è¯•æå– JSON
            json_str = ""  # åˆå§‹åŒ–ä»¥ç¡®ä¿åœ¨å¼‚å¸¸å¤„ç†ä¸­å¯ç”¨
            try:
                logger.info("ğŸ” [JSONè§£æ] å¼€å§‹æå–JSONæ•°æ®...")

                # æŸ¥æ‰¾ JSON å—
                json_start = response_content.find("{")
                json_end = response_content.rfind("}") + 1

                if json_start == -1 or json_end <= json_start:
                    logger.error(
                        f"âŒ [JSONè§£æ] AIå“åº”ä¸­æœªæ‰¾åˆ°JSONæ ¼å¼, "
                        f"response_length={len(response_content)}"
                    )
                    return None

                json_str = response_content[json_start:json_end]
                logger.debug(f"ğŸ“‹ æå–çš„JSON: {json_str[:200]}...")

                result_dict = json.loads(json_str)
                logger.info(
                    f"âœ… [JSONè§£æ] æˆåŠŸ, "
                    f"corrections_count={len(result_dict.get('corrections', []))}"
                )

                # æ„å»ºæ‰¹æ”¹ç»“æœ
                logger.info("ğŸ”¨ [æ•°æ®æ„å»º] æ„å»ºæ‰¹æ”¹ç»“æœå¯¹è±¡...")
                corrections = []
                for idx, item in enumerate(result_dict.get("corrections", []), 1):
                    correction = QuestionCorrectionItem(
                        question_number=item.get("question_number", 0),
                        question_type=item.get("question_type", ""),
                        is_unanswered=item.get("is_unanswered", False),
                        student_answer=item.get("student_answer"),
                        correct_answer=item.get("correct_answer"),
                        error_type=item.get("error_type"),
                        explanation=item.get("explanation"),
                        knowledge_points=item.get("knowledge_points", []),
                        score=item.get("score"),
                        question_text=item.get("question_text", ""),
                    )
                    corrections.append(correction)
                    logger.debug(
                        f"  é¢˜ç›®{idx}: Q{correction.question_number}, "
                        f"type={correction.question_type}, "
                        f"error={correction.error_type or 'None'}"
                    )

                correction_result = HomeworkCorrectionResult(
                    corrections=corrections,
                    summary=result_dict.get("summary"),
                    overall_score=result_dict.get("overall_score"),
                    total_questions=result_dict.get(
                        "total_questions", len(corrections)
                    ),
                    unanswered_count=result_dict.get("unanswered_count", 0),
                    error_count=result_dict.get("error_count", 0),
                )

                logger.info(
                    f"âœ… [æ‰¹æ”¹å®Œæˆ] ä½œä¸šæ‰¹æ”¹æˆåŠŸ: "
                    f"total_questions={len(corrections)}, "
                    f"unanswered={correction_result.unanswered_count}, "
                    f"errors={correction_result.error_count}, "
                    f"overall_score={correction_result.overall_score}, "
                    f"total_time={elapsed_time:.2f}s"
                )

                return correction_result

            except json.JSONDecodeError as e:
                json_str_preview = json_str[:200] if json_str else "N/A"
                logger.error(
                    f"âŒ [JSONè§£æ] è§£æå¤±è´¥: {str(e)}, "
                    f"json_preview={json_str_preview}",
                    exc_info=True,
                )
                return None

        except BailianServiceError as e:
            logger.error(f"âŒ [AIæœåŠ¡] ç™¾ç‚¼æœåŠ¡å¼‚å¸¸: {str(e)}", exc_info=True)
            return None
        except Exception as e:
            logger.error(
                f"âŒ [å¼‚å¸¸] ä½œä¸šæ‰¹æ”¹æœªçŸ¥å¼‚å¸¸: {type(e).__name__}: {str(e)}",
                exc_info=True,
            )
            return None

    async def _update_knowledge_mastery(
        self, user_id: str, subject: str, knowledge_points: List[str]
    ) -> None:
        """
        åˆ›å»ºæˆ–æ›´æ–°çŸ¥è¯†ç‚¹æŒæ¡åº¦è®°å½•

        Args:
            user_id: ç”¨æˆ·ID
            subject: å­¦ç§‘
            knowledge_points: çŸ¥è¯†ç‚¹åˆ—è¡¨
        """
        from datetime import datetime

        from sqlalchemy import select

        from src.models.study import KnowledgeMastery

        logger.info(
            f"ğŸ“Š [çŸ¥è¯†ç‚¹æŒæ¡åº¦] å¼€å§‹æ›´æ–°: user={user_id}, subject={subject}, "
            f"knowledge_points={knowledge_points}"
        )

        for kp in knowledge_points:
            try:
                # æŸ¥æ‰¾ç°æœ‰è®°å½•
                stmt = select(KnowledgeMastery).where(
                    KnowledgeMastery.user_id == user_id,
                    KnowledgeMastery.subject == subject,
                    KnowledgeMastery.knowledge_point == kp,
                )
                result = await self.db.execute(stmt)
                existing = result.scalar_one_or_none()

                if existing:
                    # æ›´æ–°ç°æœ‰è®°å½•ï¼šé”™è¯¯æ¬¡æ•°+1
                    # æ³¨æ„ï¼šSQLAlchemy ORMå¯¹è±¡åœ¨è¿è¡Œæ—¶è¡Œä¸ºæ­£ç¡®ï¼Œç±»å‹æ£€æŸ¥å™¨å¯èƒ½æŠ¥é”™
                    from decimal import Decimal

                    # æå–å€¼å¹¶è¿›è¡Œè®¡ç®—
                    mistake_count_val: int = int(existing.mistake_count or 0)  # type: ignore
                    total_attempts_val: int = int(existing.total_attempts or 0)  # type: ignore

                    mistake_count_val += 1
                    total_attempts_val += 1

                    # æ›´æ–°ORMå¯¹è±¡å±æ€§ï¼ˆè¿è¡Œæ—¶æ­£ç¡®ï¼‰
                    existing.mistake_count = mistake_count_val  # type: ignore
                    existing.total_attempts = total_attempts_val  # type: ignore
                    existing.last_practiced_at = datetime.now()  # type: ignore
                    # é‡æ–°è®¡ç®—æŒæ¡åº¦ï¼ˆé”™è¯¯æ¬¡æ•°è¶Šå¤šï¼ŒæŒæ¡åº¦è¶Šä½ï¼‰
                    # mastery_level = 1 / (1 + mistake_count * 0.1)
                    new_mastery = Decimal(
                        str(max(0.0, 1.0 / (1.0 + float(mistake_count_val) * 0.1)))
                    )
                    existing.mastery_level = new_mastery  # type: ignore
                    logger.info(
                        f"    âœ… æ›´æ–°: {kp}, mistake_count={mistake_count_val}, "
                        f"mastery_level={float(new_mastery):.2f}"
                    )
                else:
                    # åˆ›å»ºæ–°è®°å½•
                    new_mastery = KnowledgeMastery(
                        user_id=user_id,
                        subject=subject,
                        knowledge_point=kp,
                        mastery_level=0.5,  # åˆå§‹æŒæ¡åº¦50%
                        confidence_level=0.3,  # åˆå§‹ç½®ä¿¡åº¦30%
                        mistake_count=1,
                        correct_count=0,
                        total_attempts=1,
                        last_practiced_at=datetime.now(),
                    )
                    self.db.add(new_mastery)
                    logger.info(f"    â• åˆ›å»º: {kp}, mastery_level=0.5")

            except Exception as e:
                logger.error(f"âš ï¸ æ›´æ–°çŸ¥è¯†ç‚¹æŒæ¡åº¦å¤±è´¥ ({kp}): {e}", exc_info=True)

        # ğŸ¯ åˆ·æ–°åˆ°æ•°æ®åº“ä½†ä¸æäº¤ï¼Œè®©è°ƒç”¨æ–¹ç»Ÿä¸€ç®¡ç†äº‹åŠ¡
        await self.db.flush()
        logger.info(f"âœ… çŸ¥è¯†ç‚¹æŒæ¡åº¦æ›´æ–°å®Œæˆ: {len(knowledge_points)}ä¸ª")

    def _infer_subject_from_knowledge_points(self, knowledge_points: List[str]) -> str:
        """
        ä»çŸ¥è¯†ç‚¹åˆ—è¡¨æ¨æ–­å­¦ç§‘

        Args:
            knowledge_points: çŸ¥è¯†ç‚¹åˆ—è¡¨

        Returns:
            str: æ¨æ–­çš„å­¦ç§‘ï¼ˆenglish/math/chinese/physics/chemistryç­‰ï¼‰
        """
        if not knowledge_points:
            return "math"  # é»˜è®¤æ•°å­¦

        # å°†æ‰€æœ‰çŸ¥è¯†ç‚¹åˆå¹¶
        all_text = " ".join(knowledge_points).lower()

        # å­¦ç§‘å…³é”®è¯æ˜ å°„
        subject_keywords = {
            "english": [
                "è‹±è¯­",
                "grammar",
                "vocabulary",
                "reading",
                "writing",
                "è¯­æ³•",
                "è¯æ±‡",
                "é˜…è¯»",
                "å†™ä½œ",
                "å½¢å®¹è¯",
                "å‰¯è¯",
                "åŠ¨è¯",
                "åè¯",
                "æ—¶æ€",
                "å¥å‹",
                "è¿è¯",
            ],
            "chinese": [
                "è¯­æ–‡",
                "æ±‰è¯­",
                "å¤è¯—",
                "æ–‡è¨€æ–‡",
                "ä½œæ–‡",
                "é˜…è¯»ç†è§£",
                "ä¿®è¾",
                "è¯—è¯",
            ],
            "math": [
                "æ•°å­¦",
                "å‡ ä½•",
                "ä»£æ•°",
                "å‡½æ•°",
                "æ–¹ç¨‹",
                "è®¡ç®—",
                "å…¬å¼",
                "è§’",
                "ä¸‰è§’å½¢",
                "åœ†",
                "é¢ç§¯",
                "ä½“ç§¯",
            ],
            "physics": [
                "ç‰©ç†",
                "åŠ›å­¦",
                "è¿åŠ¨",
                "é€Ÿåº¦",
                "åŠ é€Ÿåº¦",
                "ç‰›é¡¿",
                "ç”µå­¦",
                "ç£åœº",
            ],
            "chemistry": ["åŒ–å­¦", "å…ƒç´ ", "åˆ†å­", "åŸå­", "ååº”", "æº¶æ¶²", "é…¸ç¢±"],
        }

        # ç»Ÿè®¡æ¯ä¸ªå­¦ç§‘çš„åŒ¹é…åº¦
        scores = {}
        for subject, keywords in subject_keywords.items():
            score = sum(1 for kw in keywords if kw in all_text)
            if score > 0:
                scores[subject] = score

        # è¿”å›å¾—åˆ†æœ€é«˜çš„å­¦ç§‘
        if scores:
            inferred = max(scores, key=lambda x: scores[x])
            logger.info(
                f"ğŸ” å­¦ç§‘æ¨æ–­: {knowledge_points[:3]}... â†’ {inferred} (åŒ¹é…åº¦: {scores})"
            )
            return inferred

        return "math"  # é»˜è®¤æ•°å­¦

    async def _create_mistakes_from_correction(
        self,
        user_id: str,
        correction_result: HomeworkCorrectionResult,
        subject: str,
        image_urls: List[str],
    ) -> Tuple[int, List[Dict[str, Any]]]:
        """
        ä»æ‰¹æ”¹ç»“æœåˆ›å»ºé”™é¢˜è®°å½•

        Args:
            user_id: ç”¨æˆ· ID
            correction_result: æ‰¹æ”¹ç»“æœ
            subject: å­¦ç§‘
            image_urls: ä½œä¸šå›¾ç‰‡ URLs

        Returns:
            Tuple[åˆ›å»ºçš„é”™é¢˜æ•°é‡, é”™é¢˜ä¿¡æ¯åˆ—è¡¨]
        """
        from src.models.study import MistakeRecord
        from src.repositories.mistake_repository import MistakeRepository
        from src.services.knowledge_graph_service import (
            normalize_subject,
        )  # ğŸ”§ å¯¼å…¥å­¦ç§‘è½¬æ¢å‡½æ•°

        mistake_repo = MistakeRepository(MistakeRecord, self.db)
        created_mistakes = []

        # ğŸ”§ å…³é”®ä¿®å¤ï¼šæ ‡å‡†åŒ–å­¦ç§‘åç§°ï¼ˆè‹±æ–‡â†’ä¸­æ–‡ï¼‰
        normalized_subject = normalize_subject(subject) if subject else "å…¶ä»–"
        logger.info(
            f"ğŸ“ [é”™é¢˜åˆ›å»º] å­¦ç§‘æ ‡å‡†åŒ–: {subject} â†’ {normalized_subject}"
        )
        logger.info(
            f"ğŸ“ [é”™é¢˜åˆ›å»º] å¼€å§‹å¤„ç†æ‰¹æ”¹ç»“æœ: "
            f"total_corrections={len(correction_result.corrections)}, "
            f"error_count={correction_result.error_count}, "
            f"unanswered_count={correction_result.unanswered_count}"
        )

        try:
            for idx, item in enumerate(correction_result.corrections, 1):
                # åªä¸ºé”™è¯¯æˆ–æœªä½œç­”çš„é¢˜ç›®åˆ›å»ºé”™é¢˜
                if not item.is_unanswered and not item.error_type:
                    logger.debug(
                        f"  [{idx}/{len(correction_result.corrections)}] "
                        f"è·³è¿‡æ­£ç¡®é¢˜ç›®: Q{item.question_number}"
                    )
                    continue

                logger.info(
                    f"  [{idx}/{len(correction_result.corrections)}] "
                    f"ğŸ”´ å¤„ç†é”™é¢˜: Q{item.question_number}, "
                    f"type={item.question_type}, "
                    f"is_unanswered={item.is_unanswered}, "
                    f"error_type={item.error_type or 'N/A'}"
                )

                # ç”Ÿæˆæ ‡é¢˜
                title = f"ç¬¬{item.question_number}é¢˜"
                if item.error_type:
                    title += f" - {item.error_type}"
                if len(title) > 200:
                    title = title[:200]

                # æ„å»ºé”™é¢˜æ•°æ®
                # ğŸ¯ é¢˜ç›®å†…å®¹é™çº§ç­–ç•¥ï¼šä¼˜å…ˆä½¿ç”¨question_textï¼Œå¦åˆ™ç”¨explanationå‰éƒ¨åˆ†
                question_content: str = getattr(item, "question_text", "") or ""
                if not question_content or not question_content.strip():
                    # é™çº§ï¼šä½¿ç”¨æ‰¹æ”¹è¯´æ˜çš„å‰80å­—
                    explanation_preview = (
                        item.explanation[:80] if item.explanation else "æ— "
                    )
                    question_content = (
                        f"é¢˜ç›®å†…å®¹è¯¦è§å›¾ç‰‡ï¼ˆæ‰¹æ”¹æç¤ºï¼š{explanation_preview}...ï¼‰"
                    )
                    logger.warning(
                        f"âš ï¸ Q{item.question_number} ç¼ºå°‘question_textï¼Œä½¿ç”¨é™çº§æ–¹æ¡ˆ"
                    )

                # ğŸ”§ ä½¿ç”¨å·²æ ‡å‡†åŒ–çš„å­¦ç§‘åï¼ˆä¸­æ–‡ï¼‰
                # å·²åœ¨å‰é¢é€šè¿‡ normalize_subject(subject) è½¬æ¢ä¸ºä¸­æ–‡
                final_subject = normalized_subject

                # å¦‚æœçŸ¥è¯†ç‚¹æ¨æ–­ç»™å‡ºæ–°çš„å­¦ç§‘ï¼Œä¹Ÿéœ€è¦æ ‡å‡†åŒ–
                if item.knowledge_points and len(item.knowledge_points) > 0:
                    inferred_en = self._infer_subject_from_knowledge_points(
                        item.knowledge_points
                    )
                    if inferred_en and inferred_en != subject:
                        # ğŸ”§ æ¨æ–­ç»“æœä¹Ÿéœ€è¦æ ‡å‡†åŒ–
                        inferred_cn = normalize_subject(inferred_en)
                        logger.info(
                            f"ğŸ”„ å­¦ç§‘ä¿®æ­£: {subject}({normalized_subject}) â†’ {inferred_en}({inferred_cn}) (åŸºäºçŸ¥è¯†ç‚¹)"
                        )
                        final_subject = inferred_cn

                mistake_data = {
                    "user_id": user_id,
                    "subject": final_subject,  # ğŸ”§ ä½¿ç”¨æ ‡å‡†åŒ–çš„ä¸­æ–‡å­¦ç§‘å
                    "title": title,
                    "ocr_text": question_content,  # ğŸ¯ ä½¿ç”¨é™çº§åçš„é¢˜ç›®å†…å®¹
                    "question_number": item.question_number,  # æ–°å¢å­—æ®µ
                    "is_unanswered": item.is_unanswered,  # æ–°å¢å­—æ®µ
                    "question_type": item.question_type,  # æ–°å¢å­—æ®µ
                    "error_type": item.error_type,  # æ–°å¢å­—æ®µ
                    "student_answer": item.student_answer,
                    "correct_answer": item.correct_answer,
                    "image_urls": image_urls,
                    "ai_feedback": {
                        "explanation": item.explanation,
                        "score": item.score,
                        "question_text": question_content,  # ğŸ¯ åŒæ­¥åˆ°ai_feedback
                    },
                    "knowledge_points": item.knowledge_points or [],
                    "difficulty_level": 2,  # é»˜è®¤ä¸­ç­‰éš¾åº¦
                    "mastery_status": "learning",
                    "source": "homework_correction",
                    "notes": f"è‡ªåŠ¨æ‰¹æ”¹ï¼š{item.explanation}",
                }

                # åˆ›å»ºé”™é¢˜è®°å½•
                mistake = await mistake_repo.create(mistake_data)
                mistake_id = str(mistake.id)
                logger.info(
                    f"    âœ… é”™é¢˜è®°å½•å·²åˆ›å»º: mistake_id={mistake_id}, "
                    f"knowledge_points={len(item.knowledge_points or [])}"
                )

                # ğŸ¯ åŒæ­¥åˆ›å»º/æ›´æ–°çŸ¥è¯†ç‚¹æŒæ¡åº¦è®°å½•
                if item.knowledge_points:
                    await self._update_knowledge_mastery(
                        user_id=user_id,
                        subject=final_subject,  # ğŸ”§ ä½¿ç”¨æ ‡å‡†åŒ–çš„å­¦ç§‘å
                        knowledge_points=item.knowledge_points,
                    )

                created_mistakes.append(
                    {
                        "id": mistake_id,
                        "question_number": item.question_number,
                        "error_type": item.error_type,
                        "title": title,
                    }
                )

            logger.info(
                f"ğŸ¯ [é”™é¢˜åˆ›å»º] å®Œæˆ: "
                f"created={len(created_mistakes)}, "
                f"total={len(correction_result.corrections)}, "
                f"success_rate={len(created_mistakes) / len(correction_result.corrections) * 100:.1f}%"
            )

            # ğŸ¯ å¼ºåˆ¶æäº¤äº‹åŠ¡ï¼Œç¡®ä¿é”™é¢˜å’ŒçŸ¥è¯†ç‚¹æ•°æ®æŒä¹…åŒ–
            try:
                await self.db.commit()
                logger.info("âœ… [äº‹åŠ¡æäº¤] é”™é¢˜å’ŒçŸ¥è¯†ç‚¹æ•°æ®å·²æŒä¹…åŒ–åˆ°æ•°æ®åº“")
            except Exception as commit_err:
                logger.error(f"âš ï¸ [äº‹åŠ¡æäº¤å¤±è´¥] {commit_err}", exc_info=True)
                await self.db.rollback()
                raise

            return len(created_mistakes), created_mistakes

        except Exception as e:
            logger.error(
                f"âŒ [é”™é¢˜åˆ›å»º] å¤±è´¥: {type(e).__name__}: {str(e)}", exc_info=True
            )
            return 0, []


# ä¾èµ–æ³¨å…¥å‡½æ•°
def get_learning_service(db: AsyncSession) -> LearningService:
    """è·å–å­¦ä¹ é—®ç­”æœåŠ¡å®ä¾‹"""
    return LearningService(db)
