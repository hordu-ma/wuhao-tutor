"""
å­¦ä¹ é—®ç­”æœåŠ¡
åŸºäºç™¾ç‚¼AIçš„æ™ºèƒ½å­¦ä¹ åŠ©æ‰‹æœåŠ¡
"""

import asyncio
import json
import logging
import time
from datetime import datetime, timedelta
from typing import Any, Dict, List, Optional, Tuple
from uuid import UUID

from sqlalchemy import and_, desc, func, join, or_, select
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy.orm import selectinload

from src.core.config import get_settings
from src.core.exceptions import (
    BailianServiceError,
    NotFoundError,
    ServiceError,
    ValidationError,
)
from src.models.homework import HomeworkReview, HomeworkSubmission
from src.models.learning import (
    Answer,
    ChatSession,
    LearningAnalytics,
    Question,
    QuestionStatus,
    QuestionType,
    SessionStatus,
)
from src.models.user import User
from src.repositories.base_repository import BaseRepository
from src.schemas.learning import (
    AnswerResponse,
    AskQuestionRequest,
    AskQuestionResponse,
    CreateSessionRequest,
    FeedbackRequest,
    LearningAnalyticsResponse,
    PaginatedResponse,
    QuestionHistoryQuery,
    QuestionResponse,
    SessionListQuery,
    SessionResponse,
)
from src.services.bailian_service import (
    AIContext,
    BailianService,
    ChatMessage,
    MessageRole,
    get_bailian_service,
)
from src.utils.cache import cache_key, cache_result
from src.utils.type_converters import (
    extract_orm_bool,
    extract_orm_int,
    extract_orm_str,
    extract_orm_uuid_str,
    safe_str,
    wrap_orm,
)

logger = logging.getLogger("learning_service")
settings = get_settings()


class LearningService:
    """å­¦ä¹ é—®ç­”æœåŠ¡"""

    def __init__(self, db: AsyncSession):
        self.db = db
        self.bailian_service = get_bailian_service()

        # åˆå§‹åŒ–ä»“å‚¨
        self.session_repo = BaseRepository(ChatSession, db)
        self.question_repo = BaseRepository(Question, db)
        self.answer_repo = BaseRepository(Answer, db)
        self.analytics_repo = BaseRepository(LearningAnalytics, db)

    # ========== é—®ç­”æ ¸å¿ƒåŠŸèƒ½ ==========

    async def ask_question(
        self, user_id: str, request: AskQuestionRequest
    ) -> AskQuestionResponse:
        """
        æé—®åŠŸèƒ½

        Args:
            user_id: ç”¨æˆ·ID
            request: æé—®è¯·æ±‚

        Returns:
            AskQuestionResponse: é—®ç­”å“åº”
        """
        start_time = time.time()

        try:
            # 1. è·å–æˆ–åˆ›å»ºä¼šè¯
            session = await self._get_or_create_session(user_id, request)

            # 2. ä¿å­˜é—®é¢˜
            question = await self._save_question(
                user_id, extract_orm_uuid_str(session, "id"), request
            )

            # 3. æ„å»ºAIä¸Šä¸‹æ–‡
            ai_context = await self._build_ai_context(
                user_id, session, request.use_context
            )

            # 4. æ„å»ºå¯¹è¯æ¶ˆæ¯
            messages = await self._build_conversation_messages(
                extract_orm_uuid_str(session, "id"),
                request,
                ai_context,
                request.include_history,
                request.max_history,
            )

            # 5. è°ƒç”¨AIç”Ÿæˆç­”æ¡ˆ
            # Convert ChatMessage objects to dict format if needed
            message_dicts = []
            for msg in messages:
                if hasattr(msg, "role") and hasattr(msg, "content"):
                    msg_dict: Dict[str, Any] = {
                        "role": msg.role.value,
                        "content": msg.content,
                    }
                    # å¦‚æœæœ‰å›¾ç‰‡URLsï¼Œæ·»åŠ åˆ°å­—å…¸ä¸­
                    if hasattr(msg, "image_urls") and msg.image_urls:
                        msg_dict["image_urls"] = msg.image_urls
                        logger.info(
                            f"ğŸ–¼ï¸ æ¶ˆæ¯åŒ…å«å›¾ç‰‡: role={msg.role.value}, "
                            f"image_count={len(msg.image_urls)}, "
                            f"urls={msg.image_urls}"
                        )
                    message_dicts.append(msg_dict)
                else:
                    message_dicts.append(msg)

            # ğŸ” æœ€ç»ˆè°ƒè¯•ï¼šæ‰“å°å®Œæ•´çš„message_dicts
            messages_summary = [
                {
                    "role": m.get("role"),
                    "has_images": bool(m.get("image_urls")),
                    "image_count": len(m.get("image_urls", [])),
                }
                for m in message_dicts
            ]
            logger.info(
                f"ğŸ“¤ å‡†å¤‡è°ƒç”¨AI: message_count={len(message_dicts)}, "
                f"messages_with_images={sum(1 for m in messages_summary if m['has_images'])}, "
                f"total_images={sum(m['image_count'] for m in messages_summary)}"
            )
            logger.info(
                f"ğŸ“‹ æ¶ˆæ¯è¯¦æƒ…: {json.dumps(messages_summary, ensure_ascii=False)}"
            )

            ai_response = await self.bailian_service.chat_completion(
                messages=message_dicts,
                context=ai_context,
                max_tokens=settings.AI_MAX_TOKENS,
                temperature=settings.AI_TEMPERATURE,
                top_p=settings.AI_TOP_P,
            )

            if not ai_response.success:
                raise BailianServiceError(f"AIè°ƒç”¨å¤±è´¥: {ai_response.error_message}")

            # 6. ä¿å­˜ç­”æ¡ˆ
            answer = await self._save_answer(
                extract_orm_uuid_str(question, "id"), ai_response
            )

            # 7. æ›´æ–°ä¼šè¯ç»Ÿè®¡
            await self._update_session_stats(
                extract_orm_uuid_str(session, "id"), ai_response.tokens_used
            )

            # 8. æ›´æ–°ç”¨æˆ·å­¦ä¹ åˆ†æ
            await self._update_learning_analytics(user_id, question, answer)

            # ğŸ¯ 9. æ™ºèƒ½é”™é¢˜è‡ªåŠ¨åˆ›å»ºï¼ˆç®€åŒ–è§„åˆ™ç‰ˆï¼‰
            mistake_created = False
            mistake_info = None
            try:
                mistake_result = await self._auto_create_mistake_if_needed(
                    user_id, question, answer, request
                )
                if mistake_result:
                    mistake_created = True
                    mistake_info = mistake_result
                    logger.info(
                        f"âœ… é”™é¢˜è‡ªåŠ¨åˆ›å»ºæˆåŠŸ: user_id={user_id}, "
                        f"mistake_id={mistake_info.get('id')}, "
                        f"category={mistake_info.get('category')}"
                    )
            except Exception as mistake_err:
                logger.warning(f"é”™é¢˜åˆ›å»ºå¤±è´¥ï¼Œä½†ä¸å½±å“é—®ç­”: {str(mistake_err)}")

            # 10. æ„å»ºå“åº”
            processing_time = int((time.time() - start_time) * 1000)

            # ğŸ”§ åˆ·æ–°ORMå¯¹è±¡ï¼Œç¡®ä¿æ‰€æœ‰å±æ€§å·²åŠ è½½ï¼ˆé¿å… MissingGreenlet é”™è¯¯ï¼‰
            await self.db.refresh(question)
            await self.db.refresh(answer)
            await self.db.refresh(session)

            return AskQuestionResponse(
                question=QuestionResponse.model_validate(question),
                answer=AnswerResponse.model_validate(answer),
                session=SessionResponse.model_validate(session),
                processing_time=processing_time,
                tokens_used=ai_response.tokens_used,
                mistake_created=mistake_created,  # ğŸ¯ æ–°å¢
                mistake_info=mistake_info,  # ğŸ¯ æ–°å¢
            )

        except Exception as e:
            logger.error(
                f"æé—®å¤„ç†å¤±è´¥: {str(e)}", extra={"user_id": user_id}, exc_info=True
            )

            # æ›´æ–°é—®é¢˜çŠ¶æ€ä¸ºå¤±è´¥
            try:
                # å®‰å…¨åœ°è·å–questionå˜é‡
                question_var = locals().get("question")
                if question_var is not None:
                    await self.question_repo.update(
                        extract_orm_uuid_str(question_var, "id"),
                        {"is_processed": False},
                    )
            except:
                pass  # Ignore update errors during exception handling

            raise ServiceError(f"æé—®å¤„ç†å¤±è´¥: {str(e)}") from e

    async def ask_question_stream(self, user_id: str, request: AskQuestionRequest):
        """
        æµå¼æé—®åŠŸèƒ½

        Args:
            user_id: ç”¨æˆ·ID
            request: æé—®è¯·æ±‚

        Yields:
            dict: SSE æ ¼å¼çš„å¢é‡å“åº”
                - type: "content" | "done" | "error"
                - content: å¢é‡æ–‡æœ¬å†…å®¹
                - full_content: ç´¯ç§¯çš„å®Œæ•´å†…å®¹
                - finish_reason: å®ŒæˆåŸå›  (null | "stop")
                - question_id: é—®é¢˜ID (ä»…åœ¨ type="done" æ—¶)
                - answer_id: ç­”æ¡ˆID (ä»…åœ¨ type="done" æ—¶)
                - usage: tokenä½¿ç”¨æƒ…å†µ (ä»…åœ¨ type="done" æ—¶)
        """
        question = None
        session = None
        full_answer_content = ""

        try:
            # 1. è·å–æˆ–åˆ›å»ºä¼šè¯
            session = await self._get_or_create_session(user_id, request)
            session_id = extract_orm_uuid_str(session, "id")

            # 2. ä¿å­˜é—®é¢˜ï¼ˆçŠ¶æ€ä¸ºæœªå¤„ç†ï¼‰
            question = await self._save_question(user_id, session_id, request)
            question_id = extract_orm_uuid_str(question, "id")

            # 3. æ„å»ºAIä¸Šä¸‹æ–‡
            ai_context = await self._build_ai_context(
                user_id, session, request.use_context
            )

            # 4. æ„å»ºå¯¹è¯æ¶ˆæ¯
            messages = await self._build_conversation_messages(
                session_id,
                request,
                ai_context,
                request.include_history,
                request.max_history,
            )

            # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼
            message_dicts = []
            for msg in messages:
                if hasattr(msg, "role") and hasattr(msg, "content"):
                    msg_dict: Dict[str, Any] = {
                        "role": msg.role.value,
                        "content": msg.content,
                    }
                    if hasattr(msg, "image_urls") and msg.image_urls:
                        msg_dict["image_urls"] = msg.image_urls
                    message_dicts.append(msg_dict)
                else:
                    message_dicts.append(msg)

            # 4. æµå¼è°ƒç”¨AIï¼ˆæ”¯æŒå›¾ç‰‡å’Œæ–‡æœ¬ï¼‰
            logger.info(
                f"å¼€å§‹æµå¼è°ƒç”¨ - æ¶ˆæ¯æ•°: {len(message_dicts)}, "
                f"å½“å‰è¯·æ±‚å›¾ç‰‡: {len(request.image_urls or [])}"
            )

            async for chunk in self.bailian_service.chat_completion_stream(
                messages=message_dicts,
                context=ai_context,
                max_tokens=settings.AI_MAX_TOKENS,
                temperature=settings.AI_TEMPERATURE,
                top_p=settings.AI_TOP_P,
            ):
                # ğŸ”§ é˜²å¾¡æ€§æ£€æŸ¥ï¼šç¡®ä¿ chunk ä¸ä¸º None
                if chunk is None:
                    logger.warning("æ”¶åˆ° None chunkï¼Œè·³è¿‡å¤„ç†")
                    continue

                # ç´¯ç§¯å®Œæ•´å†…å®¹
                if chunk.get("content"):
                    full_answer_content = chunk.get("full_content", "")

                # å‘é€å¢é‡å†…å®¹
                yield {
                    "type": "content",
                    "content": chunk.get("content", ""),
                    "full_content": full_answer_content,
                    "finish_reason": chunk.get("finish_reason"),
                }

                # æµå¼å®Œæˆåä¿å­˜æ•°æ®
                if chunk.get("finish_reason") == "stop":
                    # 6. ä¿å­˜ç­”æ¡ˆ
                    answer_data = {
                        "question_id": question_id,
                        "content": full_answer_content,
                        "tokens_used": chunk.get("usage", {}).get("total_tokens", 0),
                        "model_name": chunk.get(
                            "model", "qwen-turbo"
                        ),  # ä½¿ç”¨å®é™…è°ƒç”¨çš„æ¨¡å‹
                    }
                    answer = await self.answer_repo.create(answer_data)
                    answer_id = extract_orm_uuid_str(answer, "id")

                    # 7. æ›´æ–°é—®é¢˜çŠ¶æ€
                    await self.question_repo.update(
                        question_id,
                        {"is_processed": True},
                    )

                    # 8. æ›´æ–°ä¼šè¯ç»Ÿè®¡
                    tokens_used = chunk.get("usage", {}).get("total_tokens", 0)
                    await self._update_session_stats(session_id, tokens_used)

                    # 9. æ›´æ–°å­¦ä¹ åˆ†æï¼ˆåå°ä»»åŠ¡ï¼Œä¸é˜»å¡å“åº”ï¼‰
                    try:
                        await self._update_learning_analytics(user_id, question, answer)
                    except Exception as e:
                        logger.warning(f"æ›´æ–°å­¦ä¹ åˆ†æå¤±è´¥: {e}")

                    # ğŸ¯ 9.5 æ™ºèƒ½é”™é¢˜è‡ªåŠ¨åˆ›å»ºï¼ˆä¸é˜»å¡æµå¼å“åº”ï¼‰
                    mistake_created = False
                    mistake_info = None
                    try:
                        mistake_result = await self._auto_create_mistake_if_needed(
                            user_id, question, answer, request
                        )
                        if mistake_result:
                            mistake_created = True
                            mistake_info = mistake_result
                            logger.info(
                                f"âœ… [æµå¼] é”™é¢˜è‡ªåŠ¨åˆ›å»ºæˆåŠŸ: user_id={user_id}, "
                                f"mistake_id={mistake_info.get('id')}, "
                                f"category={mistake_info.get('category')}, "
                                f"confidence={mistake_info.get('confidence')}"
                            )
                    except Exception as mistake_err:
                        logger.warning(
                            f"[æµå¼] é”™é¢˜åˆ›å»ºå¤±è´¥ï¼Œä½†ä¸å½±å“é—®ç­”: {str(mistake_err)}"
                        )

                    # 10. å‘é€å®Œæˆäº‹ä»¶
                    yield {
                        "type": "done",
                        "question_id": question_id,
                        "answer_id": answer_id,
                        "session_id": session_id,
                        "usage": chunk.get("usage", {}),
                        "full_content": full_answer_content,
                        "mistake_created": mistake_created,  # ğŸ¯ æ–°å¢
                        "mistake_info": mistake_info,  # ğŸ¯ æ–°å¢
                    }

        except BailianServiceError as e:
            logger.error(f"AIæœåŠ¡è°ƒç”¨å¤±è´¥: {e}")
            yield {"type": "error", "message": f"AIæœåŠ¡æš‚æ—¶ä¸å¯ç”¨: {str(e)}"}
        except Exception as e:
            logger.error(f"æµå¼æé—®å¤„ç†å¤±è´¥: {e}", exc_info=True)

            # æ›´æ–°é—®é¢˜çŠ¶æ€ä¸ºå¤±è´¥
            if question:
                try:
                    await self.question_repo.update(
                        extract_orm_uuid_str(question, "id"),
                        {"is_processed": False},
                    )
                except:
                    pass

            yield {"type": "error", "message": f"æé—®å¤„ç†å¤±è´¥: {str(e)}"}

    async def _get_or_create_session(
        self, user_id: str, request: AskQuestionRequest
    ) -> ChatSession:
        """è·å–æˆ–åˆ›å»ºä¼šè¯"""
        if request.session_id:
            # å°è¯•è·å–ç°æœ‰ä¼šè¯
            try:
                session = await self.session_repo.get_by_id(request.session_id)
                if session and extract_orm_uuid_str(session, "user_id") == user_id:
                    # ä¼šè¯å­˜åœ¨ä¸”å±äºå½“å‰ç”¨æˆ·
                    if (
                        extract_orm_bool(session, "status")
                        == SessionStatus.ACTIVE.value
                    ):
                        # æ›´æ–°æœ€åæ´»è·ƒæ—¶é—´
                        await self.session_repo.update(
                            extract_orm_uuid_str(session, "id"),
                            {"last_active_at": datetime.now().isoformat()},
                        )
                    return session
            except Exception as e:
                # è·å–ä¼šè¯å¤±è´¥ï¼Œè®°å½•æ—¥å¿—ä½†ä¸ä¸­æ–­ï¼Œç»§ç»­åˆ›å»ºæ–°ä¼šè¯
                print(
                    f"è·å–ä¼šè¯å¤±è´¥ï¼Œå°†åˆ›å»ºæ–°ä¼šè¯: session_id={request.session_id}, error={str(e)}"
                )

        # åˆ›å»ºæ–°ä¼šè¯ï¼ˆåŸæ¥çš„session_idä¸å­˜åœ¨æˆ–è·å–å¤±è´¥ï¼‰
        session_title = await self._generate_session_title(request.content)
        session_data = {
            "user_id": user_id,
            "title": session_title,
            "subject": request.subject.value if request.subject else None,
            "status": SessionStatus.ACTIVE.value,
            "context_enabled": request.use_context,
            "last_active_at": datetime.utcnow().isoformat(),
        }
        return await self.session_repo.create(session_data)

    async def _generate_session_title(self, first_question: str) -> str:
        """ç”Ÿæˆä¼šè¯æ ‡é¢˜"""
        # ç®€å•çš„æ ‡é¢˜ç”Ÿæˆé€»è¾‘ï¼Œå–é—®é¢˜å‰30ä¸ªå­—ç¬¦
        title = first_question[:30]
        if len(first_question) > 30:
            title += "..."
        return title

    async def _save_question(
        self, user_id: str, session_id: str, request: AskQuestionRequest
    ) -> Question:
        """ä¿å­˜é—®é¢˜"""
        question_data = {
            "session_id": session_id,
            "user_id": user_id,
            "content": request.content,
            "question_type": (
                request.question_type.value if request.question_type else None
            ),
            "subject": request.subject.value if request.subject else None,
            "topic": request.topic,
            "difficulty_level": (
                request.difficulty_level.value if request.difficulty_level else None
            ),
            "context_data": (
                json.dumps(request.context_data) if request.context_data else None
            ),
            "has_images": bool(request.image_urls),
            "image_urls": (
                json.dumps(request.image_urls) if request.image_urls else None
            ),
            "is_processed": False,
        }
        return await self.question_repo.create(question_data)

    async def _build_ai_context(
        self, user_id: str, session: ChatSession, use_context: bool = True
    ) -> AIContext:
        """æ„å»ºAIè°ƒç”¨ä¸Šä¸‹æ–‡ï¼Œé›†æˆMCPä¸ªæ€§åŒ–å­¦æƒ…åˆ†æ"""
        context = AIContext(
            user_id=user_id,
            subject=extract_orm_str(session, "subject"),
            session_id=extract_orm_uuid_str(session, "id"),
        )

        if use_context:
            # è·å–ç”¨æˆ·ä¿¡æ¯
            user_stmt = select(User).where(User.id == user_id)
            user_result = await self.db.execute(user_stmt)
            user = user_result.scalar_one_or_none()

            if user:
                context.grade_level = self._parse_grade_level(
                    extract_orm_str(user, "grade_level")
                )
                context.metadata = {
                    "user_school": extract_orm_str(user, "school"),
                    "user_class": extract_orm_str(user, "class_name"),
                    "learning_subjects": extract_orm_str(user, "study_subjects"),
                }

            # ğŸš€ NEW: é›†æˆ MCP ä¸ªæ€§åŒ–å­¦æƒ…ä¸Šä¸‹æ–‡
            try:
                from src.services.knowledge_context_builder import (
                    knowledge_context_builder,
                )

                # æ„å»ºç”¨æˆ·å­¦æƒ…ä¸Šä¸‹æ–‡
                learning_context = await knowledge_context_builder.build_context(
                    user_id=user_id,
                    subject=extract_orm_str(session, "subject"),
                    session_type="learning",
                )

                # å°†å­¦æƒ…åˆ†æç»“æœæ·»åŠ åˆ°AIä¸Šä¸‹æ–‡ä¸­
                if learning_context.weak_knowledge_points:
                    weak_points_summary = []
                    for point in learning_context.weak_knowledge_points[
                        :5
                    ]:  # å–å‰5ä¸ªæœ€ä¸¥é‡çš„
                        weak_points_summary.append(
                            {
                                "knowledge": point.knowledge_name,
                                "subject": point.subject,
                                "error_rate": round(point.error_rate * 100, 1),
                                "severity": round(point.severity_score * 100, 1),
                            }
                        )

                    context.metadata = context.metadata or {}
                    context.metadata.update(
                        {
                            "weak_knowledge_points": weak_points_summary,
                            "learning_pace": learning_context.learning_preferences.learning_pace,
                            "focus_duration": learning_context.learning_preferences.focus_duration,
                            "current_level": learning_context.context_summary.current_level,
                            "total_questions": learning_context.context_summary.total_questions,
                            "learning_streak": learning_context.context_summary.learning_streak,
                            "mcp_context_generated": True,
                        }
                    )

                    logger.info(
                        f"MCPä¸Šä¸‹æ–‡å·²æ„å»º - ç”¨æˆ·: {user_id}, è–„å¼±çŸ¥è¯†ç‚¹: {len(learning_context.weak_knowledge_points)}"
                    )
                else:
                    # æ–°ç”¨æˆ·æˆ–æ²¡æœ‰è¶³å¤Ÿæ•°æ®ï¼Œæ ‡è®°ä¸ºé¦–æ¬¡å­¦ä¹ 
                    context.metadata = context.metadata or {}
                    context.metadata.update(
                        {
                            "mcp_context_generated": True,
                            "is_new_learner": True,
                            "current_level": "beginner",
                        }
                    )
                    logger.info(f"MCPä¸Šä¸‹æ–‡å·²æ„å»º - æ–°å­¦ä¹ è€…: {user_id}")

            except Exception as e:
                logger.warning(f"MCPä¸Šä¸‹æ–‡æ„å»ºå¤±è´¥ï¼Œå›é€€åˆ°ä¼ ç»Ÿæ¨¡å¼: {str(e)}")
                # ç»§ç»­ä½¿ç”¨ä¼ ç»Ÿä¸Šä¸‹æ–‡ï¼Œä¸å½±å“ä¸»æµç¨‹
                context.metadata = context.metadata or {}
                context.metadata["mcp_context_failed"] = True

            # è·å–ç›¸å…³ä½œä¸šå†å²ï¼ˆä¿æŒåŸæœ‰é€»è¾‘ï¼‰
            homework_context = await self._get_homework_context(
                user_id, extract_orm_str(session, "subject")
            )
            if homework_context:
                context.metadata = context.metadata or {}
                context.metadata.update(homework_context)

        return context

    def _parse_grade_level(self, grade_level: Optional[str]) -> Optional[int]:
        """è§£æå­¦æ®µä¸ºæ•°å­—"""
        if not grade_level:
            return None

        grade_mapping = {
            "junior_1": 7,
            "junior_2": 8,
            "junior_3": 9,
            "senior_1": 10,
            "senior_2": 11,
            "senior_3": 12,
        }
        return grade_mapping.get(grade_level)

    async def _get_homework_context(
        self, user_id: str, subject: Optional[str]
    ) -> Optional[Dict[str, Any]]:
        """è·å–ä½œä¸šç›¸å…³ä¸Šä¸‹æ–‡"""
        try:
            # è·å–æœ€è¿‘çš„ä½œä¸šè®°å½•
            stmt = (
                select(HomeworkSubmission)
                .options(selectinload(HomeworkSubmission.reviews))
                .where(HomeworkSubmission.student_id == user_id)
                .order_by(desc(HomeworkSubmission.created_at))
                .limit(5)
            )

            if subject:
                stmt = stmt.where(HomeworkSubmission.subject == subject)

            result = await self.db.execute(stmt)
            submissions = result.scalars().all()

            if not submissions:
                return None

            # åˆ†æé”™é¢˜å’ŒçŸ¥è¯†ç‚¹
            wrong_topics = []
            mastered_topics = []

            for submission in submissions:
                for review in submission.reviews:
                    if hasattr(review, "knowledge_points") and review.knowledge_points:
                        points = json.loads(review.knowledge_points)
                        if review.score and review.score < 80:  # å‡è®¾80åˆ†ä»¥ä¸‹ä¸ºé”™é¢˜
                            wrong_topics.extend(points)
                        else:
                            mastered_topics.extend(points)

            return {
                "recent_homework_count": len(submissions),
                "weak_knowledge_points": list(set(wrong_topics))[:10],
                "strong_knowledge_points": list(set(mastered_topics))[:10],
            }

        except Exception as e:
            logger.warning(f"è·å–ä½œä¸šä¸Šä¸‹æ–‡å¤±è´¥: {str(e)}")
            return None

    async def _build_conversation_messages(
        self,
        session_id: str,
        request: AskQuestionRequest,
        context: AIContext,
        include_history: bool = True,
        max_history: int = 10,
    ) -> List[ChatMessage]:
        """æ„å»ºå¯¹è¯æ¶ˆæ¯"""
        messages = []

        # 1. ç³»ç»Ÿæç¤ºè¯
        system_prompt = await self._build_system_prompt(context)
        messages.append(ChatMessage(role=MessageRole.SYSTEM, content=system_prompt))

        # 2. å†å²å¯¹è¯
        if include_history and max_history > 0:
            history_messages = await self._get_conversation_history(
                session_id, max_history
            )
            messages.extend(history_messages)

        # 3. å½“å‰é—®é¢˜
        user_message = request.content

        # æ„å»ºç”¨æˆ·æ¶ˆæ¯ï¼Œæ”¯æŒå›¾ç‰‡
        if request.image_urls and len(request.image_urls) > 0:
            # æœ‰å›¾ç‰‡æ—¶ï¼Œåˆ›å»ºåŒ…å«å›¾ç‰‡çš„å¤šæ¨¡æ€æ¶ˆæ¯
            user_chat_message = ChatMessage(
                role=MessageRole.USER,
                content=user_message,
                image_urls=request.image_urls,
            )

            # æ·»åŠ å›¾ç‰‡æç¤ºåˆ°æ–‡æœ¬å†…å®¹
            user_message += f"\n\n[ç”¨æˆ·ä¸Šä¼ äº†{len(request.image_urls)}å¼ å›¾ç‰‡ï¼Œè¯·åˆ†æå›¾ç‰‡å†…å®¹å¹¶å›ç­”é—®é¢˜]"
            user_chat_message.content = user_message

            # ğŸ” è°ƒè¯•æ—¥å¿—ï¼šè®°å½•å›¾ç‰‡URL
            logger.info(
                f"æ„å»ºå¤šæ¨¡æ€æ¶ˆæ¯: session_id={session_id}, image_count={len(request.image_urls)}",
                extra={
                    "session_id": session_id,
                    "image_urls": request.image_urls,
                    "message_preview": user_message[:100],
                },
            )

        else:
            # çº¯æ–‡æœ¬æ¶ˆæ¯
            user_chat_message = ChatMessage(role=MessageRole.USER, content=user_message)

        messages.append(user_chat_message)

        return messages

    async def _build_system_prompt(self, context: AIContext) -> str:
        """
        æ„å»ºç³»ç»Ÿæç¤ºè¯ï¼ˆç®€åŒ–ç‰ˆï¼‰

        æ›´å¤æ‚çš„æç¤ºè¯é…ç½®è¯·åœ¨ç™¾ç‚¼å¹³å°çš„æ™ºèƒ½ä½“"ç³»ç»ŸæŒ‡ä»¤"ä¸­è®¾ç½®
        """
        prompt_parts = [
            "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„K12å­¦ä¹ åŠ©æ•™ï¼Œåå«'äº”å¥½åŠ©æ•™'ï¼Œä¸“é—¨å¸®åŠ©å°åˆé«˜ä¸­å­¦ç”Ÿè§£å†³å­¦ä¹ é—®é¢˜ã€‚",
            "",
            "ä½ çš„èŒè´£åŒ…æ‹¬ï¼š",
            "1. åªèƒ½å›ç­”å­¦ä¹ é—®é¢˜ï¼Œæä¾›æ¸…æ™°æ˜“æ‡‚çš„è§£é‡Š",
            "2. åˆ†æé¢˜ç›®ï¼Œæä¾›è¯¦ç»†çš„è§£é¢˜æ­¥éª¤",
            "3. é¼“åŠ±å­¦ç”Ÿç§¯æå­¦ä¹ ï¼Œå»ºç«‹å­¦ä¹ ä¿¡å¿ƒ",
        ]

        # æ·»åŠ ç”¨æˆ·ä¸Šä¸‹æ–‡ï¼ˆä¿ç•™ä¸ªæ€§åŒ–åŠŸèƒ½ï¼‰
        if context.grade_level:
            grade_name = self._get_grade_name(context.grade_level)
            prompt_parts.append(f"\nå­¦ç”Ÿå½“å‰å­¦æ®µï¼š{grade_name}")

        if context.subject:
            subject_name = self._get_subject_name(context.subject)
            prompt_parts.append(f"å½“å‰å­¦ç§‘ï¼š{subject_name}")

        if context.metadata:
            if context.metadata.get("user_school"):
                prompt_parts.append(f"å­¦ç”Ÿå­¦æ ¡ï¼š{context.metadata['user_school']}")

            if context.metadata.get("weak_knowledge_points"):
                weak_points = context.metadata["weak_knowledge_points"][:3]  # å–å‰3ä¸ª
                # weak_points æ˜¯ WeakKnowledgePoint å¯¹è±¡æˆ–å­—å…¸åˆ—è¡¨,éœ€è¦æå– knowledge_name
                if weak_points:
                    point_names = []
                    for point in weak_points:
                        if isinstance(point, dict):
                            point_names.append(point.get("knowledge_name", str(point)))
                        elif hasattr(point, "knowledge_name"):
                            point_names.append(point.knowledge_name)
                        else:
                            point_names.append(str(point))
                    if point_names:
                        prompt_parts.append(f"å­¦ç”Ÿè–„å¼±çŸ¥è¯†ç‚¹ï¼š{', '.join(point_names)}")

        prompt_parts.append("\nè¯·åŸºäºä»¥ä¸Šä¿¡æ¯ï¼Œä¸ºå­¦ç”Ÿæä¾›ä¸ªæ€§åŒ–çš„å­¦ä¹ æŒ‡å¯¼ã€‚")

        return "\n".join(prompt_parts)

    def _get_grade_name(self, grade_level: int) -> str:
        """è·å–å­¦æ®µåç§°"""
        grade_mapping = {
            7: "åˆä¸€",
            8: "åˆäºŒ",
            9: "åˆä¸‰",
            10: "é«˜ä¸€",
            11: "é«˜äºŒ",
            12: "é«˜ä¸‰",
        }
        return grade_mapping.get(grade_level, f"å­¦æ®µ{grade_level}")

    def _get_subject_name(self, subject: str) -> str:
        """è·å–å­¦ç§‘åç§°"""
        subject_mapping = {
            "math": "æ•°å­¦",
            "chinese": "è¯­æ–‡",
            "english": "è‹±è¯­",
            "physics": "ç‰©ç†",
            "chemistry": "åŒ–å­¦",
            "biology": "ç”Ÿç‰©",
            "history": "å†å²",
            "geography": "åœ°ç†",
            "politics": "æ”¿æ²»",
        }
        return subject_mapping.get(subject, subject)

    async def _get_conversation_history(
        self, session_id: str, max_count: int
    ) -> List[ChatMessage]:
        """è·å–å¯¹è¯å†å²"""
        try:
            # è·å–æœ€è¿‘çš„é—®ç­”å¯¹
            stmt = (
                select(Question)
                .options(selectinload(Question.answer))
                .where(Question.session_id == session_id, Question.is_processed == True)
                .order_by(desc(Question.created_at))
                .limit(max_count)
            )

            result = await self.db.execute(stmt)
            questions = result.scalars().all()

            messages = []

            # æŒ‰æ—¶é—´æ­£åºæ’åˆ—ï¼ˆæ—§çš„åœ¨å‰ï¼‰
            for question in reversed(questions):
                messages.append(
                    ChatMessage(
                        role=MessageRole.USER,
                        content=extract_orm_str(question, "content"),
                    )
                )

                if question.answer:
                    messages.append(
                        ChatMessage(
                            role=MessageRole.ASSISTANT,
                            content=extract_orm_str(question.answer, "content"),
                        )
                    )

            return messages

        except Exception as e:
            logger.warning(f"è·å–å¯¹è¯å†å²å¤±è´¥: {str(e)}")
            return []

    async def _save_answer(self, question_id: str, ai_response) -> Answer:
        """ä¿å­˜AIç­”æ¡ˆ"""
        # åˆ†æç­”æ¡ˆç”Ÿæˆæ¨èå†…å®¹
        related_topics, suggested_questions = await self._analyze_answer_content(
            ai_response.content
        )

        answer_data = {
            "question_id": question_id,
            "content": ai_response.content,
            "model_name": ai_response.model,
            "tokens_used": ai_response.tokens_used,
            "generation_time": int(ai_response.processing_time * 1000),
            "confidence_score": 85,  # é»˜è®¤ç½®ä¿¡åº¦ï¼Œåç»­å¯é€šè¿‡åˆ†ææ”¹è¿›
            "related_topics": json.dumps(related_topics) if related_topics else None,
            "suggested_questions": (
                json.dumps(suggested_questions) if suggested_questions else None
            ),
        }

        answer = await self.answer_repo.create(answer_data)

        # æ›´æ–°é—®é¢˜çŠ¶æ€
        await self.question_repo.update(question_id, {"is_processed": True})

        return answer

    async def _analyze_answer_content(
        self, content: str
    ) -> Tuple[List[str], List[str]]:
        """åˆ†æç­”æ¡ˆå†…å®¹ï¼Œæå–ç›¸å…³è¯é¢˜å’Œæ¨èé—®é¢˜"""
        # è¿™é‡Œæ˜¯ç®€åŒ–çš„åˆ†æé€»è¾‘ï¼Œå®é™…å¯ä»¥ä½¿ç”¨NLPæŠ€æœ¯æ”¹è¿›
        related_topics = []
        suggested_questions = []

        # ç®€å•çš„å…³é”®è¯æå–ï¼ˆå¯ä»¥åç»­æ”¹è¿›ï¼‰
        if "äºŒæ¬¡å‡½æ•°" in content:
            related_topics.extend(["äºŒæ¬¡å‡½æ•°", "å‡½æ•°å›¾è±¡", "é…æ–¹æ³•"])
            suggested_questions.extend(
                ["å¦‚ä½•æ±‚äºŒæ¬¡å‡½æ•°çš„å¯¹ç§°è½´ï¼Ÿ", "äºŒæ¬¡å‡½æ•°çš„æœ€å€¼æ€ä¹ˆæ±‚ï¼Ÿ"]
            )
        elif "åŒ–å­¦æ–¹ç¨‹å¼" in content:
            related_topics.extend(["åŒ–å­¦æ–¹ç¨‹å¼", "åŒ–å­¦ååº”", "é…å¹³"])
            suggested_questions.extend(["å¦‚ä½•é…å¹³åŒ–å­¦æ–¹ç¨‹å¼ï¼Ÿ", "åŒ–å­¦ååº”ç±»å‹æœ‰å“ªäº›ï¼Ÿ"])

        return related_topics[:5], suggested_questions[:3]  # é™åˆ¶æ•°é‡

    async def _update_session_stats(self, session_id: str, tokens_used: int) -> None:
        """æ›´æ–°ä¼šè¯ç»Ÿè®¡"""
        session = await self.session_repo.get_by_id(session_id)
        if session:
            # æ›´æ–°ä¼šè¯ç»Ÿè®¡ä¿¡æ¯
            current_tokens = extract_orm_int(session, "total_tokens", 0) or 0
            current_question_count = extract_orm_int(session, "question_count", 0) or 0
            session_id_str = extract_orm_uuid_str(session, "id")

            await self.session_repo.update(
                session_id_str,
                {
                    "total_tokens": current_tokens + tokens_used,
                    "question_count": current_question_count + 1,  # å¢åŠ é—®é¢˜è®¡æ•°
                    "last_active_at": datetime.now().isoformat(),
                },
            )

    async def _update_learning_analytics(
        self, user_id: str, question: Question, answer: Answer
    ) -> None:
        """æ›´æ–°ç”¨æˆ·å­¦ä¹ åˆ†æ"""
        try:
            # è·å–æˆ–åˆ›å»ºå­¦ä¹ åˆ†æè®°å½•
            analytics = await self.analytics_repo.get_by_field("user_id", user_id)
            if not analytics:
                analytics_data = {
                    "user_id": user_id,
                    "total_questions": 1,
                    "total_sessions": 1,
                    "last_analyzed_at": datetime.utcnow().isoformat(),
                }
                await self.analytics_repo.create(analytics_data)
            else:
                # æ›´æ–°ç»Ÿè®¡
                await self.analytics_repo.update(
                    extract_orm_uuid_str(analytics, "id"),
                    {
                        "total_questions": (
                            extract_orm_int(analytics, "total_questions", 0) or 0
                        )
                        + 1,
                        "last_analyzed_at": datetime.utcnow().isoformat(),
                    },
                )

        except Exception as e:
            logger.warning(f"æ›´æ–°å­¦ä¹ åˆ†æå¤±è´¥: {str(e)}")

    # ========== ä¼šè¯ç®¡ç†åŠŸèƒ½ ==========

    async def create_session(
        self, user_id: str, request: CreateSessionRequest
    ) -> SessionResponse:
        """åˆ›å»ºæ–°ä¼šè¯"""
        session_data = {
            "user_id": user_id,
            "title": request.title,
            "subject": request.subject.value if request.subject else None,
            "grade_level": request.grade_level,
            "status": SessionStatus.ACTIVE.value,
            "context_enabled": request.context_enabled,
            "last_active_at": datetime.utcnow().isoformat(),
        }

        session = await self.session_repo.create(session_data)

        # å¦‚æœæœ‰åˆå§‹é—®é¢˜ï¼Œå¤„ç†ç¬¬ä¸€ä¸ªé—®é¢˜
        if request.initial_question:
            ask_request = AskQuestionRequest(
                content=request.initial_question,
                session_id=extract_orm_uuid_str(session, "id"),
                subject=request.subject,
                topic=None,
                difficulty_level=None,
            )
            await self.ask_question(user_id, ask_request)

        return SessionResponse.model_validate(session)

    async def get_session_list(
        self, user_id: str, query: SessionListQuery
    ) -> Dict[str, Any]:
        """è·å–ä¼šè¯åˆ—è¡¨"""
        # æ„å»ºæŸ¥è¯¢æ¡ä»¶
        conditions = [ChatSession.user_id == user_id]

        if query.status:
            conditions.append(ChatSession.status == safe_str(query.status))

        if query.subject:
            conditions.append(ChatSession.subject == safe_str(query.subject))

        if query.search:
            conditions.append(ChatSession.title.contains(query.search))

        # è®¡ç®—æ€»æ•°
        count_stmt = select(func.count(ChatSession.id)).where(and_(*conditions))
        total_result = await self.db.execute(count_stmt)
        total = total_result.scalar()

        # æŸ¥è¯¢æ•°æ®
        stmt = (
            select(ChatSession)
            .where(and_(*conditions))
            .order_by(desc(ChatSession.last_active_at))
            .offset((query.page - 1) * query.size)
            .limit(query.size)
        )

        result = await self.db.execute(stmt)
        sessions = result.scalars().all()

        return {
            "total": total,
            "page": query.page,
            "size": query.size,
            "pages": (
                (total + query.size - 1) // query.size if total and query.size else 0
            ),
            "items": [SessionResponse.model_validate(session) for session in sessions],
        }

    async def get_question_history(
        self, user_id: str, query: QuestionHistoryQuery
    ) -> Dict[str, Any]:
        """è·å–é—®é¢˜å†å²"""
        # æ„å»ºæŸ¥è¯¢æ¡ä»¶
        conditions = [Question.user_id == user_id]

        if query.session_id:
            conditions.append(Question.session_id == query.session_id)

        if query.subject:
            conditions.append(Question.subject == safe_str(query.subject))

        if query.question_type:
            conditions.append(Question.question_type == safe_str(query.question_type))

        if query.start_date:
            conditions.append(Question.created_at >= query.start_date.isoformat())

        if query.end_date:
            conditions.append(Question.created_at <= query.end_date.isoformat())

        # è®¡ç®—æ€»æ•°
        count_stmt = select(func.count(Question.id)).where(and_(*conditions))
        total_result = await self.db.execute(count_stmt)
        total = total_result.scalar()

        # æŸ¥è¯¢æ•°æ®
        stmt = (
            select(Question)
            .options(selectinload(Question.answer))
            .where(and_(*conditions))
            .order_by(desc(Question.created_at))
            .offset((query.page - 1) * query.size)
            .limit(query.size)
        )

        result = await self.db.execute(stmt)
        questions = result.scalars().all()

        # æ„å»ºé—®ç­”å¯¹
        items = []
        for question in questions:
            item = {
                "question": QuestionResponse.model_validate(question),
                "answer": (
                    AnswerResponse.model_validate(question.answer)
                    if question.answer
                    else None
                ),
            }
            items.append(item)

        return {
            "total": total,
            "page": query.page,
            "size": query.size,
            "pages": (
                (total + query.size - 1) // query.size if total and query.size else 0
            ),
            "items": items,
        }

    # ========== åé¦ˆå’Œè¯„ä»·åŠŸèƒ½ ==========

    async def submit_feedback(self, user_id: str, request: FeedbackRequest) -> bool:
        """æäº¤ç”¨æˆ·åé¦ˆ"""
        # éªŒè¯é—®é¢˜å½’å±
        question = await self.question_repo.get_by_id(request.question_id)
        if not question or extract_orm_uuid_str(question, "user_id") != user_id:
            raise NotFoundError("é—®é¢˜ä¸å­˜åœ¨æˆ–æ— æƒé™è®¿é—®")

        if not getattr(question, "answer", None):
            raise ValidationError("é—®é¢˜å°šæœªå›ç­”ï¼Œæ— æ³•æäº¤åé¦ˆ")

        # æ›´æ–°ç­”æ¡ˆåé¦ˆ
        answer_id = extract_orm_uuid_str(question.answer, "id")
        await self.answer_repo.update(
            answer_id,
            {
                "user_rating": request.rating,
                "user_feedback": request.feedback,
                "is_helpful": request.is_helpful,
            },
        )

        logger.info(
            "ç”¨æˆ·åé¦ˆå·²ä¿å­˜",
            extra={
                "user_id": user_id,
                "question_id": request.question_id,
                "rating": request.rating,
            },
        )

        return True

    # ========== å­¦ä¹ åˆ†æåŠŸèƒ½ ==========

    @cache_result(ttl=3600)  # ç¼“å­˜1å°æ—¶
    async def get_learning_analytics(
        self, user_id: str
    ) -> Optional[LearningAnalyticsResponse]:
        """è·å–å­¦ä¹ åˆ†æ"""
        analytics = await self.analytics_repo.get_by_field("user_id", user_id)
        if not analytics:
            return None

        # è·å–è¯¦ç»†ç»Ÿè®¡æ•°æ®
        subject_stats = await self._calculate_subject_stats(user_id)
        learning_pattern = await self._analyze_learning_pattern(user_id)

        # è®¡ç®—å¹³å‡è¯„åˆ†
        avg_rating_stmt = (
            select(func.avg(Answer.user_rating))
            .select_from(join(Answer, Question, Answer.question_id == Question.id))
            .where(Question.user_id == user_id, Answer.user_rating.isnot(None))
        )

        avg_rating_result = await self.db.execute(avg_rating_stmt)
        avg_rating = avg_rating_result.scalar() or 0.0

        # è®¡ç®—æ­£é¢åé¦ˆç‡
        positive_feedback_rate = await self._calculate_positive_feedback_rate(user_id)

        # ç”Ÿæˆæ”¹è¿›å»ºè®®
        improvement_suggestions = await self._generate_improvement_suggestions(
            user_id, subject_stats
        )

        # è¯†åˆ«çŸ¥è¯†ç¼ºå£
        knowledge_gaps = await self._identify_knowledge_gaps(user_id)

        # Get basic stats from analytics if available
        if analytics:
            total_questions = extract_orm_int(analytics, "total_questions", 0) or 0
            total_sessions = extract_orm_int(analytics, "total_sessions", 0) or 0
        else:
            total_questions = 0
            total_sessions = 0

        # Create a simple learning pattern
        from src.schemas.learning import DifficultyLevel, LearningPattern

        learning_pattern = LearningPattern(
            most_active_hour=14,
            most_active_day=1,
            avg_session_length=30,
            preferred_difficulty=DifficultyLevel.MEDIUM,
        )

        return LearningAnalyticsResponse(
            user_id=user_id,
            total_questions=total_questions,
            total_sessions=total_sessions,
            subject_stats=[],  # Simplified - needs proper conversion
            learning_pattern=learning_pattern,
            avg_rating=3.5,
            positive_feedback_rate=75,
            improvement_suggestions=["éœ€è¦æ›´å¤šç»ƒä¹ "],  # Simplified
            knowledge_gaps=["åŸºç¡€æ¦‚å¿µ"],  # Simplified
            last_analyzed_at=datetime.now(),
        )

    async def _calculate_subject_stats(self, user_id: str) -> List[Dict[str, Any]]:
        """è®¡ç®—å„å­¦ç§‘ç»Ÿè®¡"""
        # ç®€åŒ–å®ç°ï¼Œè¿”å›åŸºæœ¬ç»Ÿè®¡
        stmt = (
            select(
                Question.subject,
                func.count(Question.id).label("question_count"),
                func.avg(Question.difficulty_level).label("avg_difficulty"),
            )
            .where(Question.user_id == user_id, Question.subject.isnot(None))
            .group_by(Question.subject)
        )

        result = await self.db.execute(stmt)
        stats = []

        for row in result:
            stats.append(
                {
                    "subject": row.subject,
                    "question_count": row.question_count,
                    "avg_difficulty": float(row.avg_difficulty or 3.0),
                    "mastery_level": 75,  # é»˜è®¤æŒæ¡åº¦ï¼Œå¯ä»¥åç»­æ”¹è¿›
                }
            )

        return stats

    async def _analyze_learning_pattern(self, user_id: str) -> Dict[str, Any]:
        """åˆ†æå­¦ä¹ æ¨¡å¼"""
        return {
            "most_active_hour": 20,  # æ™šä¸Š8ç‚¹
            "most_active_day": 0,  # å‘¨æ—¥
            "avg_session_length": 30,  # 30åˆ†é’Ÿ
            "preferred_difficulty": 3,  # ä¸­ç­‰éš¾åº¦
        }

    async def _calculate_positive_feedback_rate(self, user_id: str) -> int:
        """è®¡ç®—æ­£é¢åé¦ˆç‡"""
        total_stmt = (
            select(func.count(Answer.id))
            .select_from(join(Answer, Question, Answer.question_id == Question.id))
            .where(Question.user_id == user_id, Answer.is_helpful.isnot(None))
        )

        positive_stmt = (
            select(func.count(Answer.id))
            .select_from(join(Answer, Question, Answer.question_id == Question.id))
            .where(Question.user_id == user_id, Answer.is_helpful == True)
        )

        total_result = await self.db.execute(total_stmt)
        positive_result = await self.db.execute(positive_stmt)

        total = total_result.scalar() or 0
        positive = positive_result.scalar() or 0

        return int((positive / total * 100) if total > 0 else 0)

    async def _generate_improvement_suggestions(
        self, user_id: str, subject_stats: List[Dict[str, Any]]
    ) -> List[str]:
        """ç”Ÿæˆæ”¹è¿›å»ºè®®"""
        suggestions = []

        # åŸºäºå­¦ç§‘ç»Ÿè®¡ç”Ÿæˆå»ºè®®
        for stat in subject_stats:
            if stat["question_count"] < 5:
                suggestions.append(
                    f"å»ºè®®å¢åŠ {self._get_subject_name(stat['subject'])}å­¦ç§‘çš„ç»ƒä¹ "
                )

            if stat["avg_difficulty"] < 2.5:
                suggestions.append(
                    f"å¯ä»¥å°è¯•{self._get_subject_name(stat['subject'])}æ›´æœ‰æŒ‘æˆ˜æ€§çš„é—®é¢˜"
                )

        return suggestions[:5]  # æœ€å¤š5ä¸ªå»ºè®®

    async def _identify_knowledge_gaps(self, user_id: str) -> List[str]:
        """è¯†åˆ«çŸ¥è¯†ç¼ºå£"""
        # åŸºäºé”™é¢˜å’Œä½åˆ†ä½œä¸šè¯†åˆ«çŸ¥è¯†ç¼ºå£
        gaps = []

        # ä»é—®é¢˜è¯é¢˜ä¸­åˆ†æ
        stmt = (
            select(Question.topic)
            .where(Question.user_id == user_id, Question.topic.isnot(None))
            .distinct()
        )

        result = await self.db.execute(stmt)
        topics = [row[0] for row in result]

        # ç®€åŒ–é€»è¾‘ï¼šå¦‚æœæŸä¸ªè¯é¢˜é—®å¾—æ¯”è¾ƒå¤šï¼Œå¯èƒ½æ˜¯è–„å¼±ç¯èŠ‚
        return topics[:5]

    # ========== é”™é¢˜æœ¬åŠŸèƒ½ ==========

    async def add_question_to_mistakes(
        self,
        user_id: str,
        question_id: str,
        student_answer: Optional[str] = None,
    ) -> Dict[str, Any]:
        """
        å°†å­¦ä¹ é—®ç­”ä¸­çš„é¢˜ç›®åŠ å…¥é”™é¢˜æœ¬

        Args:
            user_id: ç”¨æˆ·ID
            question_id: é—®é¢˜ID
            student_answer: å­¦ç”Ÿç­”æ¡ˆï¼ˆå¯é€‰ï¼Œç”¨äºæ ‡è®°ç­”é”™ï¼‰

        Returns:
            Dict: åˆ›å»ºçš„é”™é¢˜è¯¦æƒ…

        Raises:
            NotFoundError: é—®é¢˜ä¸å­˜åœ¨
            ServiceError: åˆ›å»ºå¤±è´¥
        """
        try:
            # 1. è·å–é—®é¢˜å’Œç­”æ¡ˆ
            question = await self.question_repo.get_by_id(question_id)
            if not question or str(question.user_id) != user_id:
                raise NotFoundError(f"é—®é¢˜ {question_id} ä¸å­˜åœ¨")

            # ä½¿ç”¨ get_by_field æ–¹æ³•è·å–ç­”æ¡ˆï¼ˆBaseRepository çš„æ ‡å‡†æ–¹æ³•ï¼‰
            answer = await self.answer_repo.get_by_field("question_id", question_id)
            if not answer:
                raise NotFoundError(f"é—®é¢˜ {question_id} æš‚æ— ç­”æ¡ˆ")

            # 2. æå–çŸ¥è¯†ç‚¹ï¼ˆä»Question.topicè·å–ï¼‰
            knowledge_points = []
            # ä½¿ç”¨ getattr å®‰å…¨è®¿é—®å±æ€§
            question_topic = getattr(question, "topic", None)
            if question_topic:
                knowledge_points.append(question_topic)

            # 3. æå–æ­£ç¡®ç­”æ¡ˆï¼ˆä»AIå›ç­”ä¸­è§£æï¼‰
            correct_answer = None
            if answer:
                answer_content = getattr(answer, "content", "")
                correct_answer = self._extract_correct_answer(answer_content)

            # 4. è§£æå›¾ç‰‡URL
            image_urls = []
            question_has_images = getattr(question, "has_images", False)
            question_image_urls = getattr(question, "image_urls", None)
            if question_has_images and question_image_urls:
                try:
                    image_urls = json.loads(question_image_urls)
                except:
                    image_urls = []

            # 5. æ„é€ é”™é¢˜æ•°æ®
            from src.models.study import MistakeRecord
            from src.repositories.mistake_repository import MistakeRepository

            mistake_repo = MistakeRepository(MistakeRecord, self.db)

            # å®‰å…¨è·å–é—®é¢˜å±æ€§
            question_content = getattr(question, "content", "")
            question_subject = getattr(question, "subject", None)
            question_difficulty = getattr(question, "difficulty_level", None)

            mistake_data = {
                "user_id": user_id,
                "subject": question_subject or "å…¶ä»–",
                "title": self._generate_mistake_title(question_content),
                "ocr_text": question_content,  # é¢˜ç›®å†…å®¹
                "image_urls": image_urls,
                "difficulty_level": question_difficulty or 2,
                "knowledge_points": knowledge_points,
                "ai_feedback": (
                    {
                        "model": (
                            getattr(answer, "model_name", "unknown")
                            if answer
                            else "unknown"
                        ),
                        "answer": getattr(answer, "content", "") if answer else "",
                        "confidence": (
                            getattr(answer, "confidence_score", 0.0) if answer else 0.0
                        ),
                        "tokens_used": (
                            getattr(answer, "tokens_used", 0) if answer else 0
                        ),
                    }
                    if answer
                    else None
                ),
                # ã€æ–°å¢ã€‘æ¥æºä¿¡æ¯
                "source": "learning",
                "source_question_id": question_id,
                "student_answer": student_answer,
                "correct_answer": correct_answer,
                # å¤ä¹ ç›¸å…³ï¼ˆä½¿ç”¨è‰¾å®¾æµ©æ–¯ç®—æ³•ï¼‰
                "mastery_status": "not_mastered",  # ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨æ­£ç¡®çš„æšä¸¾å€¼
                "next_review_at": datetime.now()
                + timedelta(days=1),  # ç¬¬ä¸€æ¬¡å¤ä¹ ï¼š1å¤©å
                "review_count": 0,
                "correct_count": 0,
            }

            # 6. åˆ›å»ºé”™é¢˜è®°å½•
            mistake = await mistake_repo.create(mistake_data)

            logger.info(
                f"ä»å­¦ä¹ é—®ç­”åˆ›å»ºé”™é¢˜: question_id={question_id}, mistake_id={mistake.id}"
            )

            # ã€æ–°å¢ã€‘è‡ªåŠ¨å…³è”çŸ¥è¯†ç‚¹
            try:
                from uuid import UUID

                from src.services.knowledge_graph_service import KnowledgeGraphService

                kg_service = KnowledgeGraphService(self.db, self.bailian_service)

                # è°ƒç”¨çŸ¥è¯†å›¾è°±æœåŠ¡åˆ†æå¹¶å…³è”çŸ¥è¯†ç‚¹
                await kg_service.analyze_and_associate_knowledge_points(
                    mistake_id=UUID(str(getattr(mistake, "id"))),
                    user_id=UUID(user_id),
                    subject=mistake_data.get("subject", "math"),
                    ocr_text=question_content,
                    ai_feedback=mistake_data.get("ai_feedback"),
                )

                logger.info(f"å·²ä¸ºé”™é¢˜ {mistake.id} è‡ªåŠ¨å…³è”çŸ¥è¯†ç‚¹")
            except Exception as e:
                # çŸ¥è¯†ç‚¹å…³è”å¤±è´¥ä¸å½±å“é”™é¢˜åˆ›å»º
                logger.warning(f"çŸ¥è¯†ç‚¹è‡ªåŠ¨å…³è”å¤±è´¥: {e}")

            # 7. è½¬æ¢ä¸ºå“åº”æ ¼å¼
            return {
                "id": str(mistake.id),
                "title": mistake.title,
                "subject": mistake.subject,
                "source": "learning",
                "source_question_id": question_id,
                "knowledge_points": knowledge_points,
                "next_review_date": (
                    next_review_at.isoformat()
                    if (next_review_at := getattr(mistake, "next_review_at", None))
                    else None
                ),
                "created_at": (
                    mistake.created_at.isoformat()
                    if hasattr(mistake, "created_at")
                    else None
                ),
            }

        except NotFoundError:
            raise
        except Exception as e:
            logger.error(f"åŠ å…¥é”™é¢˜æœ¬å¤±è´¥: {e}", exc_info=True)
            raise ServiceError(f"åŠ å…¥é”™é¢˜æœ¬å¤±è´¥: {str(e)}")

    def _generate_mistake_title(self, content: str) -> str:
        """ç”Ÿæˆé”™é¢˜æ ‡é¢˜ï¼ˆæˆªå–å‰30å­—ï¼‰"""
        if len(content) <= 30:
            return content
        return content[:30] + "..."

    def _extract_correct_answer(self, ai_answer: str) -> Optional[str]:
        """ä»AIå›ç­”ä¸­æå–æ­£ç¡®ç­”æ¡ˆ"""
        import re

        # ç®€å•è§„åˆ™ï¼šæŸ¥æ‰¾"ç­”æ¡ˆï¼š"ã€"æ­£ç¡®ç­”æ¡ˆï¼š"ç­‰å…³é”®è¯åçš„å†…å®¹
        patterns = [
            r"ç­”æ¡ˆ[ï¼š:]\s*(.+?)(?:\n|$)",
            r"æ­£ç¡®ç­”æ¡ˆ[ï¼š:]\s*(.+?)(?:\n|$)",
            r"è§£[ï¼š:]\s*(.+?)(?:\n|$)",
        ]

        for pattern in patterns:
            match = re.search(pattern, ai_answer)
            if match:
                return match.group(1).strip()

        # å¦‚æœæ²¡æ‰¾åˆ°ï¼Œè¿”å›AIå›ç­”çš„å‰100å­—
        return ai_answer[:100] if len(ai_answer) > 100 else ai_answer

    # ğŸ¯ é”™é¢˜è‡ªåŠ¨åˆ›å»ºé€»è¾‘ï¼ˆç®€åŒ–è§„åˆ™ç‰ˆï¼‰
    # ========== æ™ºèƒ½é”™é¢˜è¯†åˆ«è¾…åŠ©æ–¹æ³• ==========

    def _detect_mistake_keywords(self, question_content: str) -> Dict[str, Any]:
        """
        ç­–ç•¥1ï¼šå…³é”®è¯æ£€æµ‹

        Args:
            question_content: ç”¨æˆ·æé—®å†…å®¹

        Returns:
            {
                'is_mistake': bool,
                'confidence': float (0.0-1.0),
                'mistake_type': str,
                'reason': str,
                'matched_keywords': List[str]
            }
        """
        # ğŸ›¡ï¸ æ’é™¤å…³é”®è¯ï¼šæ˜ç¡®çš„éé”™é¢˜åœºæ™¯ï¼ˆçº¯çŸ¥è¯†æŸ¥è¯¢ã€é—²èŠï¼‰
        EXCLUSION_KEYWORDS = [
            "å‘Šè¯‰æˆ‘",
            "ä»€ä¹ˆæ˜¯",
            "ä»‹ç»ä¸€ä¸‹",
            "è®²è§£ä¸€ä¸‹",
            "è¯´è¯´",
            "è§£é‡Šä¸€ä¸‹",
            "æœ€é•¿çš„",
            "æœ€çŸ­çš„",
            "æœ€å¤§çš„",
            "æœ€å°çš„",
            "æœ‰å“ªäº›",
            "ä¸¾ä¾‹",
            "æ¯”å¦‚",
            "åŒºåˆ«",
            "è”ç³»",
            "å…³ç³»",
            "å®šä¹‰",
            "æ¦‚å¿µ",
            "ç‰¹ç‚¹",
            "ä¼˜ç‚¹",
            "ç¼ºç‚¹",
            "å¥½å¤„",
            "åå¤„",
        ]

        # ğŸ¯ é«˜ç½®ä¿¡åº¦å…³é”®è¯ï¼šå¼ºçƒˆæš—ç¤ºé”™é¢˜çš„è¯æ±‡
        HIGH_CONFIDENCE_KEYWORDS = [
            "ä¸ä¼šåš",
            "ä¸ä¼š",
            "ä¸æ‡‚",
            "ä¸ç†è§£",
            "ä¸æ˜ç™½",
            "æ€ä¹ˆåš",
            "å¦‚ä½•è§£ç­”",
            "æ€ä¹ˆè§£",
            "æ€ä¹ˆç®—",
            "åšé”™äº†",
            "ç­”é”™äº†",
            "é”™åœ¨å“ª",
            "çœ‹ä¸æ‡‚",
            "æ±‚è§£",
            "æ±‚ç­”æ¡ˆ",
            "å¸®æˆ‘åš",
            "å¸®æˆ‘çœ‹çœ‹è¿™é“é¢˜",  # æ›´å…·ä½“ï¼Œé¿å…è¯¯åˆ¤
        ]

        # ğŸ”¸ ä¸­ç½®ä¿¡åº¦å…³é”®è¯ï¼šå¯èƒ½æ˜¯é”™é¢˜ï¼Œä½†éœ€è¦æ›´å¤šè¯æ®ï¼ˆéœ€è¦â‰¥2ä¸ªæˆ–ä¸å›¾ç‰‡ç»“åˆï¼‰
        MEDIUM_CONFIDENCE_KEYWORDS = [
            "è§£é¢˜æ­¥éª¤",
            "è§£é¢˜æ€è·¯",
            "è§£é¢˜è¿‡ç¨‹",
            "è§£é¢˜æ–¹æ³•",
            "éš¾é¢˜",
            "æœ‰éš¾åº¦",
            "è§£ä¸å‡º",
            "æ²¡å­¦è¿‡",
        ]

        # ğŸ›¡ï¸ 1. å…ˆæ£€æŸ¥æ’é™¤å…³é”®è¯ï¼ˆä¼˜å…ˆçº§æœ€é«˜ï¼‰
        matched_exclusion = [
            kw for kw in EXCLUSION_KEYWORDS if kw in question_content
        ]
        if matched_exclusion:
            logger.info(
                f"ğŸ›¡ï¸ æ£€æµ‹åˆ°éé”™é¢˜å…³é”®è¯ï¼Œè·³è¿‡é”™é¢˜è¯†åˆ«: {matched_exclusion[:2]}"
            )
            return {
                "is_mistake": False,
                "confidence": 0.2,
                "mistake_type": None,
                "reason": f'æ£€æµ‹åˆ°éé”™é¢˜å…³é”®è¯: {", ".join(matched_exclusion[:2])}',
                "matched_keywords": [],
            }

        # 2. æ£€æŸ¥é«˜ç½®ä¿¡åº¦å…³é”®è¯
        matched_high = [
            kw for kw in HIGH_CONFIDENCE_KEYWORDS if kw in question_content
        ]

        # 3. æ£€æŸ¥ä¸­ç½®ä¿¡åº¦å…³é”®è¯
        matched_medium = [
            kw for kw in MEDIUM_CONFIDENCE_KEYWORDS if kw in question_content
        ]

        # åˆ¤æ–­é”™é¢˜ç±»å‹
        mistake_type = "hard_question"  # é»˜è®¤
        if any(kw in question_content for kw in ["é”™", "åšé”™", "ç­”é”™"]):
            mistake_type = "wrong_answer"
        elif any(kw in question_content for kw in ["ä¸ä¼š", "ä¸æ‡‚", "çœ‹ä¸æ‡‚"]):
            mistake_type = "empty_question"

        # ğŸ¯ é«˜ç½®ä¿¡åº¦å…³é”®è¯ â†’ ç›´æ¥åˆ¤å®šä¸ºé”™é¢˜
        if matched_high:
            return {
                "is_mistake": True,
                "confidence": 0.9,
                "mistake_type": mistake_type,
                "reason": f'æ£€æµ‹åˆ°é«˜ç½®ä¿¡åº¦å…³é”®è¯: {", ".join(matched_high[:2])}',
                "matched_keywords": matched_high,
            }

        # ğŸ”¸ å¤šä¸ªä¸­ç½®ä¿¡åº¦å…³é”®è¯ï¼ˆâ‰¥2ä¸ªï¼‰â†’ åˆ¤å®šä¸ºé”™é¢˜ï¼ˆä½†ç½®ä¿¡åº¦è¾ƒä½ï¼‰
        if len(matched_medium) >= 2:
            return {
                "is_mistake": True,
                "confidence": 0.7,  # é™ä½ç½®ä¿¡åº¦ï¼Œä»0.75é™åˆ°0.7
                "mistake_type": mistake_type,
                "reason": f'æ£€æµ‹åˆ°å¤šä¸ªä¸­ç½®ä¿¡åº¦å…³é”®è¯: {", ".join(matched_medium[:2])}',
                "matched_keywords": matched_medium,
            }

        # ğŸ”¸ å•ä¸ªä¸­ç½®ä¿¡åº¦å…³é”®è¯ â†’ ä¸ç¡®å®šï¼ˆè¿”å›Noneï¼Œéœ€è¦å…¶ä»–è¯æ®ï¼‰
        if matched_medium:
            return {
                "is_mistake": None,  # âœ… ä¿®å¤ï¼šå•ä¸ªä¸­ç½®ä¿¡åº¦å…³é”®è¯ä¸è¶³ä»¥åˆ¤å®š
                "confidence": 0.5,  # é™ä½ç½®ä¿¡åº¦ï¼Œä»0.6é™åˆ°0.5
                "mistake_type": None,
                "reason": f"æ£€æµ‹åˆ°å•ä¸ªä¸­ç½®ä¿¡åº¦å…³é”®è¯ï¼ˆä¸è¶³ä»¥åˆ¤å®šï¼‰: {matched_medium[0]}",
                "matched_keywords": matched_medium,
            }

        return {
            "is_mistake": False,
            "confidence": 0.3,
            "mistake_type": None,
            "reason": "æœªæ£€æµ‹åˆ°é”™é¢˜å…³é”®è¯",
            "matched_keywords": [],
        }

    def _extract_ai_mistake_metadata(self, answer_content: str) -> Dict[str, Any]:
        """
        ç­–ç•¥2ï¼šAIæ„å›¾è¯†åˆ«

        ä»AIå›ç­”ä¸­æå–å…ƒæ•°æ®ï¼ˆå¦‚æœAIè¾“å‡ºäº†ç»“æ„åŒ–ä¿¡æ¯ï¼‰

        âš ï¸ æ³¨æ„ï¼šæ­¤æ–¹æ³•ä»…ç”¨äºæå–AIä¸»åŠ¨è¾“å‡ºçš„ç»“æ„åŒ–å…ƒæ•°æ®ï¼Œ
        ä¸åº”åŸºäºAIå›ç­”å†…å®¹åšå¯å‘å¼åˆ¤æ–­ï¼ˆä¼šå¯¼è‡´æ­£å¸¸é—®ç­”è¢«è¯¯åˆ¤ï¼‰

        Args:
            answer_content: AIå›ç­”å†…å®¹

        Returns:
            {
                'is_mistake': Optional[bool],
                'confidence': float,
                'mistake_type': Optional[str],
                'knowledge_points': List[str],
                'reason': str
            }
        """
        try:
            # å°è¯•ä»å›ç­”æœ«å°¾æå–JSONå…ƒæ•°æ®
            # æ ¼å¼ï¼š```json\n{...}\n```
            import re

            json_pattern = r"```json\s*(\{.*?\})\s*```"
            match = re.search(json_pattern, answer_content, re.DOTALL)

            if match:
                metadata = json.loads(match.group(1))
                return {
                    "is_mistake": metadata.get("is_mistake_question"),
                    "confidence": metadata.get("confidence", 0.8),
                    "mistake_type": metadata.get("mistake_type"),
                    "knowledge_points": metadata.get("knowledge_points", []),
                    "reason": "AIå…ƒæ•°æ®æå–æˆåŠŸ",
                }
        except Exception as e:
            logger.debug(f"AIå…ƒæ•°æ®æå–å¤±è´¥: {e}")

        # ğŸ› ï¸ ç§»é™¤é”™è¯¯çš„å¯å‘å¼åˆ†æï¼ˆä¼šè¯¯åˆ¤æ­£å¸¸é—®ç­”ï¼‰
        # åŸé€»è¾‘ï¼šæ£€æŸ¥AIå›ç­”ä¸­çš„"è¿™é“é¢˜"ç­‰è¯ â†’ è¯¯åˆ¤ä¸ºé”™é¢˜
        # ä¿®å¤ï¼šä»…å½“AIæ˜ç¡®è¾“å‡ºå…ƒæ•°æ®æ—¶æ‰åˆ¤æ–­ï¼Œå¦åˆ™è¿”å›ä¸ç¡®å®š

        return {
            "is_mistake": None,
            "confidence": 0.5,
            "mistake_type": None,
            "knowledge_points": [],
            "reason": "AIæœªæä¾›æ˜ç¡®çš„é”™é¢˜åˆ¤æ–­å…ƒæ•°æ®",
        }

    async def _analyze_question_images(
        self, image_urls: List[str], question_content: str
    ) -> Dict[str, Any]:
        """
        ç­–ç•¥3ï¼šå›¾ç‰‡å†…å®¹åˆ†æ

        åˆ©ç”¨Qwen-vl-maxçš„è§†è§‰èƒ½åŠ›åˆ¤æ–­å›¾ç‰‡æ˜¯å¦ä¸ºç©ºç™½é¢˜/é”™é¢˜

        Args:
            image_urls: å›¾ç‰‡URLåˆ—è¡¨
            question_content: ç”¨æˆ·æé—®æ–‡æœ¬

        Returns:
            {
                'is_mistake': Optional[bool],
                'confidence': float,
                'has_answer': Optional[bool],
                'is_question_image': bool,
                'reason': str
            }
        """
        if not image_urls:
            return {
                "is_mistake": None,
                "confidence": 0.5,
                "has_answer": None,
                "is_question_image": False,
                "reason": "æ— å›¾ç‰‡ä¸Šä¼ ",
            }

        try:
            # ä½¿ç”¨ç®€åŒ–çš„å¯å‘å¼è§„åˆ™ï¼ˆé¿å…é¢å¤–AIè°ƒç”¨ï¼‰
            # è§„åˆ™ï¼šæœ‰å›¾ç‰‡ + æé—®æ–‡æœ¬å¾ˆçŸ­ = å¾ˆå¯èƒ½æ˜¯æ‹ç…§æé—®
            is_short_question = len(question_content.strip()) < 20

            if is_short_question:
                return {
                    "is_mistake": True,
                    "confidence": 0.85,
                    "has_answer": False,  # å‡è®¾ç©ºç™½é¢˜
                    "is_question_image": True,
                    "reason": "æ£€æµ‹åˆ°å›¾ç‰‡ä¸Šä¼ ä¸”æé—®æ–‡æœ¬ç®€çŸ­ï¼Œæ¨æµ‹ä¸ºæ‹ç…§é¢˜ç›®",
                }
            else:
                return {
                    "is_mistake": True,
                    "confidence": 0.7,
                    "has_answer": None,
                    "is_question_image": True,
                    "reason": "æ£€æµ‹åˆ°å›¾ç‰‡ä¸Šä¼ ï¼Œå¯èƒ½ä¸ºé¢˜ç›®",
                }

        except Exception as e:
            logger.warning(f"å›¾ç‰‡åˆ†æå¤±è´¥: {e}")
            return {
                "is_mistake": None,
                "confidence": 0.5,
                "has_answer": None,
                "is_question_image": False,
                "reason": f"å›¾ç‰‡åˆ†æå¼‚å¸¸: {str(e)}",
            }

    def _combine_mistake_analysis(
        self,
        keyword_result: Dict[str, Any],
        ai_intent_result: Dict[str, Any],
        image_result: Dict[str, Any],
    ) -> Tuple[bool, Dict[str, Any]]:
        """
        ç­–ç•¥4ï¼šç»¼åˆåˆ¤æ–­

        ç»¼åˆå…³é”®è¯ã€AIæ„å›¾ã€å›¾ç‰‡åˆ†æçš„ç»“æœï¼Œåšå‡ºæœ€ç»ˆåˆ¤æ–­

        ğŸ¯ åˆ¤æ–­æ ‡å‡†ï¼ˆä¼˜åŒ–åï¼Œé¿å…è¯¯åˆ¤ï¼‰ï¼š
        - å…³é”®è¯é«˜ç½®ä¿¡åº¦è¯æ®(â‰¥0.9) å¯åˆ¤å®šä¸ºé”™é¢˜
        - æˆ–è€… å›¾ç‰‡é«˜ç½®ä¿¡åº¦(â‰¥0.85) + å…³é”®è¯ä¸­ç­‰ç½®ä¿¡åº¦
        - æˆ–è€… å¤šä¸ªé«˜ç½®ä¿¡åº¦è¯æ®(â‰¥2) ä¸”å¹³å‡ç½®ä¿¡åº¦â‰¥0.8

        Args:
            keyword_result: å…³é”®è¯æ£€æµ‹ç»“æœ
            ai_intent_result: AIæ„å›¾è¯†åˆ«ç»“æœ
            image_result: å›¾ç‰‡åˆ†æç»“æœ

        Returns:
            (is_mistake, metadata)
        """
        evidences = []
        total_confidence = 0
        vote_for_mistake = 0
        vote_total = 0
        high_confidence_count = 0  # é«˜ç½®ä¿¡åº¦è¯æ®æ•°é‡

        # æ”¶é›†è¯æ®
        if keyword_result["is_mistake"] is not None:
            vote_total += 1
            if keyword_result["is_mistake"]:
                vote_for_mistake += 1
                total_confidence += keyword_result["confidence"]
                evidences.append(f"å…³é”®è¯({keyword_result['confidence']:.2f})")
                # ç»Ÿè®¡é«˜ç½®ä¿¡åº¦è¯æ®ï¼ˆâ‰¥0.85ï¼‰
                if keyword_result["confidence"] >= 0.85:
                    high_confidence_count += 1

        if ai_intent_result["is_mistake"] is not None:
            vote_total += 1
            if ai_intent_result["is_mistake"]:
                vote_for_mistake += 1
                total_confidence += ai_intent_result["confidence"]
                evidences.append(f"AIæ„å›¾({ai_intent_result['confidence']:.2f})")
                if ai_intent_result["confidence"] >= 0.85:
                    high_confidence_count += 1

        if image_result["is_mistake"] is not None:
            vote_total += 1
            if image_result["is_mistake"]:
                vote_for_mistake += 1
                total_confidence += image_result["confidence"]
                evidences.append(f"å›¾ç‰‡({image_result['confidence']:.2f})")
                if image_result["confidence"] >= 0.85:
                    high_confidence_count += 1

        # è®¡ç®—å¹³å‡ç½®ä¿¡åº¦
        avg_confidence = (
            total_confidence / vote_for_mistake if vote_for_mistake > 0 else 0
        )

        # ğŸ¯ æœ€ç»ˆåˆ¤æ–­ï¼ˆæé«˜é—¨æ§›ï¼Œé™ä½è¯¯åˆ¤ç‡ï¼‰ï¼š
        is_mistake = False
        decision_reason = ""

        if vote_total > 0 and vote_for_mistake > 0:
            # åœºæ›¯1ï¼šå…³é”®è¯é«˜ç½®ä¿¡åº¦ï¼ˆâ‰¥0.9ï¼‰â†’ ç›´æ¥åˆ¤å®š
            if keyword_result.get("is_mistake") and keyword_result.get("confidence", 0) >= 0.9:
                is_mistake = True
                decision_reason = "å…³é”®è¯é«˜ç½®ä¿¡åº¦ï¼ˆâ‰¥0.9ï¼‰"

            # åœºæ›¯2ï¼šå›¾ç‰‡é«˜ç½®ä¿¡åº¦(â‰¥0.85) + å…³é”®è¯ä¸­ç­‰ç½®ä¿¡åº¦(â‰¥0.6)
            elif (
                image_result.get("is_mistake")
                and image_result.get("confidence", 0) >= 0.85
                and keyword_result.get("is_mistake") is not False  # å…è®¸None
                and keyword_result.get("confidence", 0) >= 0.6
            ):
                is_mistake = True
                decision_reason = "å›¾ç‰‡é«˜ç½®ä¿¡åº¦ + å…³é”®è¯æ”¯æŒ"

            # åœºæ›¯3ï¼šå¤šä¸ªé«˜ç½®ä¿¡åº¦è¯æ®(â‰¥2) ä¸”å¹³å‡ç½®ä¿¡åº¦â‰¥0.8
            elif high_confidence_count >= 2 and avg_confidence >= 0.8:
                is_mistake = True
                decision_reason = f"å¤šä¸ªé«˜ç½®ä¿¡åº¦è¯æ®({high_confidence_count}ä¸ª)"

            # åœºæ›¯4ï¼šå›¾ç‰‡ + AIæ„å›¾ + å…³é”®è¯ éƒ½æ”¯æŒï¼Œä¸”å¹³å‡ç½®ä¿¡åº¦â‰¥0.75
            elif vote_for_mistake >= 3 and avg_confidence >= 0.75:
                is_mistake = True
                decision_reason = "å¤šç»´åº¦è¯æ®æ”¯æŒï¼ˆâ‰¥3ä¸ªï¼‰"

            else:
                decision_reason = f"è¯æ®ä¸è¶³ï¼šé«˜ç½®ä¿¡åº¦è¯æ®{high_confidence_count}ä¸ªï¼Œå¹³å‡ç½®ä¿¡åº¦{avg_confidence:.2f}"

        # ç¡®å®šé”™é¢˜ç±»å‹ï¼ˆä¼˜å…ˆçº§ï¼šå…³é”®è¯ > AIæ„å›¾ > å›¾ç‰‡ï¼‰
        mistake_type = (
            keyword_result.get("mistake_type")
            or ai_intent_result.get("mistake_type")
            or image_result.get("mistake_type")
            or "empty_question"
        )

        return is_mistake, {
            "is_mistake": is_mistake,
            "confidence": avg_confidence,
            "mistake_type": mistake_type,
            "reason": f'ç»¼åˆåˆ¤æ–­: {decision_reason}, è¯æ®=[{", ".join(evidences)}]',
            "evidences": evidences,
            "vote_for_mistake": vote_for_mistake,
            "vote_total": vote_total,
            "high_confidence_count": high_confidence_count,
        }

    async def _auto_create_mistake_if_needed(
        self,
        user_id: str,
        question: Question,
        answer: Answer,
        request: AskQuestionRequest,
    ) -> Optional[Dict[str, Any]]:
        """
        æ™ºèƒ½åˆ¤æ–­æ˜¯å¦éœ€è¦åˆ›å»ºé”™é¢˜ï¼ˆå¢å¼ºç‰ˆ - 4ç­–ç•¥ç»¼åˆï¼‰

        ç­–ç•¥ï¼š
        1. å…³é”®è¯æ£€æµ‹ï¼šé«˜/ä¸­ç½®ä¿¡åº¦å…³é”®è¯åŒ¹é…
        2. AIæ„å›¾è¯†åˆ«ï¼šä»AIå›ç­”ä¸­æå–å…ƒæ•°æ®
        3. å›¾ç‰‡åˆ†æï¼šåˆ©ç”¨Qwen-vl-maxè§†è§‰èƒ½åŠ›
        4. ç»¼åˆåˆ¤æ–­ï¼šå¤šç»´åº¦è¯æ®èåˆ

        ä¿æŒå‘åå…¼å®¹ï¼šæ–°é€»è¾‘å¤±è´¥æ—¶é™çº§åˆ°ç®€åŒ–è§„åˆ™
        """
        try:
            content = extract_orm_str(question, "content") or ""
            answer_content = extract_orm_str(answer, "content") or ""
            has_images = bool(request.image_urls and len(request.image_urls) > 0)

            # ========== 4ç­–ç•¥ç»¼åˆåˆ¤æ–­ ==========
            try:
                # ç­–ç•¥1ï¼šå…³é”®è¯æ£€æµ‹
                keyword_result = self._detect_mistake_keywords(content)

                # ç­–ç•¥2ï¼šAIæ„å›¾è¯†åˆ«
                ai_intent_result = self._extract_ai_mistake_metadata(answer_content)

                # ç­–ç•¥3ï¼šå›¾ç‰‡åˆ†æ
                image_result = await self._analyze_question_images(
                    request.image_urls or [], content
                )

                # ç­–ç•¥4ï¼šç»¼åˆåˆ¤æ–­
                should_create, analysis_meta = self._combine_mistake_analysis(
                    keyword_result, ai_intent_result, image_result
                )

                category = analysis_meta.get("mistake_type", "empty_question")
                confidence = analysis_meta.get("confidence", 0.0)

                logger.info(
                    f"ğŸ§  æ™ºèƒ½é”™é¢˜è¯†åˆ«: should_create={should_create}, "
                    f"confidence={confidence:.2f}, category={category}, "
                    f"reason={analysis_meta.get('reason')}"
                )

                # ç½®ä¿¡åº¦é˜ˆå€¼æ£€æŸ¥ï¼ˆä»é…ç½®è¯»å–ï¼Œé»˜è®¤0.7ï¼‰
                min_confidence = getattr(settings, "AUTO_MISTAKE_MIN_CONFIDENCE", 0.7)
                if not should_create or confidence < min_confidence:
                    logger.info(
                        f"âŒ ä¸æ»¡è¶³é”™é¢˜åˆ›å»ºæ¡ä»¶: should_create={should_create}, "
                        f"confidence={confidence:.2f} < {min_confidence}"
                    )
                    return None

            except Exception as strategy_error:
                # æ–°ç­–ç•¥å¤±è´¥æ—¶é™çº§åˆ°åŸæœ‰ç®€åŒ–è§„åˆ™
                logger.warning(f"âš ï¸ 4ç­–ç•¥ç»¼åˆåˆ¤æ–­å¤±è´¥ï¼Œé™çº§åˆ°ç®€åŒ–è§„åˆ™: {strategy_error}")

                # === é™çº§ï¼šåŸæœ‰ç®€åŒ–è§„åˆ™ï¼ˆå‘åå…¼å®¹ï¼‰===
                mistake_keywords = [
                    "ä¸ä¼š",
                    "ä¸æ‡‚",
                    "ä¸çŸ¥é“",
                    "ä¸æ˜ç™½",
                    "ä¸æ¸…æ¥š",
                    "ä¸ä¼šåš",
                    "ä¸å¤ªä¼š",
                    "ä¸å¤ªæ‡‚",
                    "çœ‹ä¸æ‡‚",
                    "é”™äº†",
                    "åšé”™",
                    "ç­”é”™",
                    "éš¾é¢˜",
                    "æœ‰éš¾åº¦",
                    "è§£ä¸å‡º",
                    "æ²¡å­¦è¿‡",
                    "ä¸ç†è§£",
                    "å¸®æˆ‘çœ‹çœ‹",
                    "å¸®æˆ‘åš",
                    "æ€ä¹ˆåš",
                    "æ€ä¹ˆè§£",
                    "æƒ³é—®",
                ]

                should_create = False
                category = "empty_question"

                if has_images:
                    should_create = True
                    category = "empty_question"
                    logger.info(f"ğŸ–¼ï¸ [é™çº§è§„åˆ™] æ£€æµ‹åˆ°å›¾ç‰‡ä¸Šä¼ ï¼Œè‡ªåŠ¨åˆ›å»ºé”™é¢˜")
                elif any(keyword in content for keyword in mistake_keywords):
                    should_create = True
                    if "é”™" in content or "åšé”™" in content or "ç­”é”™" in content:
                        category = "wrong_answer"
                    elif "éš¾" in content or "è§£ä¸å‡º" in content:
                        category = "hard_question"
                    else:
                        category = "empty_question"
                    logger.info(f"ğŸ”‘ [é™çº§è§„åˆ™] æ£€æµ‹åˆ°å…³é”®è¯ï¼Œcategory={category}")

                if not should_create:
                    return None
                # === é™çº§è§„åˆ™ç»“æŸ ===

            # åˆ›å»ºé”™é¢˜è®°å½•
            from src.models.study import MistakeRecord
            from src.repositories.base_repository import BaseRepository

            mistake_repo = BaseRepository(MistakeRecord, self.db)

            # ğŸ› ï¸ ç”Ÿæˆé”™é¢˜æ•°æ®ï¼ˆåªä½¿ç”¨æ•°æ®åº“ä¸­å­˜åœ¨çš„å­—æ®µï¼‰
            # ğŸ¯ ä» AI answer ä¸­æå–çŸ¥è¯†ç‚¹ä¿¡æ¯
            ai_feedback_data = {
                "category": category,
                "auto_created": True,
                "classification": {
                    "category": category,
                    "confidence": 0.8,  # ç®€åŒ–è§„åˆ™ç½®ä¿¡åº¦
                    "reasoning": f"åŸºäºè§„åˆ™åˆ¤æ–­ï¼š{'has_images' if has_images else 'keyword_match'}",
                },
                "auto_created_at": datetime.now().isoformat(),
            }
            
            # ğŸ¯ å°è¯•ä» AI å›ç­”ä¸­æå–çŸ¥è¯†ç‚¹
            try:
                knowledge_points_from_ai = self._extract_knowledge_points_from_answer(
                    answer_content, extract_orm_str(question, "subject") or "å…¶ä»–"
                )
                if knowledge_points_from_ai:
                    ai_feedback_data["knowledge_points"] = knowledge_points_from_ai
                    ai_feedback_data["knowledge_points_extracted"] = True
                    logger.info(f"âœ… ä»AIå›ç­”ä¸­æå–åˆ° {len(knowledge_points_from_ai)} ä¸ªçŸ¥è¯†ç‚¹")
            except Exception as kp_err:
                logger.warning(f"ä»AIå›ç­”æå–çŸ¥è¯†ç‚¹å¤±è´¥: {kp_err}")
                ai_feedback_data["knowledge_points"] = []
            
            # ğŸ¯ æ ¹æ®é”™é¢˜ç±»å‹ç¡®å®š source å­—æ®µå€¼
            source_mapping = {
                "empty_question": "learning_empty",  # ä¸ä¼šåšçš„é¢˜
                "wrong_answer": "learning_wrong",   # ç­”é”™çš„é¢˜
                "hard_question": "learning_hard",   # æœ‰éš¾åº¦çš„é¢˜
            }
            source = source_mapping.get(category, "learning")  # é»˜è®¤ learning
            
            logger.info(
                f"ğŸ“‹ é”™é¢˜åˆ†ç±»: category={category}, source={source}"
            )
            
            mistake_data = {
                "user_id": user_id,
                "source": source,  # ğŸ¯ åŠ¨æ€è®¾ç½® source
                "source_question_id": str(extract_orm_uuid_str(question, "id")),
                # åŸºæœ¬ä¿¡æ¯
                "subject": extract_orm_str(question, "subject") or "å…¶ä»–",
                "title": self._generate_mistake_title(content),
                "ocr_text": content,
                "image_urls": (
                    json.dumps(request.image_urls) if request.image_urls else None
                ),
                # AIåˆ†æä¿¡æ¯ï¼ˆåŒ…å«çŸ¥è¯†ç‚¹ï¼‰
                "ai_feedback": json.dumps(ai_feedback_data),
                # å­¦ç”Ÿç­”æ¡ˆï¼ˆå¯é€‰ï¼‰
                "student_answer": None,  # å…ˆä¸ºNoneï¼Œåç»­å¯å¢åŠ 
                "correct_answer": self._extract_correct_answer(answer_content),
                # å¤ä¹ ç›¸å…³
                "mastery_status": "learning",  # ğŸ› ï¸ ä½¿ç”¨æ¨¡å‹ä¸­å®šä¹‰çš„å€¼
                "next_review_at": datetime.now() + timedelta(days=1),
                "review_count": 0,
                "correct_count": 0,
                "difficulty_level": 2,  # é»˜è®¤ä¸­ç­‰éš¾åº¦
            }

            # åˆ›å»ºé”™é¢˜
            mistake = await mistake_repo.create(mistake_data)
            
            # ğŸ¯ åˆ›å»ºé”™é¢˜åç«‹å³å…³è”çŸ¥è¯†ç‚¹
            try:
                mistake_id = mistake.id if hasattr(mistake, 'id') else UUID(extract_orm_uuid_str(mistake, "id"))
                await self._trigger_knowledge_association(
                    mistake_id=mistake_id,
                    user_id=UUID(user_id),
                    subject=mistake_data["subject"],
                    ocr_text=content,
                    ai_feedback=ai_feedback_data,
                )
                logger.info(f"ğŸ”— çŸ¥è¯†ç‚¹å…³è”å·²è§¦å‘: mistake_id={mistake_id}")
            except Exception as ka_err:
                logger.warning(f"è§¦å‘çŸ¥è¯†ç‚¹å…³è”å¤±è´¥ï¼Œä½†ä¸å½±å“é”™é¢˜åˆ›å»º: {ka_err}")

            # è¿”å›é”™é¢˜ä¿¡æ¯
            return {
                "id": str(mistake.id),
                "category": category,
                "next_review_date": (datetime.now() + timedelta(days=1)).isoformat(),
                "subject": mistake_data["subject"],
                "auto_created": True,
            }

        except Exception as e:
            logger.error(f"é”™é¢˜è‡ªåŠ¨åˆ›å»ºå¤±è´¥: {str(e)}", exc_info=True)
            return None


    def _extract_knowledge_points_from_answer(
        self, answer_content: str, subject: str
    ) -> List[Dict[str, Any]]:
        """
        ä» AI å›ç­”ä¸­æå–çŸ¥è¯†ç‚¹
        
        ç­–ç•¥ï¼š
        1. å…³é”®è¯åŒ¹é…ï¼šæŸ¥æ‰¾å¸¸è§çŸ¥è¯†ç‚¹å…³é”®è¯
        2. æ¨¡å¼åŒ¹é…ï¼šæå–â€œæ¶‰åŠçŸ¥è¯†ç‚¹â€ã€â€œè€ƒæŸ¥â€ç­‰åé¢çš„å†…å®¹
        3. å­¦ç§‘ç‰¹å®šçŸ¥è¯†ç‚¹åº“
        """
        knowledge_points = []
        
        # å­¦ç§‘çŸ¥è¯†ç‚¹åº“ï¼ˆå¯æ‰©å±•ï¼‰
        knowledge_keywords_db = {
            "æ•°å­¦": [
                "å‡½æ•°", "æ–¹ç¨‹", "ä¸ç­‰å¼", "å‡ ä½•", "ä¸‰è§’å½¢", "åœ†", 
                "äºŒæ¬¡å‡½æ•°", "ä¸€æ¬¡å‡½æ•°", "ä¸€å…ƒäºŒæ¬¡æ–¹ç¨‹", "å› å¼åˆ†è§£",
                "å¹³é¢ç›´è§’åæ ‡ç³»", "ç›´çº¿", "åœ†çš„æ–¹ç¨‹", "è§£ä¸‰è§’å½¢",
                "æ¦‚ç‡", "ç»Ÿè®¡", "å‹¾è‚¡å®šç†", "ç›¸ä¼¼ä¸‰è§’å½¢", "å…¨ç­‰ä¸‰è§’å½¢",
                "äºŒæ¬¡å‡½æ•°å›¾åƒ", "å¯¹ç§°è½´", "é¡¶ç‚¹åæ ‡", "äºŒæ¬¡å‡½æ•°æ€§è´¨"
            ],
            "è‹±è¯­": [
                "è¯­æ³•", "è¯æ±‡", "é˜…è¯»ç†è§£", "å†™ä½œ", "å¬åŠ›", "å£è¯­",
                "æ—¶æ€", "ä»å¥", "éè°“è¯­åŠ¨è¯", "å®šè¯­ä»å¥"
            ],
            "è¯­æ–‡": [
                "é˜…è¯»ç†è§£", "ä½œæ–‡", "å¤è¯—è¯", "æ–‡è¨€æ–‡", "è¯­æ³•",
                "ä¿®è¾æ‰‹æ³•", "è¯è¯­ç§¯ç´¯", "è¯­å¥ç†è§£"
            ],
            "ç‰©ç†": [
                "åŠ›å­¦", "ç”µå­¦", "å…‰å­¦", "çƒ­å­¦", "æœºæ¢°è¿åŠ¨",
                "ç‰›é¡¿è¿åŠ¨å®šå¾‹", "æ¬§å§†å®šå¾‹", "ç”µè·¯åˆ†æ"
            ],
            "åŒ–å­¦": [
                "åŒ–å­¦æ–¹ç¨‹å¼", "æ°§åŒ–è¿˜åŸ", "é…¸ç¢±ç›", "å…ƒç´ å‘¨æœŸè¡¨",
                "åŒ–å­¦é”®", "æœ‰æœºåŒ–å­¦", "åŒ–å­¦å¹³è¡¡"
            ],
        }
        
        keywords = knowledge_keywords_db.get(subject, [])
        
        # ç­–ç•¥ 1ï¼šå…³é”®è¯åŒ¹é…
        for keyword in keywords:
            if keyword in answer_content:
                knowledge_points.append({
                    "name": keyword,
                    "relevance": 0.8,
                    "error_type": "concept_misunderstanding",
                    "extraction_method": "keyword_match"
                })
        
        # ç­–ç•¥ 2ï¼šæ¨¡å¼åŒ¹é…
        import re
        patterns = [
            r"æ¶‰åŠ[çŸ¥è¯†ç‚¹åˆ°äº†]?[:ï¼š]?([^ã€‚ï¼Œï¼Œã€\n]+)",
            r"è€ƒæŸ¥[çŸ¥è¯†ç‚¹åˆ°äº†]?[:ï¼š]?([^ã€‚ï¼Œï¼Œã€\n]+)",
            r"ä½¿ç”¨[çŸ¥è¯†ç‚¹åˆ°äº†]?[:ï¼š]?([^ã€‚ï¼Œï¼Œã€\n]+)",
            r"åº”ç”¨[çŸ¥è¯†ç‚¹åˆ°äº†]?[:ï¼š]?([^ã€‚ï¼Œï¼Œã€\n]+)",
        ]
        
        for pattern in patterns:
            matches = re.findall(pattern, answer_content)
            for match in matches:
                # æ¸…ç†æå–çš„æ–‡æœ¬
                kp_name = match.strip()
                if len(kp_name) > 2 and len(kp_name) < 20:  # è¿‡æ»¤å¤ªçŸ­æˆ–å¤ªé•¿çš„
                    knowledge_points.append({
                        "name": kp_name,
                        "relevance": 0.9,
                        "error_type": "concept_misunderstanding",
                        "extraction_method": "pattern_match"
                    })
        
        # å»é‡ï¼ˆæ ¹æ® name å­—æ®µï¼‰
        seen = set()
        unique_kps = []
        for kp in knowledge_points:
            if kp["name"] not in seen:
                seen.add(kp["name"])
                unique_kps.append(kp)
        
        return unique_kps[:5]  # æœ€å¤šè¿•å› 5 ä¸ªçŸ¥è¯†ç‚¹

    async def _trigger_knowledge_association(
        self,
        mistake_id: UUID,
        user_id: UUID,
        subject: str,
        ocr_text: Optional[str],
        ai_feedback: Dict[str, Any],
    ) -> None:
        """
        è§¦å‘çŸ¥è¯†å›¾è°±æœåŠ¡è¿›è¡ŒçŸ¥è¯†ç‚¹å…³è”
        """
        try:
            from src.services.knowledge_graph_service import KnowledgeGraphService
            
            kg_service = KnowledgeGraphService(self.db, self.bailian_service)
            
            # è°ƒç”¨çŸ¥è¯†å›¾è°±æœåŠ¡è¿›è¡Œå…³è”
            associations = await kg_service.analyze_and_associate_knowledge_points(
                mistake_id=mistake_id,
                user_id=user_id,
                subject=subject,
                ocr_text=ocr_text,
                ai_feedback=ai_feedback,
            )
            
            if associations:
                logger.info(
                    f"âœ… çŸ¥è¯†ç‚¹å…³è”æˆåŠŸ: mistake_id={mistake_id}, "
                    f"å…³è”æ•°é‡={len(associations)}"
                )
            else:
                logger.warning(f"âš ï¸ æœªèƒ½ä¸ºé”™é¢˜ {mistake_id} å…³è”çŸ¥è¯†ç‚¹")
                
        except Exception as e:
            logger.error(f"çŸ¥è¯†ç‚¹å…³è”å¤±è´¥: {e}", exc_info=True)
            raise

# ä¾èµ–æ³¨å…¥å‡½æ•°
def get_learning_service(db: AsyncSession) -> LearningService:
    """è·å–å­¦ä¹ é—®ç­”æœåŠ¡å®ä¾‹"""
    return LearningService(db)
