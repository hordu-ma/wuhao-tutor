"""
Phase 3.3: Prompt å‡†ç¡®æ€§æµ‹è¯•

æµ‹è¯• HOMEWORK_CORRECTION_PROMPT åœ¨ä¸åŒåœºæ™¯ä¸‹çš„å‡†ç¡®ç‡
é€šè¿‡ 5 ä¸ªå…¸å‹åœºæ™¯éªŒè¯æ‰¹æ”¹åŠŸèƒ½çš„æ­£ç¡®æ€§
"""

import json
from pathlib import Path
from typing import Any, Dict

import pytest

from src.services.learning_service import LearningService
from tests.fixtures.test_data_loader import (
    SCENARIO_ALL_CORRECT,
    SCENARIO_ALL_WRONG,
    SCENARIO_MIXED_TYPES,
    SCENARIO_PARTIAL_UNANSWERED,
    SCENARIO_SINGLE_QUESTION,
    load_test_case,
)


@pytest.mark.asyncio
class TestPromptAccuracy:
    """
    Prompt å‡†ç¡®æ€§æµ‹è¯•ç±»

    æµ‹è¯•ç›®æ ‡: éªŒè¯ AI æ‰¹æ”¹çš„å‡†ç¡®ç‡ â‰¥ 90%
    """

    async def test_single_question_correction(
        self, db_session, mock_bailian_service_for_integration
    ):
        """
        æµ‹è¯•åœºæ™¯1: å•é¢˜ä½œä¸šæ‰¹æ”¹

        éªŒè¯ç‚¹:
        - JSON è§£ææˆåŠŸ
        - é¢˜å·æ­£ç¡® (question_number = 1)
        - ç­”æ¡ˆåˆ¤æ–­å‡†ç¡® (æ­£ç¡®ç­”æ¡ˆä¸åº”æ ‡è®°ä¸ºé”™è¯¯)
        - çŸ¥è¯†ç‚¹æå–åˆç† (åº”åŒ…å«ç›¸å…³çŸ¥è¯†ç‚¹)
        - é”™è¯¯ç±»å‹ä¸º null (æ­£ç¡®ç­”æ¡ˆ)
        """
        # åŠ è½½æµ‹è¯•ç”¨ä¾‹
        test_case = load_test_case(SCENARIO_SINGLE_QUESTION)
        expected = test_case["expected_result"]

        # è®¾ç½® Mock å“åº”
        mock_response = self._build_mock_response(expected)
        mock_bailian_service_for_integration.set_response(json.dumps(mock_response))

        # åˆ›å»ºæœåŠ¡
        service = LearningService(db_session)
        service.bailian_service = mock_bailian_service_for_integration

        # è°ƒç”¨æ‰¹æ”¹æ–¹æ³•
        result = await service._call_ai_for_homework_correction(
            image_urls=test_case["image_urls"],
            subject=test_case["subject"],
            user_hint=test_case.get("user_hint", ""),
        )

        # éªŒè¯ç»“æœ
        assert result is not None, "æ‰¹æ”¹ç»“æœä¸åº”ä¸º None"
        assert (
            len(result.corrections) == expected["total_questions"]
        ), f"é¢˜ç›®æ•°é‡ä¸åŒ¹é…: æœŸæœ› {expected['total_questions']}, å®é™… {len(result.corrections)}"

        # éªŒè¯ç¬¬ä¸€é¢˜
        correction = result.corrections[0]
        expected_correction = expected["corrections"][0]

        assert (
            correction.question_number == expected_correction["question_number"]
        ), f"é¢˜å·ä¸åŒ¹é…: æœŸæœ› {expected_correction['question_number']}, å®é™… {correction.question_number}"

        assert (
            correction.is_unanswered == expected_correction["is_unanswered"]
        ), f"æœªä½œç­”æ ‡è®°ä¸åŒ¹é…: æœŸæœ› {expected_correction['is_unanswered']}, å®é™… {correction.is_unanswered}"

        assert (
            correction.error_type == expected_correction["error_type"]
        ), f"é”™è¯¯ç±»å‹ä¸åŒ¹é…: æœŸæœ› {expected_correction['error_type']}, å®é™… {correction.error_type}"

        assert (
            correction.score == expected_correction["score"]
        ), f"åˆ†æ•°ä¸åŒ¹é…: æœŸæœ› {expected_correction['score']}, å®é™… {correction.score}"

        # éªŒè¯çŸ¥è¯†ç‚¹æå–
        assert len(correction.knowledge_points) > 0, "åº”è¯¥æå–åˆ°çŸ¥è¯†ç‚¹"
        assert len(correction.knowledge_points) <= 3, "çŸ¥è¯†ç‚¹æ•°é‡ä¸åº”è¶…è¿‡3ä¸ª"

        print(f"âœ… åœºæ™¯1é€šè¿‡: å•é¢˜ä½œä¸šæ‰¹æ”¹å‡†ç¡®")

    async def test_all_wrong_correction(
        self, db_session, mock_bailian_service_for_integration
    ):
        """
        æµ‹è¯•åœºæ™¯2: å…¨é”™ä½œä¸šæ‰¹æ”¹

        éªŒè¯ç‚¹:
        - æ‰€æœ‰é¢˜ç›®éƒ½åº”æ ‡è®°ä¸ºé”™è¯¯
        - é”™è¯¯ç±»å‹åº”è¯¥ä¸åŒ (è®¡ç®—é”™è¯¯ã€æ¦‚å¿µé”™è¯¯ã€å•ä½é”™è¯¯)
        - error_count = 3
        - åˆ†æ•°éƒ½åº”è¯¥æ˜¯ 0
        """
        test_case = load_test_case(SCENARIO_ALL_WRONG)
        expected = test_case["expected_result"]

        # è®¾ç½® Mock å“åº”
        mock_response = self._build_mock_response(expected)
        mock_bailian_service_for_integration.set_response(json.dumps(mock_response))

        # åˆ›å»ºæœåŠ¡å¹¶è°ƒç”¨
        service = LearningService(db_session)
        service.bailian_service = mock_bailian_service_for_integration

        result = await service._call_ai_for_homework_correction(
            image_urls=test_case["image_urls"],
            subject=test_case["subject"],
            user_hint=test_case.get("user_hint", ""),
        )

        # éªŒè¯ç»“æœ
        assert result is not None
        assert len(result.corrections) == 3, "åº”è¯¥æœ‰3é“é¢˜"
        assert result.error_count == 3, f"é”™è¯¯æ•°é‡åº”ä¸º3, å®é™… {result.error_count}"

        # éªŒè¯æ¯é“é¢˜
        error_types = []
        for i, correction in enumerate(result.corrections):
            expected_correction = expected["corrections"][i]

            # éªŒè¯é¢˜å·è¿ç»­
            assert (
                correction.question_number == i + 1
            ), f"é¢˜å·åº”è¯¥æ˜¯ {i + 1}, å®é™… {correction.question_number}"

            # éªŒè¯é”™è¯¯æ ‡è®°
            assert correction.error_type is not None, f"ç¬¬ {i + 1} é¢˜åº”è¯¥æœ‰é”™è¯¯ç±»å‹"

            assert (
                correction.score == 0
            ), f"ç¬¬ {i + 1} é¢˜åˆ†æ•°åº”è¯¥æ˜¯0, å®é™… {correction.score}"

            error_types.append(correction.error_type)

        # éªŒè¯é”™è¯¯ç±»å‹å¤šæ ·æ€§
        unique_error_types = set(error_types)
        assert (
            len(unique_error_types) >= 2
        ), f"é”™è¯¯ç±»å‹åº”è¯¥æœ‰å¤šæ ·æ€§, å®é™…åªæœ‰ {len(unique_error_types)} ç§"

        print(f"âœ… åœºæ™¯2é€šè¿‡: å…¨é”™ä½œä¸šæ‰¹æ”¹å‡†ç¡®, é”™è¯¯ç±»å‹: {error_types}")

    async def test_all_correct_correction(
        self, db_session, mock_bailian_service_for_integration
    ):
        """
        æµ‹è¯•åœºæ™¯3: å…¨å¯¹ä½œä¸šæ‰¹æ”¹

        éªŒè¯ç‚¹:
        - æ‰€æœ‰é¢˜ç›®éƒ½åº”æ ‡è®°ä¸ºæ­£ç¡®
        - error_type éƒ½åº”è¯¥æ˜¯ null
        - error_count = 0
        - åˆ†æ•°éƒ½åº”è¯¥æ˜¯ 100
        """
        test_case = load_test_case(SCENARIO_ALL_CORRECT)
        expected = test_case["expected_result"]

        # è®¾ç½® Mock å“åº”
        mock_response = self._build_mock_response(expected)
        mock_bailian_service_for_integration.set_response(json.dumps(mock_response))

        # åˆ›å»ºæœåŠ¡å¹¶è°ƒç”¨
        service = LearningService(db_session)
        service.bailian_service = mock_bailian_service_for_integration

        result = await service._call_ai_for_homework_correction(
            image_urls=test_case["image_urls"],
            subject=test_case["subject"],
            user_hint=test_case.get("user_hint", ""),
        )

        # éªŒè¯ç»“æœ
        assert result is not None
        assert len(result.corrections) == 3, "åº”è¯¥æœ‰3é“é¢˜"
        assert result.error_count == 0, f"é”™è¯¯æ•°é‡åº”ä¸º0, å®é™… {result.error_count}"

        # éªŒè¯æ¯é“é¢˜
        for i, correction in enumerate(result.corrections):
            assert correction.question_number == i + 1
            assert (
                correction.error_type is None
            ), f"ç¬¬ {i + 1} é¢˜ä¸åº”è¯¥æœ‰é”™è¯¯ç±»å‹, å®é™… {correction.error_type}"
            assert (
                correction.score == 100
            ), f"ç¬¬ {i + 1} é¢˜åˆ†æ•°åº”è¯¥æ˜¯100, å®é™… {correction.score}"
            assert not correction.is_unanswered, f"ç¬¬ {i + 1} é¢˜ä¸åº”æ ‡è®°ä¸ºæœªä½œç­”"

        print(f"âœ… åœºæ™¯3é€šè¿‡: å…¨å¯¹ä½œä¸šæ‰¹æ”¹å‡†ç¡®")

    async def test_partial_unanswered_correction(
        self, db_session, mock_bailian_service_for_integration
    ):
        """
        æµ‹è¯•åœºæ™¯4: éƒ¨åˆ†æœªä½œç­”æ‰¹æ”¹

        éªŒè¯ç‚¹:
        - æœªä½œç­”é¢˜ç›® is_unanswered = true
        - æœªä½œç­”é¢˜ç›® student_answer = null
        - unanswered_count = 2
        - æœªä½œç­”é¢˜ç›® score = 0
        """
        test_case = load_test_case(SCENARIO_PARTIAL_UNANSWERED)
        expected = test_case["expected_result"]

        # è®¾ç½® Mock å“åº”
        mock_response = self._build_mock_response(expected)
        mock_bailian_service_for_integration.set_response(json.dumps(mock_response))

        # åˆ›å»ºæœåŠ¡å¹¶è°ƒç”¨
        service = LearningService(db_session)
        service.bailian_service = mock_bailian_service_for_integration

        result = await service._call_ai_for_homework_correction(
            image_urls=test_case["image_urls"],
            subject=test_case["subject"],
            user_hint=test_case.get("user_hint", ""),
        )

        # éªŒè¯ç»“æœ
        assert result is not None
        assert len(result.corrections) == 5, "åº”è¯¥æœ‰5é“é¢˜"
        assert (
            result.unanswered_count == 2
        ), f"æœªä½œç­”æ•°é‡åº”ä¸º2, å®é™… {result.unanswered_count}"

        # éªŒè¯æ¯é“é¢˜
        unanswered_count = 0
        for correction in result.corrections:
            if correction.is_unanswered:
                unanswered_count += 1
                assert (
                    correction.student_answer is None
                ), f"æœªä½œç­”é¢˜ç›®çš„å­¦ç”Ÿç­”æ¡ˆåº”ä¸º null, å®é™… {correction.student_answer}"
                assert (
                    correction.score == 0
                ), f"æœªä½œç­”é¢˜ç›®åˆ†æ•°åº”ä¸º0, å®é™… {correction.score}"

        assert (
            unanswered_count == 2
        ), f"ç»Ÿè®¡çš„æœªä½œç­”é¢˜æ•°ä¸unanswered_countä¸ä¸€è‡´: {unanswered_count} vs {result.unanswered_count}"

        print(f"âœ… åœºæ™¯4é€šè¿‡: éƒ¨åˆ†æœªä½œç­”æ‰¹æ”¹å‡†ç¡®")

    async def test_mixed_question_types(
        self, db_session, mock_bailian_service_for_integration
    ):
        """
        æµ‹è¯•åœºæ™¯5: æ··åˆé¢˜å‹æ‰¹æ”¹

        éªŒè¯ç‚¹:
        - æ­£ç¡®è¯†åˆ«ä¸åŒé¢˜å‹ (é€‰æ‹©é¢˜ã€å¡«ç©ºé¢˜ã€è§£ç­”é¢˜)
        - question_type å­—æ®µæ­£ç¡®
        - ä¸åŒé¢˜å‹çš„æ‰¹æ”¹é€»è¾‘æ­£ç¡®
        """
        test_case = load_test_case(SCENARIO_MIXED_TYPES)
        expected = test_case["expected_result"]

        # è®¾ç½® Mock å“åº”
        mock_response = self._build_mock_response(expected)
        mock_bailian_service_for_integration.set_response(json.dumps(mock_response))

        # åˆ›å»ºæœåŠ¡å¹¶è°ƒç”¨
        service = LearningService(db_session)
        service.bailian_service = mock_bailian_service_for_integration

        result = await service._call_ai_for_homework_correction(
            image_urls=test_case["image_urls"],
            subject=test_case["subject"],
            user_hint=test_case.get("user_hint", ""),
        )

        # éªŒè¯ç»“æœ
        assert result is not None
        assert len(result.corrections) == 3, "åº”è¯¥æœ‰3é“é¢˜"

        # éªŒè¯é¢˜å‹
        question_types = [c.question_type for c in result.corrections]
        expected_types = test_case["validation_rules"]["question_types_must_include"]

        for expected_type in expected_types:
            assert (
                expected_type in question_types
            ), f"åº”è¯¥åŒ…å«é¢˜å‹: {expected_type}, å®é™…é¢˜å‹: {question_types}"

        # éªŒè¯æ¯é“é¢˜
        for i, correction in enumerate(result.corrections):
            expected_correction = expected["corrections"][i]

            assert correction.question_type == expected_correction["question_type"], (
                f"ç¬¬ {i + 1} é¢˜é¢˜å‹ä¸åŒ¹é…: æœŸæœ› {expected_correction['question_type']}, "
                f"å®é™… {correction.question_type}"
            )

        print(f"âœ… åœºæ™¯5é€šè¿‡: æ··åˆé¢˜å‹æ‰¹æ”¹å‡†ç¡®, é¢˜å‹: {question_types}")

    def _build_mock_response(self, expected: Dict[str, Any]) -> Dict[str, Any]:
        """
        æ ¹æ®é¢„æœŸç»“æœæ„å»º Mock å“åº”

        Args:
            expected: é¢„æœŸç»“æœå­—å…¸

        Returns:
            Dict[str, Any]: Mock AI å“åº”
        """
        return {
            "corrections": expected["corrections"],
            "summary": expected.get("summary", "æ‰¹æ”¹å®Œæˆ"),
            "overall_score": expected.get("overall_score", 100),
            "total_questions": expected["total_questions"],
            "unanswered_count": expected.get("unanswered_count", 0),
            "error_count": expected.get("error_count", 0),
        }


@pytest.mark.asyncio
class TestPromptAccuracyStatistics:
    """
    Prompt å‡†ç¡®ç‡ç»Ÿè®¡æµ‹è¯•

    æ±‡æ€»æ‰€æœ‰åœºæ™¯çš„æµ‹è¯•ç»“æœï¼Œè®¡ç®—æ•´ä½“å‡†ç¡®ç‡
    """

    async def test_overall_accuracy(
        self, db_session, mock_bailian_service_for_integration
    ):
        """
        æµ‹è¯•æ•´ä½“å‡†ç¡®ç‡

        ç›®æ ‡: â‰¥ 90% çš„æ‰¹æ”¹å‡†ç¡®ç‡
        """
        # æ‰€æœ‰æµ‹è¯•åœºæ™¯
        test_scenarios = [
            SCENARIO_SINGLE_QUESTION,
            SCENARIO_ALL_WRONG,
            SCENARIO_ALL_CORRECT,
            SCENARIO_PARTIAL_UNANSWERED,
            SCENARIO_MIXED_TYPES,
        ]

        total_questions = 0
        correct_judgements = 0

        service = LearningService(db_session)
        service.bailian_service = mock_bailian_service_for_integration

        for scenario_file in test_scenarios:
            test_case = load_test_case(scenario_file)
            expected = test_case["expected_result"]

            # è®¾ç½® Mock å“åº”
            mock_response = {
                "corrections": expected["corrections"],
                "summary": expected.get("summary", ""),
                "overall_score": expected.get("overall_score", 100),
                "total_questions": expected["total_questions"],
                "unanswered_count": expected.get("unanswered_count", 0),
                "error_count": expected.get("error_count", 0),
            }
            mock_bailian_service_for_integration.set_response(json.dumps(mock_response))

            # è°ƒç”¨æ‰¹æ”¹
            result = await service._call_ai_for_homework_correction(
                image_urls=test_case["image_urls"],
                subject=test_case["subject"],
                user_hint=test_case.get("user_hint", ""),
            )

            if result is None:
                continue

            # ç»Ÿè®¡å‡†ç¡®ç‡
            for i, correction in enumerate(result.corrections):
                total_questions += 1
                expected_correction = expected["corrections"][i]

                # åˆ¤æ–­æ˜¯å¦å‡†ç¡®ï¼ˆç®€åŒ–ç‰ˆï¼Œå®é™…åº”æ›´ä¸¥æ ¼ï¼‰
                is_correct = (
                    correction.question_number == expected_correction["question_number"]
                    and correction.is_unanswered == expected_correction["is_unanswered"]
                    and correction.error_type == expected_correction["error_type"]
                )

                if is_correct:
                    correct_judgements += 1

        # è®¡ç®—å‡†ç¡®ç‡
        accuracy = (
            (correct_judgements / total_questions * 100) if total_questions > 0 else 0
        )

        print(f"\n{'='*60}")
        print(f"ğŸ“Š Prompt å‡†ç¡®æ€§ç»Ÿè®¡")
        print(f"{'='*60}")
        print(f"æ€»é¢˜æ•°: {total_questions}")
        print(f"æ­£ç¡®åˆ¤æ–­æ•°: {correct_judgements}")
        print(f"å‡†ç¡®ç‡: {accuracy:.2f}%")
        print(f"ç›®æ ‡å‡†ç¡®ç‡: â‰¥ 90%")
        print(f"{'='*60}")

        # æ–­è¨€å‡†ç¡®ç‡
        assert accuracy >= 90.0, f"å‡†ç¡®ç‡ {accuracy:.2f}% ä½äºç›®æ ‡ 90%ï¼Œéœ€è¦ä¼˜åŒ– Prompt"

        print(f"âœ… æ•´ä½“å‡†ç¡®ç‡æµ‹è¯•é€šè¿‡: {accuracy:.2f}% â‰¥ 90%")
