# å°ç¨‹åºä½œä¸šé—®ç­”é¡µé¢å®é™…ä½¿ç”¨çš„æç¤ºè¯

## ğŸ“ è°ƒç”¨é“¾è·¯

```
å°ç¨‹åºå‰ç«¯ â†’ åç«¯API â†’ Learning Service â†’ Bailian Service
```

### è¯¦ç»†è·¯å¾„

1. **å°ç¨‹åºé¡µé¢**: `miniprogram/pages/learning/index/index.js`

   - ç”¨æˆ·è¾“å…¥é—®é¢˜
   - è°ƒç”¨ `sendMessage()` æ–¹æ³•
   - å‘é€åˆ° `api.learning.askQuestion()`

2. **å‰ç«¯ API**: `miniprogram/api/learning.js`

   - æ–¹æ³•: `askQuestion()`
   - ç«¯ç‚¹: `POST /api/v1/learning/ask`

3. **åç«¯ç«¯ç‚¹**: `src/api/v1/endpoints/learning.py`

   - è·¯ç”±: `@router.post("/ask")`
   - è°ƒç”¨ `learning_service.ask_question()`

4. **ä¸šåŠ¡é€»è¾‘**: `src/services/learning_service.py`

   - æ–¹æ³•: `ask_question()` (ç¬¬ 84 è¡Œ)
   - æ„å»ºæ¶ˆæ¯: `_build_conversation_messages()` (ç¬¬ 472 è¡Œ)
   - **æ„å»ºæç¤ºè¯**: `_build_system_prompt()` (ç¬¬ 528 è¡Œ) â­

5. **AI æœåŠ¡**: `src/services/bailian_service.py`
   - è°ƒç”¨ç™¾ç‚¼ API
   - è‡ªåŠ¨é€‰æ‹©æ¨¡å‹ï¼ˆqwen-turbo / qwen-vl-maxï¼‰

---

## âœ… å®é™…ä½¿ç”¨çš„æç¤ºè¯

**ä½ç½®**: `src/services/learning_service.py:528`

### å®Œæ•´æç¤ºè¯

```python
async def _build_system_prompt(self, context: AIContext) -> str:
    """æ„å»ºç³»ç»Ÿæç¤ºè¯ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
    prompt_parts = [
        "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„K12å­¦ä¹ åŠ©æ•™ï¼Œåå«'äº”å¥½åŠ©æ•™'ï¼Œä¸“é—¨å¸®åŠ©å°åˆé«˜ä¸­å­¦ç”Ÿè§£å†³å­¦ä¹ é—®é¢˜ã€‚",
        "",
        "ä½ çš„èŒè´£åŒ…æ‹¬ï¼š",
        "1. åªèƒ½å›ç­”å­¦ä¹ é—®é¢˜ï¼Œæä¾›æ¸…æ™°æ˜“æ‡‚çš„è§£é‡Š",
        "2. åˆ†æé¢˜ç›®ï¼Œæä¾›è¯¦ç»†çš„è§£é¢˜æ­¥éª¤",
        "3. é¼“åŠ±å­¦ç”Ÿç§¯æå­¦ä¹ ï¼Œå»ºç«‹å­¦ä¹ ä¿¡å¿ƒ",
    ]

    # æ·»åŠ ç”¨æˆ·ä¸Šä¸‹æ–‡ï¼ˆä¸ªæ€§åŒ–ï¼‰
    if context.grade_level:
        grade_name = self._get_grade_name(context.grade_level)
        prompt_parts.append(f"\nå­¦ç”Ÿå½“å‰å­¦æ®µï¼š{grade_name}")

    if context.subject:
        subject_name = self._get_subject_name(context.subject)
        prompt_parts.append(f"å½“å‰å­¦ç§‘ï¼š{subject_name}")

    if context.metadata:
        if context.metadata.get("user_school"):
            prompt_parts.append(f"å­¦ç”Ÿå­¦æ ¡ï¼š{context.metadata['user_school']}")

        if context.metadata.get("weak_knowledge_points"):
            weak_points = context.metadata["weak_knowledge_points"][:3]
            if weak_points:
                point_names = []
                for point in weak_points:
                    if isinstance(point, dict):
                        point_names.append(point.get("knowledge_name", str(point)))
                    elif hasattr(point, "knowledge_name"):
                        point_names.append(point.knowledge_name)
                    else:
                        point_names.append(str(point))
                if point_names:
                    prompt_parts.append(f"å­¦ç”Ÿè–„å¼±çŸ¥è¯†ç‚¹ï¼š{', '.join(point_names)}")

    prompt_parts.append("\nè¯·åŸºäºä»¥ä¸Šä¿¡æ¯ï¼Œä¸ºå­¦ç”Ÿæä¾›ä¸ªæ€§åŒ–çš„å­¦ä¹ æŒ‡å¯¼ã€‚")

    return "\n".join(prompt_parts)
```

**è¯´æ˜**ï¼šå·²ç®€åŒ–ä¸ºåŸºç¡€ç‰ˆæœ¬ï¼Œæ›´å¤æ‚çš„æç¤ºè¯é…ç½®è¯·åœ¨ç™¾ç‚¼å¹³å°çš„æ™ºèƒ½ä½“"ç³»ç»ŸæŒ‡ä»¤"ä¸­è®¾ç½®ã€‚

---

## ğŸ¯ æç¤ºè¯ç‰¹ç‚¹ï¼ˆå·²ç®€åŒ–ï¼‰

### åŸºç¡€è§’è‰²è®¾å®š

- **åç§°**: äº”å¥½åŠ©æ•™
- **å®šä½**: K12 å­¦ä¹ åŠ©æ•™
- **æœåŠ¡å¯¹è±¡**: å°åˆé«˜ä¸­å­¦ç”Ÿ

### æ ¸å¿ƒèŒè´£ï¼ˆ3 é¡¹ - ç®€åŒ–ç‰ˆï¼‰

1. âœ… **åªèƒ½å›ç­”å­¦ä¹ é—®é¢˜**ï¼Œæä¾›æ¸…æ™°æ˜“æ‡‚çš„è§£é‡Š
2. âœ… åˆ†æé¢˜ç›®ï¼Œè¯¦ç»†æ­¥éª¤
3. âœ… é¼“åŠ±å­¦ç”Ÿå­¦ä¹ 

### ä¸ªæ€§åŒ–ä¸Šä¸‹æ–‡ï¼ˆåŠ¨æ€æ·»åŠ ï¼‰

- âœ… å­¦ç”Ÿå­¦æ®µï¼ˆå°å­¦ï½é«˜ä¸‰ï¼‰
- âœ… å½“å‰å­¦ç§‘
- âœ… å­¦ç”Ÿå­¦æ ¡ï¼ˆå¯é€‰ï¼‰
- âœ… è–„å¼±çŸ¥è¯†ç‚¹ï¼ˆæœ€å¤š 3 ä¸ªï¼‰

### âš ï¸ é‡è¦è¯´æ˜

**æ›´å¤æ‚çš„æç¤ºè¯ï¼ˆå¦‚å›ç­”è¦æ±‚ã€æ ¼å¼è§„èŒƒç­‰ï¼‰è¯·åœ¨ç™¾ç‚¼å¹³å°çš„æ™ºèƒ½ä½“"ç³»ç»ŸæŒ‡ä»¤"ä¸­é…ç½®**ï¼Œä¾‹å¦‚ï¼š

- ç”¨è¯­é£æ ¼è¦æ±‚
- Markdown æ ¼å¼è¦æ±‚
- æ•°å­¦å…¬å¼æ ¼å¼
- æ¨èé¢˜å‹è¦æ±‚
- ç­‰ç­‰...

---

## âš™ï¸ è°ƒç”¨å‚æ•°

```python
# learning_service.py:163
ai_response = await self.bailian_service.chat_completion(
    messages=message_dicts,      # åŒ…å« system_prompt + å†å²å¯¹è¯ + å½“å‰é—®é¢˜
    context=ai_context,           # ç”¨æˆ·ä¸Šä¸‹æ–‡ï¼ˆå¹´çº§ã€å­¦ç§‘ç­‰ï¼‰
    max_tokens=1500,              # æ¥è‡ªé…ç½® AI_MAX_TOKENS
    temperature=0.7,              # æ¥è‡ªé…ç½® AI_TEMPERATURE
    top_p=0.8,                    # æ¥è‡ªé…ç½® AI_TOP_P
)
```

### é…ç½®æ¥æº

- `src/core/config.py`
- ç¯å¢ƒå˜é‡: `.env`

---

## ğŸ”„ æ¶ˆæ¯æ„å»ºæµç¨‹

```python
# _build_conversation_messages() - ç¬¬472è¡Œ
messages = []

# 1. System Promptï¼ˆå§‹ç»ˆç¬¬ä¸€æ¡ï¼‰
system_prompt = await self._build_system_prompt(context)
messages.append(ChatMessage(role=MessageRole.SYSTEM, content=system_prompt))

# 2. å†å²å¯¹è¯ï¼ˆå¦‚æœ include_history=Trueï¼‰
if include_history and max_history > 0:
    history_messages = await self._get_conversation_history(session_id, max_history)
    messages.extend(history_messages)

# 3. å½“å‰ç”¨æˆ·é—®é¢˜
user_message_content = request.content
if request.image_urls:
    # å¦‚æœæœ‰å›¾ç‰‡ï¼Œæ„å»ºå¤šæ¨¡æ€å†…å®¹
    pass

messages.append(ChatMessage(
    role=MessageRole.USER,
    content=user_message_content,
    image_urls=request.image_urls
))

return messages
```

---

## ğŸ“Š å®é™…æ•ˆæœï¼ˆç®€åŒ–ç‰ˆï¼‰

### System Prompt ç¤ºä¾‹è¾“å‡º

```
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„K12å­¦ä¹ åŠ©æ•™ï¼Œåå«'äº”å¥½åŠ©æ•™'ï¼Œä¸“é—¨å¸®åŠ©å°åˆé«˜ä¸­å­¦ç”Ÿè§£å†³å­¦ä¹ é—®é¢˜ã€‚

ä½ çš„èŒè´£åŒ…æ‹¬ï¼š
1. åªèƒ½å›ç­”å­¦ä¹ é—®é¢˜ï¼Œæä¾›æ¸…æ™°æ˜“æ‡‚çš„è§£é‡Š
2. åˆ†æé¢˜ç›®ï¼Œæä¾›è¯¦ç»†çš„è§£é¢˜æ­¥éª¤
3. é¼“åŠ±å­¦ç”Ÿç§¯æå­¦ä¹ ï¼Œå»ºç«‹å­¦ä¹ ä¿¡å¿ƒ

å­¦ç”Ÿå½“å‰å­¦æ®µï¼šåˆäºŒ
å½“å‰å­¦ç§‘ï¼šæ•°å­¦
å­¦ç”Ÿè–„å¼±çŸ¥è¯†ç‚¹ï¼šäºŒæ¬¡å‡½æ•°, å‡½æ•°å›¾è±¡, ä¸€å…ƒäºŒæ¬¡æ–¹ç¨‹

è¯·åŸºäºä»¥ä¸Šä¿¡æ¯ï¼Œä¸ºå­¦ç”Ÿæä¾›ä¸ªæ€§åŒ–çš„å­¦ä¹ æŒ‡å¯¼ã€‚
```

**æ³¨æ„**ï¼šå›ç­”è¦æ±‚ã€æ ¼å¼è§„èŒƒç­‰è¯¦ç»†æŒ‡ä»¤è¯·åœ¨ç™¾ç‚¼å¹³å°é…ç½®ã€‚

---

## ğŸ¨ æ¨¡å‹è‡ªåŠ¨é€‰æ‹©

```python
# bailian_service.py:398
def _build_request_payload(messages, context, **kwargs):
    # æ£€æŸ¥æ˜¯å¦åŒ…å«å›¾ç‰‡
    has_images = self._has_images_in_messages(messages)

    if has_images:
        model = "qwen-vl-max"      # å¤šæ¨¡æ€æ¨¡å‹ï¼ˆå›¾ç‰‡è¯†åˆ«ï¼‰
    else:
        model = "qwen-turbo"       # çº¯æ–‡æœ¬æ¨¡å‹

    return payload
```

### æ¨¡å‹é€‰æ‹©é€»è¾‘

- âœ… **æœ‰å›¾ç‰‡**: è‡ªåŠ¨åˆ‡æ¢åˆ° `qwen-vl-max`
- âœ… **çº¯æ–‡æœ¬**: ä½¿ç”¨ `qwen-turbo`
- âœ… **é€æ˜åˆ‡æ¢**: å‰ç«¯æ— éœ€å…³å¿ƒ

---

## ğŸ” ä¸å…¶ä»–åœºæ™¯çš„åŒºåˆ«

| åœºæ™¯         | æç¤ºè¯ä½ç½®                      | è§’è‰²     | Temperature | è¾“å‡ºæ ¼å¼ |
| ------------ | ------------------------------- | -------- | ----------- | -------- |
| **ä½œä¸šé—®ç­”** | `learning_service.py:528`       | äº”å¥½åŠ©æ•™ | 0.7         | Markdown |
| é”™é¢˜åˆ†æ     | `mistake_service.py:628`        | å­¦ç§‘æ•™å¸ˆ | 0.7         | JSON     |
| è´¨é‡è¯„ä¼°     | `answer_quality_service.py:259` | æ•™è‚²ä¸“å®¶ | 0.3         | JSON     |
| çŸ¥è¯†æå–     | `extraction_service.py:215`     | æ— è§’è‰²   | 0.3         | åˆ—è¡¨     |

---

## âœ… æ€»ç»“

**å°ç¨‹åºä½œä¸šé—®ç­”é¡µé¢å®é™…ä½¿ç”¨çš„æ˜¯**ï¼š

ğŸ“ **æ–‡ä»¶**: `src/services/learning_service.py`  
ğŸ“ **æ–¹æ³•**: `_build_system_prompt()` (ç¬¬ 528 è¡Œ)  
ğŸ“ **è§’è‰²**: äº”å¥½åŠ©æ•™ï¼ˆK12 å­¦ä¹ åŠ©æ‰‹ï¼‰  
ğŸ“ **å‚æ•°**: Temperature 0.7, Max Tokens 1500  
ğŸ“ **æ¨¡å‹**: qwen-turboï¼ˆæ–‡æœ¬ï¼‰/ qwen-vl-maxï¼ˆå›¾ç‰‡ï¼‰  
ğŸ“ **ç‰¹ç‚¹**: æ”¯æŒä¸ªæ€§åŒ–ä¸Šä¸‹æ–‡ã€å¤šæ¨¡æ€è¾“å…¥ã€å†å²å¯¹è¯

---

**è¯¦ç»†æ–‡æ¡£**: `docs/operations/prompt-settings-summary.md`
