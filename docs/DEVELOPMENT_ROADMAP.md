# äº”å¥½ä¼´å­¦ - å¼€å‘è·¯çº¿å›¾ (RAG åç½®ç­–ç•¥)

> **åˆ¶å®šæ—¶é—´**: 2025-10-05  
> **ç­–ç•¥**: RAG ç³»ç»Ÿåç½®å¼€å‘ï¼Œä¼˜å…ˆäº¤ä»˜å¿«é€Ÿä»·å€¼  
> **æ€»å·¥æ—¶**: çº¦ 176 å°æ—¶ (22 å·¥ä½œæ—¥ï¼ŒæŒ‰æ¯å¤© 8 å°æ—¶è®¡)

---

## ğŸ“Š æ€»è§ˆ

### å¼€å‘æ‰¹æ¬¡è§„åˆ’

```mermaid
gantt
    title äº”å¥½ä¼´å­¦å¼€å‘æ—¶é—´çº¿ (RAGåç½®ç­–ç•¥)
    dateFormat  YYYY-MM-DD
    section ç¬¬ä¸€æ‰¹
    çŸ¥è¯†ç‚¹æå–ä¼˜åŒ–           :a1, 2025-10-06, 3d
    çŸ¥è¯†å›¾è°±æ•°æ®å¯¼å…¥         :a2, after a1, 2d
    ç­”æ¡ˆè´¨é‡è¯„ä¼°             :a3, after a2, 1d

    section ç¬¬äºŒæ‰¹
    æµå¼å“åº”å®ç°             :b1, after a3, 2d
    è¯·æ±‚ç¼“å­˜æœºåˆ¶             :b2, after b1, 1d
    é”™é¢˜æœ¬åŠŸèƒ½               :b3, after b2, 2d
    å­¦æƒ…åˆ†æç®—æ³•ä¼˜åŒ–         :b4, after b3, 2d

    section ç¬¬ä¸‰æ‰¹
    å‘é‡æ•°æ®åº“é›†æˆ           :c1, after b4, 2d
    EmbeddingæœåŠ¡å¯¹æ¥        :c2, after c1, 1d
    çŸ¥è¯†ç‰‡æ®µç®¡ç†             :c3, after c2, 2d
    æ£€ç´¢ç­–ç•¥å®ç°             :c4, after c3, 2d
    RAGè”è°ƒæµ‹è¯•              :c5, after c4, 1d

    section ç¬¬å››æ‰¹
    RAGå¢å¼ºä¼˜åŒ–              :d1, after c5, 2d
```

### ä»·å€¼äº¤ä»˜æ—¶é—´ç‚¹

| æ—¶é—´èŠ‚ç‚¹   | å¯äº¤ä»˜ä»·å€¼                    | ç”¨æˆ·å¯æ„ŸçŸ¥æå‡               |
| ---------- | ----------------------------- | ---------------------------- |
| **Week 3** | çŸ¥è¯†ç‚¹å‡†ç¡®æå– + çŸ¥è¯†å›¾è°±æ•°æ® | å­¦æƒ…åˆ†ææ›´å‡†ç¡®ï¼Œæ¨èæ›´ä¸ªæ€§åŒ– |
| **Week 5** | æµå¼å“åº” + é”™é¢˜æœ¬             | ç­‰å¾…ä½“éªŒä¼˜åŒ–ï¼Œå­¦ä¹ é—­ç¯å®Œæ•´   |
| **Week 9** | RAG ç³»ç»Ÿä¸Šçº¿                  | é—®ç­”è´¨é‡æ˜¾è‘—æå‡ï¼ŒçœŸæ­£ä¸ªæ€§åŒ– |

---

## ğŸ“‹ ç¬¬ä¸€æ‰¹ï¼šå¿«é€Ÿä»·å€¼äº¤ä»˜ (Week 1-3)

### ğŸ¯ ç›®æ ‡

- æ”¹å–„æ•°æ®è´¨é‡ (çŸ¥è¯†ç‚¹æå–)
- æ¿€æ´»ç°æœ‰åŠŸèƒ½ (çŸ¥è¯†å›¾è°±)
- æå‡å¯ä¿¡åº¦ (ç­”æ¡ˆè¯„ä¼°)

### ä»»åŠ¡æ¸…å•

#### 1ï¸âƒ£ çŸ¥è¯†ç‚¹æå–ä¼˜åŒ– (TD-002)

**é¢„ä¼°å·¥æ—¶**: 24 å°æ—¶ (3 å¤©)  
**ä¼˜å…ˆçº§**: ğŸ”¥ğŸ”¥ğŸ”¥ æœ€é«˜  
**ä¾èµ–**: æ— 

**å®ç°æ–¹æ¡ˆ**:

```python
# src/services/knowledge_extraction_service.py

from typing import List, Dict
import jieba
import jieba.posseg as pseg

class KnowledgeExtractionService:
    """çŸ¥è¯†ç‚¹æå–æœåŠ¡"""

    def __init__(self):
        # åŠ è½½å­¦ç§‘çŸ¥è¯†ç‚¹è¯å…¸
        self._load_knowledge_dict()

    async def extract_from_homework(
        self,
        content: str,
        subject: str
    ) -> List[Dict[str, any]]:
        """
        ä»ä½œä¸šå†…å®¹æå–çŸ¥è¯†ç‚¹

        Returns:
            [
                {
                    "name": "äºŒæ¬¡å‡½æ•°",
                    "confidence": 0.95,
                    "matched_keywords": ["æŠ›ç‰©çº¿", "é¡¶ç‚¹"],
                    "context": "...åŸæ–‡ç‰‡æ®µ..."
                }
            ]
        """
        # æ–¹æ¡ˆ A: åŸºäºè¯å…¸çš„å®ä½“è¯†åˆ« (å¿«é€Ÿ)
        rule_based = self._rule_based_extraction(content, subject)

        # æ–¹æ¡ˆ B: è°ƒç”¨ç™¾ç‚¼ API è¿›è¡Œè¯­ä¹‰æå– (å‡†ç¡®)
        ai_based = await self._ai_extraction(content, subject)

        # æ–¹æ¡ˆ C: èåˆç»“æœå¹¶å»é‡
        merged = self._merge_results(rule_based, ai_based)

        return merged

    def _rule_based_extraction(self, content: str, subject: str) -> List[Dict]:
        """åŸºäºè§„åˆ™çš„æå–"""
        knowledge_points = []

        # 1. åˆ†è¯
        words = pseg.cut(content)

        # 2. åŒ¹é…çŸ¥è¯†ç‚¹è¯å…¸
        for word, flag in words:
            if self._is_knowledge_point(word, subject):
                knowledge_points.append({
                    "name": word,
                    "confidence": 0.8,
                    "method": "rule"
                })

        return knowledge_points

    async def _ai_extraction(self, content: str, subject: str) -> List[Dict]:
        """åŸºäº AI çš„æå–"""
        prompt = f"""
        è¯·ä»ä»¥ä¸‹{subject}é¢˜ç›®ä¸­æå–æ¶‰åŠçš„çŸ¥è¯†ç‚¹ï¼ŒæŒ‰é‡è¦æ€§æ’åºï¼š

        é¢˜ç›®: {content}

        è¿”å› JSON æ ¼å¼:
        [
            {{"name": "çŸ¥è¯†ç‚¹åç§°", "confidence": 0.95, "reason": "è¯†åˆ«ä¾æ®"}}
        ]
        """

        response = await self.bailian_service.chat_completion(
            messages=[{"role": "user", "content": prompt}],
            temperature=0.3  # ä½æ¸©åº¦ï¼Œæ›´ç¡®å®šçš„è¾“å‡º
        )

        # è§£æ JSON å“åº”
        return self._parse_ai_response(response.content)
```

**æ•°æ®å‡†å¤‡**:

```python
# data/knowledge_dict/math_grade_9.json
{
    "äºŒæ¬¡å‡½æ•°": {
        "keywords": ["æŠ›ç‰©çº¿", "é¡¶ç‚¹", "å¯¹ç§°è½´", "å¼€å£", "æœ€å€¼"],
        "related": ["ä¸€æ¬¡å‡½æ•°", "å‡½æ•°å›¾è±¡"],
        "difficulty": 3
    },
    "åœ†": {
        "keywords": ["åœ†å¿ƒ", "åŠå¾„", "ç›´å¾„", "åœ†å‘¨è§’", "åˆ‡çº¿"],
        "related": ["ä¸‰è§’å½¢", "ç›¸ä¼¼"],
        "difficulty": 4
    }
}
```

**éªŒæ”¶æ ‡å‡†**:

- âœ… çŸ¥è¯†ç‚¹æå–å‡†ç¡®ç‡ > 80% (äººå·¥æ ‡æ³¨ 100 é“é¢˜æµ‹è¯•)
- âœ… å¹³å‡æå–æ—¶é—´ < 500ms
- âœ… æ”¯æŒæ•°å­¦/è¯­æ–‡/è‹±è¯­ä¸‰ç§‘
- âœ… ç½®ä¿¡åº¦è¯„åˆ†æœºåˆ¶å®Œå–„

---

#### 2ï¸âƒ£ çŸ¥è¯†å›¾è°±æ•°æ®å¯¼å…¥ (TD-003)

**é¢„ä¼°å·¥æ—¶**: 16 å°æ—¶ (2 å¤©)  
**ä¼˜å…ˆçº§**: ğŸ”¥ğŸ”¥ é«˜  
**ä¾èµ–**: çŸ¥è¯†ç‚¹æå–ä¼˜åŒ– (éƒ¨åˆ†ä¾èµ–)

**æ•°æ®æ¥æº**:

1. **äººæ•™ç‰ˆæ•™æç›®å½•** (å®˜æ–¹æƒå¨)

   - è·å–æ–¹å¼: çˆ¬å–äººæ°‘æ•™è‚²å‡ºç‰ˆç¤¾å®˜ç½‘
   - è¦†ç›–èŒƒå›´: å°å­¦-é«˜ä¸­ï¼Œå„å­¦ç§‘

2. **æ•™è‚²éƒ¨è¯¾ç¨‹æ ‡å‡†** (æ ‡å‡†å‚è€ƒ)

   - è·å–æ–¹å¼: æ•™è‚²éƒ¨å®˜ç½‘ PDF æ–‡æ¡£
   - å†…å®¹: çŸ¥è¯†ç‚¹åˆ†çº§ã€èƒ½åŠ›è¦æ±‚

3. **å¼€æºçŸ¥è¯†å›¾è°±** (è¡¥å……æ•°æ®)
   - GitHub: K12-Knowledge-Graph
   - æ ¼å¼: JSON/CSV

**å¯¼å…¥è®¡åˆ’**:

```bash
# Week 2: æ•°å­¦å­¦ç§‘ (ä¼˜å…ˆçº§æœ€é«˜)
scripts/import_knowledge.py --subject math --grade all
# é¢„è®¡å¯¼å…¥: 500+ çŸ¥è¯†ç‚¹, 1000+ å…³è”å…³ç³»

# Week 3: è¯­æ–‡å­¦ç§‘
scripts/import_knowledge.py --subject chinese --grade all
# é¢„è®¡å¯¼å…¥: 300+ çŸ¥è¯†ç‚¹ (å¤è¯—æ–‡ã€é˜…è¯»æŠ€å·§ã€å†™ä½œæ–¹æ³•)

# Week 3: è‹±è¯­å­¦ç§‘
scripts/import_knowledge.py --subject english --grade all
# é¢„è®¡å¯¼å…¥: 200+ çŸ¥è¯†ç‚¹ (è¯­æ³•ã€è¯æ±‡ã€é˜…è¯»)
```

**è„šæœ¬å®ç°**:

```python
# scripts/import_knowledge.py

import asyncio
from pathlib import Path
import json
from src.core.database import get_session
from src.models.knowledge import KnowledgeNode, KnowledgeRelation

async def import_math_knowledge():
    """å¯¼å…¥æ•°å­¦å­¦ç§‘çŸ¥è¯†å›¾è°±"""

    # è¯»å–æ•°æ®æ–‡ä»¶
    data_file = Path("data/knowledge/math_knowledge_graph.json")
    with open(data_file) as f:
        data = json.load(f)

    async with get_session() as session:
        # 1. å¯¼å…¥çŸ¥è¯†èŠ‚ç‚¹
        for node_data in data["nodes"]:
            node = KnowledgeNode(
                name=node_data["name"],
                code=node_data["code"],
                node_type=node_data["type"],  # subject/chapter/section/concept
                subject="æ•°å­¦",
                level=node_data["level"],
                parent_id=node_data.get("parent_id"),
                difficulty=node_data.get("difficulty", 3),
                importance=node_data.get("importance", 3),
                keywords=node_data.get("keywords", []),
                description=node_data.get("description")
            )
            session.add(node)

        # 2. å¯¼å…¥çŸ¥è¯†å…³ç³»
        for rel_data in data["relations"]:
            relation = KnowledgeRelation(
                from_node_id=rel_data["from"],
                to_node_id=rel_data["to"],
                relation_type=rel_data["type"],  # prerequisite/contains/similar
                strength=rel_data.get("strength", 0.8)
            )
            session.add(relation)

        await session.commit()
        print(f"âœ… å¯¼å…¥å®Œæˆ: {len(data['nodes'])} ä¸ªçŸ¥è¯†ç‚¹, {len(data['relations'])} ä¸ªå…³ç³»")

if __name__ == "__main__":
    asyncio.run(import_math_knowledge())
```

**éªŒæ”¶æ ‡å‡†**:

- âœ… æ•°å­¦çŸ¥è¯†ç‚¹ > 500 ä¸ª
- âœ… çŸ¥è¯†å…³ç³» > 1000 ä¸ª
- âœ… å­¦ä¹ è·¯å¾„æ¨¡æ¿ > 10 ä¸ª (å¦‚"åˆä¸­æ•°å­¦å‡½æ•°ä¸“é¢˜")
- âœ… æ•°æ®å¯è§†åŒ–éªŒè¯ (Neo4j Browser æˆ–è‡ªå®šä¹‰å‰ç«¯)

---

#### 3ï¸âƒ£ ç­”æ¡ˆè´¨é‡è¯„ä¼°æœºåˆ¶ (TD-005)

**é¢„ä¼°å·¥æ—¶**: 8 å°æ—¶ (1 å¤©)  
**ä¼˜å…ˆçº§**: ğŸ”¥ ä¸­é«˜  
**ä¾èµ–**: æ— 

**å®ç°æ–¹æ¡ˆ**:

```python
# src/services/quality_service.py

from typing import Dict
from pydantic import BaseModel

class AnswerQualityMetrics(BaseModel):
    """ç­”æ¡ˆè´¨é‡æŒ‡æ ‡"""
    accuracy: float        # å‡†ç¡®æ€§ (0-1)
    completeness: float    # å®Œæ•´æ€§ (0-1)
    age_appropriate: float # é€‚é¾„æ€§ (0-1)
    clarity: float         # æ¸…æ™°åº¦ (0-1)
    overall: float         # ç»¼åˆè¯„åˆ† (0-1)
    feedback: str          # æ–‡å­—åé¦ˆ

class AnswerQualityService:
    """ç­”æ¡ˆè´¨é‡è¯„ä¼°æœåŠ¡"""

    async def evaluate_answer(
        self,
        question: str,
        answer: str,
        student_grade: str,
        subject: str
    ) -> AnswerQualityMetrics:
        """è¯„ä¼°ç­”æ¡ˆè´¨é‡"""

        # 1. AI è‡ªåŠ¨è¯„ä¼°
        ai_metrics = await self._ai_evaluation(question, answer, student_grade)

        # 2. è§„åˆ™è¯„ä¼° (è¾…åŠ©)
        rule_metrics = self._rule_evaluation(answer, student_grade)

        # 3. èåˆç»“æœ
        final_metrics = self._merge_metrics(ai_metrics, rule_metrics)

        return final_metrics

    async def _ai_evaluation(self, question, answer, grade) -> Dict:
        """AI è¯„ä¼°"""
        prompt = f"""
        è¯·è¯„ä¼°ä»¥ä¸‹ç­”æ¡ˆçš„è´¨é‡ (å­¦ç”Ÿå¹´çº§: {grade}):

        é—®é¢˜: {question}
        ç­”æ¡ˆ: {answer}

        è¯„ä¼°ç»´åº¦ (0-1åˆ†):
        1. å‡†ç¡®æ€§: ç­”æ¡ˆæ˜¯å¦æ­£ç¡®?
        2. å®Œæ•´æ€§: æ˜¯å¦æ¶µç›–å…³é”®ç‚¹?
        3. é€‚é¾„æ€§: æ˜¯å¦ç¬¦åˆå­¦ç”Ÿè®¤çŸ¥æ°´å¹³?
        4. æ¸…æ™°åº¦: æ˜¯å¦æ˜“äºç†è§£?

        è¿”å› JSON: {{"accuracy": 0.9, "completeness": 0.85, ...}}
        """

        response = await self.bailian_service.chat_completion(...)
        return self._parse_metrics(response.content)

    def _rule_evaluation(self, answer: str, grade: str) -> Dict:
        """è§„åˆ™è¯„ä¼°"""
        metrics = {}

        # é•¿åº¦æ£€æŸ¥
        if len(answer) < 50:
            metrics["completeness"] = 0.5
        elif len(answer) > 1000:
            metrics["completeness"] = 0.9

        # æœ¯è¯­æ£€æŸ¥ (é€‚é¾„æ€§)
        complex_terms = ["å¾®åˆ†", "ç§¯åˆ†", "æé™"]  # é«˜ä¸­æœ¯è¯­
        if grade in ["å°å­¦", "åˆä¸­"] and any(term in answer for term in complex_terms):
            metrics["age_appropriate"] = 0.3

        return metrics
```

**éªŒæ”¶æ ‡å‡†**:

- âœ… è¯„ä¼°ç»´åº¦å®Œæ•´ (4 ç»´åº¦ + ç»¼åˆè¯„åˆ†)
- âœ… è¯„ä¼°æ—¶é—´ < 2s
- âœ… æ”¯æŒäººå·¥åé¦ˆä¿®æ­£
- âœ… è´¨é‡æ•°æ®å¯è¿½è¸ª

---

## ğŸ“‹ ç¬¬äºŒæ‰¹ï¼šä½“éªŒä¼˜åŒ– (Week 4-5)

### ğŸ¯ ç›®æ ‡

- æå‡äº¤äº’ä½“éªŒ (æµå¼å“åº”)
- é™ä½æœåŠ¡æˆæœ¬ (è¯·æ±‚ç¼“å­˜)
- å®Œå–„å­¦ä¹ é—­ç¯ (é”™é¢˜æœ¬)

### ä»»åŠ¡æ¸…å•

#### 4ï¸âƒ£ æµå¼å“åº”å®ç° (TD-006)

**é¢„ä¼°å·¥æ—¶**: 12 å°æ—¶ (1.5 å¤©)  
**ä¼˜å…ˆçº§**: ğŸ”¥ğŸ”¥ é«˜  
**ä¾èµ–**: æ— 

**åç«¯å®ç°** (FastAPI SSE):

```python
# src/api/v1/endpoints/learning.py

from fastapi import APIRouter
from fastapi.responses import StreamingResponse
import asyncio
import json

@router.post("/ask/stream")
async def ask_question_stream(request: AskQuestionRequest):
    """æµå¼é—®ç­”æ¥å£"""

    async def event_generator():
        """SSE äº‹ä»¶ç”Ÿæˆå™¨"""
        try:
            # 1. å‘é€å¼€å§‹äº‹ä»¶
            yield f"data: {json.dumps({'type': 'start', 'session_id': session_id})}\n\n"

            # 2. æµå¼è°ƒç”¨ç™¾ç‚¼ API
            async for chunk in bailian_service.chat_stream(messages):
                event_data = {
                    "type": "chunk",
                    "content": chunk.content,
                    "finish_reason": chunk.finish_reason
                }
                yield f"data: {json.dumps(event_data)}\n\n"
                await asyncio.sleep(0.01)  # æ¨¡æ‹Ÿæ‰“å­—é€Ÿåº¦

            # 3. å‘é€å®Œæˆäº‹ä»¶
            yield f"data: {json.dumps({'type': 'done'})}\n\n"

        except Exception as e:
            yield f"data: {json.dumps({'type': 'error', 'message': str(e)})}\n\n"

    return StreamingResponse(
        event_generator(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive"
        }
    )
```

**å‰ç«¯å®ç°** (EventSource):

```typescript
// frontend/src/composables/useStreamingChat.ts

export function useStreamingChat() {
  const answer = ref('')
  const isStreaming = ref(false)

  const askStream = async (question: string) => {
    isStreaming.value = true
    answer.value = ''

    const eventSource = new EventSource(
      `/api/v1/learning/ask/stream?question=${encodeURIComponent(question)}`
    )

    eventSource.onmessage = (e) => {
      const data = JSON.parse(e.data)

      switch (data.type) {
        case 'start':
          console.log('å¼€å§‹æ¥æ”¶...')
          break
        case 'chunk':
          answer.value += data.content // æ‰“å­—æœºæ•ˆæœ
          break
        case 'done':
          isStreaming.value = false
          eventSource.close()
          break
        case 'error':
          ElMessage.error(data.message)
          eventSource.close()
          break
      }
    }

    eventSource.onerror = () => {
      isStreaming.value = false
      eventSource.close()
    }
  }

  return { answer, isStreaming, askStream }
}
```

**éªŒæ”¶æ ‡å‡†**:

- âœ… æµå¼è¾“å‡ºå»¶è¿Ÿ < 200ms
- âœ… æ”¯æŒä¸­æ–­æ“ä½œ
- âœ… é”™è¯¯æ¢å¤æœºåˆ¶
- âœ… æ‰“å­—æœºæ•ˆæœæµç•…

---

#### 5ï¸âƒ£ è¯·æ±‚ç¼“å­˜æœºåˆ¶ (TD-007)

**é¢„ä¼°å·¥æ—¶**: 8 å°æ—¶ (1 å¤©)  
**ä¼˜å…ˆçº§**: ğŸ”¥ ä¸­  
**ä¾èµ–**: æ— 

**ç¼“å­˜ç­–ç•¥**:

```python
# src/core/cache.py

import hashlib
from typing import Optional
from redis import Redis

class QuestionCache:
    """é—®é¢˜ç­”æ¡ˆç¼“å­˜"""

    def __init__(self, redis_client: Redis):
        self.redis = redis_client
        self.ttl = 3600  # 1å°æ—¶

    def generate_cache_key(
        self,
        question: str,
        user_id: str,
        context: Optional[str] = None
    ) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        # åŒ…å«é—®é¢˜å†…å®¹ + ç”¨æˆ·ID + ä¸Šä¸‹æ–‡æ‘˜è¦
        cache_input = f"{question}:{user_id}:{context or ''}"
        return f"qa:{hashlib.md5(cache_input.encode()).hexdigest()}"

    async def get_cached_answer(self, cache_key: str) -> Optional[str]:
        """è·å–ç¼“å­˜ç­”æ¡ˆ"""
        return await self.redis.get(cache_key)

    async def set_cached_answer(self, cache_key: str, answer: str):
        """è®¾ç½®ç¼“å­˜ç­”æ¡ˆ"""
        await self.redis.setex(cache_key, self.ttl, answer)

    async def find_similar_cached(
        self,
        question: str,
        threshold: float = 0.85
    ) -> Optional[str]:
        """
        æŸ¥æ‰¾ç›¸ä¼¼é—®é¢˜çš„ç¼“å­˜

        ç­–ç•¥:
        1. ç¼–è¾‘è·ç¦»ç›¸ä¼¼åº¦
        2. å…³é”®è¯åŒ¹é…åº¦
        """
        # å®ç°ç›¸ä¼¼åº¦æ£€ç´¢é€»è¾‘
        pass
```

**éªŒæ”¶æ ‡å‡†**:

- âœ… ç¼“å­˜å‘½ä¸­ç‡ > 20%
- âœ… ç¼“å­˜æŸ¥è¯¢æ—¶é—´ < 10ms
- âœ… æ”¯æŒç›¸ä¼¼é—®é¢˜åŒ¹é…
- âœ… ç¼“å­˜å¤±æ•ˆç­–ç•¥åˆç†

---

#### 6ï¸âƒ£ é”™é¢˜æœ¬åŠŸèƒ½ (TD-008)

**é¢„ä¼°å·¥æ—¶**: 16 å°æ—¶ (2 å¤©)  
**ä¼˜å…ˆçº§**: ğŸ”¥ğŸ”¥ é«˜  
**ä¾èµ–**: çŸ¥è¯†ç‚¹æå–ä¼˜åŒ–

**æ•°æ®æ¨¡å‹**:

```python
# src/models/mistake.py

from sqlalchemy import Column, String, Integer, JSON, ForeignKey
from src.models.base import BaseModel

class MistakeRecord(BaseModel):
    """é”™é¢˜è®°å½•"""
    __tablename__ = "mistake_records"

    user_id = Column(String(50), ForeignKey("users.id"), nullable=False)
    homework_id = Column(String(50), ForeignKey("homework_submissions.id"))
    question_id = Column(String(50), ForeignKey("questions.id"))

    subject = Column(String(20), nullable=False)
    grade = Column(String(20))

    question_content = Column(String(2000), nullable=False)
    correct_answer = Column(String(1000))
    user_answer = Column(String(1000))

    knowledge_points = Column(JSON)  # ["äºŒæ¬¡å‡½æ•°", "å‡½æ•°å›¾è±¡"]
    difficulty = Column(Integer, default=3)

    mistake_type = Column(String(50))  # careless/concept/method
    review_count = Column(Integer, default=0)
    mastered = Column(Boolean, default=False)

    next_review_at = Column(DateTime)  # è‰¾å®¾æµ©æ–¯å¤ä¹ æ—¶é—´
```

**å¤ä¹ ç®—æ³•** (è‰¾å®¾æµ©æ–¯é—å¿˜æ›²çº¿):

```python
# src/utils/review_scheduler.py

from datetime import datetime, timedelta

class ReviewScheduler:
    """å¤ä¹ æ—¶é—´è°ƒåº¦å™¨"""

    # è‰¾å®¾æµ©æ–¯å¤ä¹ é—´éš” (å¤©)
    INTERVALS = [1, 2, 4, 7, 15, 30, 60]

    def calculate_next_review(
        self,
        last_review: datetime,
        review_count: int,
        mastered: bool
    ) -> datetime:
        """è®¡ç®—ä¸‹æ¬¡å¤ä¹ æ—¶é—´"""
        if mastered:
            return None  # å·²æŒæ¡ï¼Œæ— éœ€å¤ä¹ 

        interval_days = self.INTERVALS[min(review_count, len(self.INTERVALS) - 1)]
        return last_review + timedelta(days=interval_days)
```

**å‰ç«¯é¡µé¢**:

```vue
<!-- frontend/src/views/MistakeBook.vue -->
<template>
  <div class="mistake-book">
    <el-tabs v-model="activeTab">
      <el-tab-pane label="å¾…å¤ä¹ " name="pending">
        <MistakeList :mistakes="pendingMistakes" />
      </el-tab-pane>
      <el-tab-pane label="å·²æŒæ¡" name="mastered">
        <MistakeList :mistakes="masteredMistakes" />
      </el-tab-pane>
      <el-tab-pane label="çŸ¥è¯†ç‚¹åˆ†æ" name="analysis">
        <KnowledgeAnalysis :data="knowledgeStats" />
      </el-tab-pane>
    </el-tabs>
  </div>
</template>
```

**éªŒæ”¶æ ‡å‡†**:

- âœ… é”™é¢˜è‡ªåŠ¨æ”¶é›† (ä½œä¸šæ‰¹æ”¹å)
- âœ… å¤ä¹ æé†’æ¨é€
- âœ… çŸ¥è¯†ç‚¹ç»Ÿè®¡åˆ†æ
- âœ… å¯¼å‡ºåŠŸèƒ½ (PDF/Word)

---

#### 7ï¸âƒ£ å­¦æƒ…åˆ†æç®—æ³•ä¼˜åŒ– (åŸºç¡€ç‰ˆ)

**é¢„ä¼°å·¥æ—¶**: 16 å°æ—¶ (2 å¤©)  
**ä¼˜å…ˆçº§**: ğŸ”¥ ä¸­é«˜  
**ä¾èµ–**: çŸ¥è¯†ç‚¹æå–ã€é”™é¢˜æœ¬

**ç®—æ³•å®ç°**:

```python
# src/utils/learning_curve.py

import math
from datetime import datetime, timedelta

class LearningCurveCalculator:
    """å­¦ä¹ æ›²çº¿è®¡ç®—å™¨"""

    def calculate_mastery_level(
        self,
        correct_count: int,
        total_count: int,
        last_practice_time: datetime,
        difficulty: int
    ) -> float:
        """
        è®¡ç®—çŸ¥è¯†ç‚¹æŒæ¡åº¦

        å…¬å¼: M = C * T * D
        - C: æ­£ç¡®ç‡å› å­ (0-1)
        - T: æ—¶é—´è¡°å‡å› å­ (0-1)
        - D: éš¾åº¦è°ƒæ•´å› å­ (0.8-1.2)
        """
        # æ­£ç¡®ç‡å› å­
        correctness = correct_count / total_count if total_count > 0 else 0

        # æ—¶é—´è¡°å‡å› å­ (è‰¾å®¾æµ©æ–¯æ›²çº¿)
        days_elapsed = (datetime.now() - last_practice_time).days
        time_decay = math.exp(-days_elapsed / 7)  # 7å¤©è¡°å‡å‘¨æœŸ

        # éš¾åº¦è°ƒæ•´å› å­
        difficulty_factor = 1.2 - (difficulty / 10) * 0.4  # [0.8, 1.2]

        mastery = correctness * time_decay * difficulty_factor
        return min(max(mastery, 0), 1)  # é™åˆ¶åœ¨ [0, 1]

    def predict_learning_efficiency(
        self,
        recent_scores: List[float],
        practice_frequency: int
    ) -> float:
        """é¢„æµ‹å­¦ä¹ æ•ˆç‡"""
        # è¶‹åŠ¿åˆ†æ
        if len(recent_scores) < 3:
            return 0.5

        # è®¡ç®—åˆ†æ•°å¢é•¿ç‡
        growth_rate = (recent_scores[-1] - recent_scores[0]) / len(recent_scores)

        # é¢‘ç‡åŠ æˆ
        frequency_bonus = min(practice_frequency / 7, 1.0)  # æ¯å‘¨æœ€å¤š1.0åŠ æˆ

        efficiency = 0.5 + growth_rate + frequency_bonus * 0.2
        return min(max(efficiency, 0), 1)
```

**éªŒæ”¶æ ‡å‡†**:

- âœ… æŒæ¡åº¦è®¡ç®—è€ƒè™‘æ—¶é—´è¡°å‡
- âœ… å­¦ä¹ æ•ˆç‡é¢„æµ‹
- âœ… çŸ¥è¯†ç‚¹è¶‹åŠ¿åˆ†æ
- âœ… å¯è§†åŒ–å±•ç¤º

---

## ğŸ“‹ ç¬¬ä¸‰æ‰¹ï¼šRAG æ ¸å¿ƒæˆ˜å½¹ (Week 6-9)

### ğŸ¯ ç›®æ ‡

- é›†æˆå‘é‡æ•°æ®åº“
- å®ç°è¯­ä¹‰æ£€ç´¢
- ä¸Šä¸‹æ–‡å¢å¼ºé—®ç­”

### ä»»åŠ¡æ¸…å•

#### 8ï¸âƒ£ å‘é‡æ•°æ®åº“é›†æˆ (PGVector)

**é¢„ä¼°å·¥æ—¶**: 16 å°æ—¶ (2 å¤©)  
**ä¼˜å…ˆçº§**: ğŸ”¥ğŸ”¥ğŸ”¥ æœ€é«˜  
**ä¾èµ–**: PostgreSQL 14+

**æ•°æ®åº“è¿ç§»**:

```python
# alembic/versions/xxx_add_vector_support.py

from alembic import op
import sqlalchemy as sa

def upgrade():
    # 1. åˆ›å»º vector æ‰©å±•
    op.execute('CREATE EXTENSION IF NOT EXISTS vector')

    # 2. åˆ›å»ºå‘é‡è¡¨
    op.create_table(
        'knowledge_chunks',
        sa.Column('id', sa.UUID(), primary_key=True),
        sa.Column('content', sa.Text(), nullable=False),
        sa.Column('embedding', sa.String(), nullable=False),  # vector(1536)
        sa.Column('chunk_type', sa.String(50)),  # mistake/qa/knowledge
        sa.Column('metadata', sa.JSON()),
        sa.Column('created_at', sa.DateTime()),
        sa.Column('updated_at', sa.DateTime())
    )

    # 3. åˆ›å»º HNSW ç´¢å¼• (é«˜æ€§èƒ½å‘é‡æ£€ç´¢)
    op.execute('''
        CREATE INDEX knowledge_chunks_embedding_idx
        ON knowledge_chunks
        USING hnsw (embedding vector_cosine_ops)
    ''')

def downgrade():
    op.drop_table('knowledge_chunks')
    op.execute('DROP EXTENSION vector')
```

**å‘é‡æ•°æ®åº“æœåŠ¡**:

```python
# src/core/vector_db.py

from typing import List, Dict
import numpy as np
from sqlalchemy.ext.asyncio import AsyncSession

class VectorDB:
    """å‘é‡æ•°æ®åº“æ“ä½œå°è£…"""

    async def insert_chunk(
        self,
        content: str,
        embedding: List[float],
        chunk_type: str,
        metadata: Dict
    ):
        """æ’å…¥çŸ¥è¯†ç‰‡æ®µ"""
        query = """
        INSERT INTO knowledge_chunks (id, content, embedding, chunk_type, metadata)
        VALUES (gen_random_uuid(), :content, :embedding, :chunk_type, :metadata)
        """
        await self.session.execute(
            query,
            {
                "content": content,
                "embedding": str(embedding),  # PGVector æ ¼å¼
                "chunk_type": chunk_type,
                "metadata": metadata
            }
        )

    async def search_similar(
        self,
        query_embedding: List[float],
        top_k: int = 5,
        threshold: float = 0.7
    ) -> List[Dict]:
        """è¯­ä¹‰ç›¸ä¼¼åº¦æ£€ç´¢"""
        query = """
        SELECT
            content,
            metadata,
            1 - (embedding <=> :query_embedding) AS similarity
        FROM knowledge_chunks
        WHERE 1 - (embedding <=> :query_embedding) > :threshold
        ORDER BY embedding <=> :query_embedding
        LIMIT :top_k
        """
        result = await self.session.execute(
            query,
            {
                "query_embedding": str(query_embedding),
                "threshold": threshold,
                "top_k": top_k
            }
        )
        return [dict(row) for row in result]
```

**éªŒæ”¶æ ‡å‡†**:

- âœ… PGVector æ‰©å±•å®‰è£…æˆåŠŸ
- âœ… å‘é‡è¡¨åˆ›å»ºå®Œæˆ
- âœ… HNSW ç´¢å¼•ç”Ÿæ•ˆ
- âœ… æ£€ç´¢å»¶è¿Ÿ < 100ms (1000 æ¡æ•°æ®)

---

#### 9ï¸âƒ£ Embedding æœåŠ¡å¯¹æ¥

**é¢„ä¼°å·¥æ—¶**: 8 å°æ—¶ (1 å¤©)  
**ä¼˜å…ˆçº§**: ğŸ”¥ğŸ”¥ğŸ”¥ æœ€é«˜  
**ä¾èµ–**: å‘é‡æ•°æ®åº“é›†æˆ

**æœåŠ¡å®ç°**:

```python
# src/services/embedding_service.py

import httpx
from typing import List

class EmbeddingService:
    """æ–‡æœ¬å‘é‡åŒ–æœåŠ¡"""

    EMBEDDING_API = "https://dashscope.aliyuncs.com/api/v1/services/embeddings/text-embedding/text-embedding"
    EMBEDDING_DIM = 1536  # é€šä¹‰åƒé—® Embedding ç»´åº¦

    async def embed_text(self, text: str) -> List[float]:
        """æ–‡æœ¬å‘é‡åŒ–"""
        async with httpx.AsyncClient() as client:
            response = await client.post(
                self.EMBEDDING_API,
                headers={"Authorization": f"Bearer {self.api_key}"},
                json={
                    "model": "text-embedding-v1",
                    "input": {"texts": [text]}
                },
                timeout=10.0
            )
            response.raise_for_status()
            data = response.json()
            return data["output"]["embeddings"][0]["embedding"]

    async def embed_batch(self, texts: List[str]) -> List[List[float]]:
        """æ‰¹é‡å‘é‡åŒ– (æå‡æ•ˆç‡)"""
        async with httpx.AsyncClient() as client:
            response = await client.post(
                self.EMBEDDING_API,
                headers={"Authorization": f"Bearer {self.api_key}"},
                json={
                    "model": "text-embedding-v1",
                    "input": {"texts": texts}
                },
                timeout=30.0
            )
            response.raise_for_status()
            data = response.json()
            return [e["embedding"] for e in data["output"]["embeddings"]]
```

**éªŒæ”¶æ ‡å‡†**:

- âœ… å•æ¬¡å‘é‡åŒ– < 500ms
- âœ… æ‰¹é‡å‘é‡åŒ– (100 æ¡) < 3s
- âœ… é”™è¯¯å¤„ç†å®Œå–„
- âœ… æˆæœ¬ç›‘æ§ (è®¡è´¹ç»Ÿè®¡)

---

#### ğŸ”Ÿ çŸ¥è¯†ç‰‡æ®µç®¡ç†

**é¢„ä¼°å·¥æ—¶**: 12 å°æ—¶ (1.5 å¤©)  
**ä¼˜å…ˆçº§**: ğŸ”¥ğŸ”¥ é«˜  
**ä¾èµ–**: Embedding æœåŠ¡

**ç‰‡æ®µç±»å‹**:

```python
# src/services/knowledge_chunk_service.py

class KnowledgeChunkService:
    """çŸ¥è¯†ç‰‡æ®µç®¡ç†æœåŠ¡"""

    async def add_mistake_chunk(self, mistake_record: MistakeRecord):
        """æ·»åŠ é”™é¢˜ç‰‡æ®µ"""
        content = f"""
        é”™é¢˜:
        {mistake_record.question_content}

        æ­£ç¡®ç­”æ¡ˆ: {mistake_record.correct_answer}
        å­¦ç”Ÿç­”æ¡ˆ: {mistake_record.user_answer}
        çŸ¥è¯†ç‚¹: {', '.join(mistake_record.knowledge_points)}
        """

        embedding = await self.embedding_service.embed_text(content)

        await self.vector_db.insert_chunk(
            content=content,
            embedding=embedding,
            chunk_type="mistake",
            metadata={
                "user_id": mistake_record.user_id,
                "subject": mistake_record.subject,
                "knowledge_points": mistake_record.knowledge_points,
                "difficulty": mistake_record.difficulty
            }
        )

    async def add_qa_chunk(self, question: Question, answer: Answer):
        """æ·»åŠ ä¼˜è´¨ QA å¯¹ç‰‡æ®µ"""
        if answer.quality_score < 0.8:
            return  # ä»…æ”¶å½•é«˜è´¨é‡ç­”æ¡ˆ

        content = f"""
        é—®é¢˜: {question.content}
        ç­”æ¡ˆ: {answer.content}
        çŸ¥è¯†ç‚¹: {', '.join(answer.knowledge_points)}
        """

        embedding = await self.embedding_service.embed_text(content)

        await self.vector_db.insert_chunk(
            content=content,
            embedding=embedding,
            chunk_type="qa",
            metadata={
                "subject": question.subject,
                "grade": question.grade,
                "quality_score": answer.quality_score
            }
        )

    async def add_knowledge_chunk(self, knowledge_node: KnowledgeNode):
        """æ·»åŠ å­¦ç§‘çŸ¥è¯†ç‰‡æ®µ"""
        content = f"""
        çŸ¥è¯†ç‚¹: {knowledge_node.name}
        æè¿°: {knowledge_node.description}
        å…³é”®è¯: {', '.join(knowledge_node.keywords)}
        ç¤ºä¾‹: {knowledge_node.examples}
        """

        embedding = await self.embedding_service.embed_text(content)

        await self.vector_db.insert_chunk(
            content=content,
            embedding=embedding,
            chunk_type="knowledge",
            metadata={
                "subject": knowledge_node.subject,
                "difficulty": knowledge_node.difficulty,
                "importance": knowledge_node.importance
            }
        )
```

**éªŒæ”¶æ ‡å‡†**:

- âœ… é”™é¢˜è‡ªåŠ¨å‘é‡åŒ–
- âœ… ä¼˜è´¨ QA å¯¹æ”¶å½•
- âœ… å­¦ç§‘çŸ¥è¯†åº“å‘é‡åŒ–
- âœ… ç‰‡æ®µæ›´æ–°æœºåˆ¶

---

#### 1ï¸âƒ£1ï¸âƒ£ æ£€ç´¢ç­–ç•¥å®ç°

**é¢„ä¼°å·¥æ—¶**: 12 å°æ—¶ (1.5 å¤©)  
**ä¼˜å…ˆçº§**: ğŸ”¥ğŸ”¥ğŸ”¥ æœ€é«˜  
**ä¾èµ–**: çŸ¥è¯†ç‰‡æ®µç®¡ç†

**æ··åˆæ£€ç´¢å®ç°**:

```python
# src/services/rag_service.py

class RAGService:
    """RAG æ£€ç´¢å¢å¼ºç”ŸæˆæœåŠ¡"""

    async def retrieve_context(
        self,
        query: str,
        user_id: str,
        subject: str,
        top_k: int = 5
    ) -> List[Dict]:
        """æ··åˆæ£€ç´¢ä¸Šä¸‹æ–‡"""

        # 1. è¯­ä¹‰æ£€ç´¢ (æƒé‡ 0.7)
        query_embedding = await self.embedding_service.embed_text(query)
        semantic_results = await self.vector_db.search_similar(
            query_embedding,
            top_k=10,
            threshold=0.7
        )

        # 2. å…³é”®è¯æ£€ç´¢ (æƒé‡ 0.2)
        keywords = self._extract_keywords(query)
        keyword_results = await self._keyword_search(keywords, subject)

        # 3. ç”¨æˆ·ä¸ªæ€§åŒ–è¿‡æ»¤
        user_results = [r for r in semantic_results if r["metadata"].get("user_id") == user_id]

        # 4. æ—¶é—´è¡°å‡åŠ æƒ
        time_weighted = self._apply_time_decay(semantic_results)

        # 5. é‡æ’åº
        final_results = self._rerank(
            semantic_results=time_weighted,
            keyword_results=keyword_results,
            user_results=user_results,
            weights=[0.7, 0.2, 0.1]
        )

        return final_results[:top_k]

    def _apply_time_decay(self, results: List[Dict]) -> List[Dict]:
        """æ—¶é—´è¡°å‡åŠ æƒ"""
        from datetime import datetime

        for result in results:
            created_at = result["metadata"]["created_at"]
            days_ago = (datetime.now() - created_at).days

            # æŒ‡æ•°è¡°å‡: score * exp(-days / 30)
            decay_factor = math.exp(-days_ago / 30)
            result["similarity"] *= decay_factor

        return results

    def _rerank(
        self,
        semantic_results: List,
        keyword_results: List,
        user_results: List,
        weights: List[float]
    ) -> List[Dict]:
        """é‡æ’åºç®—æ³•"""
        # åˆå¹¶å¤šè·¯æ£€ç´¢ç»“æœï¼ŒæŒ‰åŠ æƒåˆ†æ•°æ’åº
        all_results = {}

        for result in semantic_results:
            chunk_id = result["id"]
            all_results[chunk_id] = {
                "content": result["content"],
                "score": result["similarity"] * weights[0]
            }

        for result in keyword_results:
            chunk_id = result["id"]
            if chunk_id in all_results:
                all_results[chunk_id]["score"] += result["score"] * weights[1]
            else:
                all_results[chunk_id] = {
                    "content": result["content"],
                    "score": result["score"] * weights[1]
                }

        # ç”¨æˆ·ä¸ªæ€§åŒ–åŠ æˆ
        for result in user_results:
            chunk_id = result["id"]
            if chunk_id in all_results:
                all_results[chunk_id]["score"] += weights[2]

        # æŒ‰åˆ†æ•°æ’åº
        sorted_results = sorted(
            all_results.values(),
            key=lambda x: x["score"],
            reverse=True
        )

        return sorted_results
```

**ä¸Šä¸‹æ–‡æ³¨å…¥**:

```python
async def ask_with_rag(self, user_id: str, question: str) -> str:
    """RAG å¢å¼ºé—®ç­”"""

    # 1. æ£€ç´¢ç›¸å…³ä¸Šä¸‹æ–‡
    context_chunks = await self.rag_service.retrieve_context(
        query=question,
        user_id=user_id,
        subject=subject,
        top_k=5
    )

    # 2. æ„å»ºå¢å¼ºæç¤ºè¯
    context_text = "\n\n".join([
        f"[ç›¸å…³çŸ¥è¯† {i+1}]\n{chunk['content']}"
        for i, chunk in enumerate(context_chunks)
    ])

    prompt = f"""
    è¯·åŸºäºä»¥ä¸‹ç›¸å…³çŸ¥è¯†å›ç­”å­¦ç”Ÿçš„é—®é¢˜:

    {context_text}

    å­¦ç”Ÿé—®é¢˜: {question}

    è¦æ±‚:
    1. ä¼˜å…ˆä½¿ç”¨æä¾›çš„ç›¸å…³çŸ¥è¯†
    2. å¦‚æœç›¸å…³çŸ¥è¯†ä¸è¶³ï¼Œå¯ä»¥è¡¥å……
    3. ç­”æ¡ˆè¦ç®€æ´æ˜“æ‡‚
    """

    # 3. è°ƒç”¨ AI ç”Ÿæˆç­”æ¡ˆ
    response = await self.bailian_service.chat_completion(
        messages=[{"role": "user", "content": prompt}],
        temperature=0.7
    )

    return response.content
```

**éªŒæ”¶æ ‡å‡†**:

- âœ… æ£€ç´¢å‡†ç¡®ç‡ > 80% (äººå·¥è¯„ä¼°)
- âœ… æ£€ç´¢å»¶è¿Ÿ < 200ms
- âœ… ä¸Šä¸‹æ–‡æ³¨å…¥æœ‰æ•ˆæå‡ç­”æ¡ˆè´¨é‡
- âœ… é‡æ’åºç®—æ³•åˆç†

---

#### 1ï¸âƒ£2ï¸âƒ£ RAG å‰åç«¯è”è°ƒä¸æµ‹è¯•

**é¢„ä¼°å·¥æ—¶**: 8 å°æ—¶ (1 å¤©)  
**ä¼˜å…ˆçº§**: ğŸ”¥ğŸ”¥ é«˜  
**ä¾èµ–**: æ£€ç´¢ç­–ç•¥å®ç°

**é›†æˆæµ‹è¯•**:

```python
# tests/integration/test_rag.py

import pytest
from src.services.rag_service import RAGService

@pytest.mark.asyncio
async def test_rag_retrieval():
    """æµ‹è¯• RAG æ£€ç´¢"""
    rag_service = RAGService()

    # æµ‹è¯•æ•°å­¦é—®é¢˜æ£€ç´¢
    results = await rag_service.retrieve_context(
        query="å¦‚ä½•æ±‚äºŒæ¬¡å‡½æ•°çš„é¡¶ç‚¹åæ ‡?",
        user_id="test_user",
        subject="æ•°å­¦",
        top_k=5
    )

    assert len(results) > 0
    assert results[0]["similarity"] > 0.7
    assert "äºŒæ¬¡å‡½æ•°" in results[0]["content"]

@pytest.mark.asyncio
async def test_rag_answer_quality():
    """æµ‹è¯• RAG å¢å¼ºç­”æ¡ˆè´¨é‡"""
    # å‡†å¤‡æµ‹è¯•æ•°æ®
    test_questions = [
        "äºŒæ¬¡å‡½æ•°çš„å¯¹ç§°è½´æ€ä¹ˆæ±‚?",
        "å¦‚ä½•åˆ¤æ–­åœ†å’Œç›´çº¿çš„ä½ç½®å…³ç³»?",
        "è‹±è¯­ä¸­ç°åœ¨å®Œæˆæ—¶æ€ä¹ˆç”¨?"
    ]

    for question in test_questions:
        # æ—  RAG ç­”æ¡ˆ
        answer_without_rag = await ask_question(question, use_rag=False)

        # RAG å¢å¼ºç­”æ¡ˆ
        answer_with_rag = await ask_question(question, use_rag=True)

        # è´¨é‡è¯„ä¼°
        quality_without = await evaluate_quality(answer_without_rag)
        quality_with = await evaluate_quality(answer_with_rag)

        # RAG åº”è¯¥æå‡ç­”æ¡ˆè´¨é‡
        assert quality_with >= quality_without
```

**æ€§èƒ½åŸºå‡†æµ‹è¯•**:

```python
# tests/performance/test_rag_performance.py

import time
import asyncio

async def benchmark_retrieval(n_queries: int = 100):
    """æ£€ç´¢æ€§èƒ½åŸºå‡†æµ‹è¯•"""
    rag_service = RAGService()

    queries = [f"æµ‹è¯•é—®é¢˜ {i}" for i in range(n_queries)]

    start_time = time.time()
    tasks = [rag_service.retrieve_context(q) for q in queries]
    await asyncio.gather(*tasks)
    end_time = time.time()

    avg_latency = (end_time - start_time) / n_queries * 1000

    print(f"å¹³å‡æ£€ç´¢å»¶è¿Ÿ: {avg_latency:.2f} ms")
    assert avg_latency < 200  # è¦æ±‚ < 200ms
```

**éªŒæ”¶æ ‡å‡†**:

- âœ… é›†æˆæµ‹è¯•é€šè¿‡ç‡ 100%
- âœ… æ€§èƒ½æµ‹è¯•è¾¾æ ‡ (< 200ms)
- âœ… ç”¨æˆ·ä½“éªŒ A/B æµ‹è¯•
- âœ… ç›‘æ§æŒ‡æ ‡å®Œå–„

---

## ğŸ“‹ ç¬¬å››æ‰¹ï¼šRAG å¢å¼ºä¼˜åŒ– (Week 10+)

### ğŸ¯ ç›®æ ‡

- åŸºäº RAG ä¼˜åŒ–å­¦æƒ…åˆ†æ
- ä¸ªæ€§åŒ–å­¦ä¹ è·¯å¾„æ¨è

#### 1ï¸âƒ£3ï¸âƒ£ å­¦æƒ…åˆ†æç®—æ³•ä¼˜åŒ– (RAG å¢å¼ºç‰ˆ)

**é¢„ä¼°å·¥æ—¶**: 12 å°æ—¶ (1.5 å¤©)  
**ä¼˜å…ˆçº§**: ğŸ”¥ ä¸­  
**ä¾èµ–**: RAG ç³»ç»Ÿå®Œæˆ

**å®ç°æ–¹æ¡ˆ**:

```python
# src/services/analytics_service.py (RAG å¢å¼ºç‰ˆ)

class AnalyticsServiceV2:
    """å­¦æƒ…åˆ†ææœåŠ¡ (RAG å¢å¼º)"""

    async def recommend_learning_path(self, user_id: str) -> List[Dict]:
        """ä¸ªæ€§åŒ–å­¦ä¹ è·¯å¾„æ¨è"""

        # 1. åˆ†æå­¦ç”Ÿè–„å¼±çŸ¥è¯†ç‚¹
        weak_points = await self._analyze_weak_points(user_id)

        # 2. åŸºäºå‘é‡ç›¸ä¼¼åº¦æŸ¥æ‰¾ç›¸å…³çŸ¥è¯†ç‚¹
        related_knowledge = []
        for point in weak_points:
            embedding = await self.embedding_service.embed_text(point["name"])
            similar = await self.vector_db.search_similar(
                embedding,
                chunk_type="knowledge",
                top_k=5
            )
            related_knowledge.extend(similar)

        # 3. æ„å»ºå­¦ä¹ è·¯å¾„
        learning_path = self._build_learning_path(
            weak_points,
            related_knowledge
        )

        return learning_path

    def _build_learning_path(
        self,
        weak_points: List[Dict],
        related_knowledge: List[Dict]
    ) -> List[Dict]:
        """æ„å»ºå­¦ä¹ è·¯å¾„"""
        # åŸºäºçŸ¥è¯†å›¾è°±çš„å‰ç½®å…³ç³»æ’åº
        # ç»“åˆéš¾åº¦æ¢¯åº¦è®¾è®¡å­¦ä¹ é¡ºåº
        pass
```

**éªŒæ”¶æ ‡å‡†**:

- âœ… æ¨èå‡†ç¡®ç‡ > 75%
- âœ… å­¦ä¹ è·¯å¾„åˆç†æ€§éªŒè¯
- âœ… ç”¨æˆ·åé¦ˆæœºåˆ¶

---

## ğŸ“ˆ æˆåŠŸæŒ‡æ ‡ä¸éªŒæ”¶æ ‡å‡†

### ç¬¬ä¸€æ‰¹éªŒæ”¶ (Week 3)

- âœ… çŸ¥è¯†ç‚¹æå–å‡†ç¡®ç‡ > 80%
- âœ… çŸ¥è¯†å›¾è°±æ•°æ® > 1000 ä¸ªèŠ‚ç‚¹
- âœ… ç­”æ¡ˆè´¨é‡è¯„ä¼°ä¸Šçº¿

### ç¬¬äºŒæ‰¹éªŒæ”¶ (Week 5)

- âœ… æµå¼å“åº”å»¶è¿Ÿ < 200ms
- âœ… ç¼“å­˜å‘½ä¸­ç‡ > 20%
- âœ… é”™é¢˜æœ¬åŠŸèƒ½å®Œæ•´

### ç¬¬ä¸‰æ‰¹éªŒæ”¶ (Week 9)

- âœ… RAG æ£€ç´¢å‡†ç¡®ç‡ > 80%
- âœ… æ£€ç´¢å»¶è¿Ÿ < 200ms
- âœ… é—®ç­”è´¨é‡æå‡ > 15%

### ç¬¬å››æ‰¹éªŒæ”¶ (Week 10+)

- âœ… å­¦ä¹ è·¯å¾„æ¨èä¸Šçº¿
- âœ… ç”¨æˆ·æ»¡æ„åº¦ > 4.0/5.0

---

## ğŸ”„ é£é™©ä¸åº”å¯¹

### æŠ€æœ¯é£é™©

| é£é™©                   | æ¦‚ç‡ | å½±å“ | åº”å¯¹æªæ–½                   |
| ---------------------- | ---- | ---- | -------------------------- |
| PGVector æ€§èƒ½ä¸è¾¾æ ‡    | ä¸­   | é«˜   | é¢„å¤‡ Milvus å¤‡é€‰æ–¹æ¡ˆ       |
| Embedding API æˆæœ¬è¿‡é«˜ | é«˜   | ä¸­   | å®æ–½ç¼“å­˜ç­–ç•¥ï¼Œè€ƒè™‘æœ¬åœ°æ¨¡å‹ |
| çŸ¥è¯†å›¾è°±æ•°æ®è´¨é‡å·®     | ä¸­   | é«˜   | äººå·¥å®¡æ ¸ + ä¼—åŒ…éªŒè¯        |
| RAG æ£€ç´¢ç²¾åº¦ä¸è¶³       | ä¸­   | é«˜   | æŒç»­ä¼˜åŒ–æ£€ç´¢ç­–ç•¥å’Œé‡æ’åº   |

### èµ„æºé£é™©

| é£é™©              | æ¦‚ç‡ | å½±å“ | åº”å¯¹æªæ–½                       |
| ----------------- | ---- | ---- | ------------------------------ |
| å¼€å‘æ—¶é—´ä¸è¶³      | ä¸­   | ä¸­   | åŠ¨æ€è°ƒæ•´ä¼˜å…ˆçº§ï¼Œç æ‰ä½ä»·å€¼åŠŸèƒ½ |
| AI æœåŠ¡è´¹ç”¨è¶…é¢„ç®— | é«˜   | ä¸­   | å®æ–½ä¸¥æ ¼çš„ç¼“å­˜å’Œé™æµç­–ç•¥       |

---

## ğŸ“ æ€»ç»“

### å…³é”®å†³ç­–ç†ç”±

1. **RAG åç½®**: é¿å…æŠ€æœ¯é£é™©é›†ä¸­ï¼Œå¿«é€Ÿäº¤ä»˜ä»·å€¼
2. **çŸ¥è¯†ç‚¹ä¼˜å…ˆ**: ä¸º RAG æ‰“å¥½æ•°æ®åŸºç¡€
3. **ä½“éªŒä¼˜åŒ–ç©¿æ’**: ä¿æŒå¼€å‘èŠ‚å¥ï¼ŒæŒç»­ç”¨æˆ·åé¦ˆ

### é¢„æœŸæ”¶ç›Š

- **Week 3**: å­¦æƒ…åˆ†æå‡†ç¡®åº¦æå‡ 30%
- **Week 5**: ç”¨æˆ·ç•™å­˜ç‡æå‡ 15%
- **Week 9**: é—®ç­”è´¨é‡æå‡ 25%ï¼Œæ ¸å¿ƒå–ç‚¹å®Œæ•´å‘ˆç°

### ä¸‹ä¸€æ­¥è¡ŒåŠ¨

**ç«‹å³å¼€å§‹**: çŸ¥è¯†ç‚¹æå–ä¼˜åŒ– (TD-002)  
**å‡†å¤‡å·¥ä½œ**: çŸ¥è¯†å›¾è°±æ•°æ®æ”¶é›†  
**æŠ€æœ¯é¢„ç ”**: PGVector ç¯å¢ƒæ­å»º

---

**æ–‡æ¡£ç»´æŠ¤**: æ¯æ‰¹æ¬¡å®Œæˆåæ›´æ–°è¿›åº¦  
**è¯„å®¡å‘¨æœŸ**: æ¯å‘¨äº”å›¢é˜Ÿ Review  
**è°ƒæ•´æœºåˆ¶**: æ ¹æ®å®é™…æƒ…å†µåŠ¨æ€è°ƒæ•´ä¼˜å…ˆçº§
